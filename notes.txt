$Id: notes.txt,v 1.130 2002/07/22 21:11:38 mstorti Exp $
OK. This is mostly in spanish...  Sorry :-)
================================================================

%===Mon Aug  2 23:29:29 ART 1999
PetscFEM es una aplicacion PETSC para hacer un programa FEM. 
Salvo. ver: 1.1

%===Wed Aug  4 23:06:31 ART 1999
habia unos cuantos errores en la lectura de datos y se colgaba al
llamar a MatCreateSeqAIJ(). Ahora no se cuelga. 
Salvo. ver: 1.3

%===Sat Sep  4 12:18:45 ART 1999
No andaba porque definia *elemset pero despues no le asignaba un valor
con new. Siempre que se define un puntero despues hay que asignarle un
valor!!
Salvo. ver: 1.4

%===Sat Sep  4 18:16:26 ART 1999
Anda calculo de la matriz y del residuo. 
Salvo. ver: 1.5

%===Sat Sep 11 19:36:04 ART 1999
Pongo toda la informacion de los puntos de Gauss en una estructura
GPdata. Tuve problemas cuando hacia el delete en el destructor de
GPdata, pero parece que ahora anda aunque no se bien porque. Lo probe
para el triangulo, ahora hay que probar el cuadrilatero. 
Salvo. ver: 1.6

%===Sat Sep 11 22:50:33 ART 1999
No es verdad que estaba andando bien el cuadrangulo. Estaba tomando el
triangulo y en el cuadrangulo todavia hay errores. Estoy debuggeando. 

%===Sat Sep 11 22:56:08 ART 1999
Ahora parece que esta andando para cuadrangulos. 
Salvo version: 1.7

%===Sun Sep 12 08:31:22 ART 1999
Parece que anda bien un ejemplito de adveccion difusion.
version: 1.8

%===Sun Sep 12 10:14:56 ART 1999
Le puse que el determinante e inversa del jacobiano se calcule con las
funciones de newmat. 
version: 1.9

%===Sun Sep 12 12:52:04 ART 1999
Anda bien adveccion-difusion con cuadrangulos. Ahora voy a probar con
triangulos. 
version: 1.10

%===Sun Sep 12 13:51:32 ART 1999
Anda bien adveccion-difusion con triangulos. 
version: 1.11

%===Sun Sep 12 22:46:02 ART 1999
Hice andar las GNU-hash-tables (try4.cpp). 
version: 1.12

%===Fri Sep 17 12:16:52 ART 1999
Voy a hacer una clase TextHashTable.
version: 1.13

%===Fri Sep 17 13:47:38 ART 1999
parece que anda la clase TextHashTable.
version: 1.14

%===Fri Sep 17 19:41:51 ART 1999
Hecha la tabla texthash (.cpp y .h). 
Ahora hay que meterla en fem.cpp
version: 1.15

%===Sat Sep 18 20:05:53 ART 1999
Leo nodos en forma directa (sin leer dos veces el archivo) pero con
alocacion dinamica, alocando en una serie de 'chunks" y despues se
copia todo en la memoria. 
version: 1.16

%===Sat Sep 18 20:39:01 ART 1999
Corregidos ciertos errores en la (buffereada) lectura de nodos. 
Ahora voy a leer elementos. 
version: 1.17

%===Thu Sep 23 13:09:35 ART 1999
Lee elememsets y los pone en una lista. Ahora tengo que hacer que lea
fijaciones y etc. 
Salvo. ver: 1.18

%===Sat Sep 25 10:07:34 ART 1999
Anda libretto!!! grande!!! Permite construir listas, etc... Hice una
lista de enteros y de punteros a dobles. try9.cpp
version: 1.19

%===Sat Sep 25 11:01:20 ART 1999
Estoy poniendo eleemsetlist en forma de un Darray. 
version: 1.20

%===Sat Sep 25 11:21:26 ART 1999
Voy a cambiar el nombre xnod por nodedata. Se supone que contiene
informacion de los nodos que no cambia. 

%===Sat Sep 25 22:20:33 ART 1999
Anda bien guardar los punteros a elemesets en un arregloe dinamico
Darray de "libretto". 
version: 1.21

%===Sat Sep 25 23:19:27 ART 1999
Tengo que ver como paso el valor de nu, el numero de columnas en
nodedata. Probablemente tendria que definir una clase nodedata y
entonces nu seria un miembro de la misma. 
version: 1.22

%===Sun Sep 26 20:46:42 ART 1999
Anda bien la llamada por elemsets. Ahora hay que definir que le asigne
el diferente tipo de elemset segun el string que esta en el archivo de
datos. 
version: 1.23

%===Sun Sep 26 21:58:02 ART 1999
falta debuggear print_vector
version: 1.24

%===Mon Sep 27 19:05:40 ART 1999
Anda bien como triangulo y quad. 
Salvo. ver: 1.25

%===Mon Sep 27 19:14:19 ART 1999
Voy a separar en mas archivos .cpp, .h.

%===Mon Sep 27 20:06:58 ART 1999
Estan splitteados los fuentes. Compila y corre bien.
Salvo. ver: 1.26

%===Mon Sep 27 21:05:08 ART 1999
Llama a diferentes elemsets dependiendo de un macro. 
CHECK_ELEMSET_TYPE.
Salvo. ver: 1.27

%===Tue Sep 28 08:43:12 ART 1999
Voy a probar con el jacobiano numerico. Primero componente a
componente. 
Salvo. ver: 1.28

%===Tue Sep 28 09:29:06 ART 1999
Hago un wrapper sobre internal, para poder llamar al jacobiano
numerico. 
Salvo. ver: 1.29

%===Wed Sep 29 19:10:30 ART 1999
El programa anda medio chancho. Verificamos que newmat es bastante
chancho para pequnhas dimensiones. 
Tiempos (en secs.) para 1e6 evaluaciones de un producto de dos matrices de NxN. 
-
N            Newmat        A pata (en C++)
----        --------     -------------------
3             84             12.4
6            129             84.3
12           462            607.
-
-
Se observa que para N=3 newmat es muy chancho (una relaicon 7 a 1),
para N=6 ya andan ahi nomas y para N=12 ya la relacion se
invierte. Conclusion: conviene seguir poe el momento ocn newmat. 
Salvo. ver: 1.30

%===Wed Sep 29 19:17:52 ART 1999
Proximo objetivo: pasar "assemble" a que calcule los residuos
desacoplados por elemento. Eso facilitaria el calculo del jacobiano
numerico y tambien la programacion de cada "elemset". 

%===Thu Sep 30 09:45:06 ART 1999
Calcula residuos desacoplados por elemento. Ahora voy a hacer que
ensamble matrices calculadas (no numericamente), despues viene el
jacobiano numerico. 
version: 1.31

%===Thu Sep 30 12:09:23 ART 1999
Calcula bien residuos y matrices con la forma "por elemento".
Ahora: a calcular el jacobiano numerico. 
version: 1.32

%===Thu Sep 30 19:32:01 ART 1999
Anda bien el calculo numerico del jacobiano!! Ahora, para
verificar voy a probar de empezar desde un vector random. 
version: 1.33

%===Thu Sep 30 19:42:00 ART 1999
Anda bien con el vector inicializado random!! 
version: 1.34

%===Thu Sep 30 19:44:08 ART 1999
Cambio a nuevo nivel de version 2.0
version: 2.1

%===Thu Sep 30 19:54:41 ART 1999
Vamos a hacer una prueba con un problema con
dos grados de libertad. 

%===Thu Sep 30 20:14:40 ART 1999
Hago una estrategia Newton Raphson. Antes estaba en una forma muy
lineal. Puede haber algunos quilombos con los signos. 

%===Thu Sep 30 20:22:33 ART 1999
Anda bien el Newton Raphson en lineal. 
version: 2.2

%===Thu Sep 30 20:51:33 ART 1999
calcula determinante del jacobiano con una rutina mydet()
version: 2.3

%===Thu Sep 30 20:53:06 ART 1999
Voy a tratar de resolver varios campos. Primero lo mas simple, dos
campos acoplados, con las mismas condiciones de contorno,
conductividad, etc... 

%===Thu Sep 30 20:56:00 ART 1999
Hay que chequear tambien como pone las cargas!! Hasta ahora no lo chequee!!

%===Thu Sep 30 22:20:00 ART 1999
Andan bien dos campos desacoplados. 
version: 2.4

%===Fri Oct  1 19:22:29 ART 1999
Anda el 3D. Objetivo siguiente: fuente no lineal
Salvo. ver: 2.5

%===Sat Oct  2 08:44:19 ART 1999
retomo en casa.

%===Sat Oct  2 09:15:45 ART 1999
Pequenhos cambios. 
version: 2.6

%===Sat Oct  2 17:49:59 ART 1999
agrego la propiedad por elementos. La lee bien y llega bien al
elemset. 

%===Sat Oct  2 19:28:36 ART 1999
Voy a agregar que lea una tabla de strings con las propiedades de los
elementos entonces despues puede pedir una propiedad
(por. ej. "conductividad")  y esta puede ser constante para todo el
elemset o por elemento. 

%===Wed Oct 13 12:53:06 ART 1999
Performance de newmat versus fastmat
Newmat: 6.9e-3 sec/hexa (Laplace,2 ptos de Gauss)
FastMat: 8.8e-4 sec/hexa (Laplace,2 ptos de Gauss)

%===Sun Oct 17 18:24:42 ART 1999
Unified PROJECTS file with notas.txt.

%===Sun Oct 17 18:25:22 ART 1999
Voy a pasar r y A como variable arguments. 

%===Sun Oct 17 18:45:11 ART 1999
No, es demasiado enquilombado, voy a pasar o un puntero generico, o
tantos argumentos como hagan falta!!

%===Sat Nov 20 19:54:02 ART 1999
Cambio de llamar a ident a llamar dofmap.get_row(), devuelve una fila
del mapeo. 

%===Mon Nov 22 10:27:01 ART 1999
Anda bien el cambio ident -> dofmap con un solo procesador para el
problemita del laplaciano. Lo voy a probar en dos procesadores.

%===Mon Nov 22 10:52:18 ART 1999
Anda bien en dos procesadores. Ahora a poner las condiciones
periodicas. 

%===Mon Nov 22 21:59:58 ART 1999
Anda bien las fijaciones. 

%===Mon Nov 22 23:25:00 ART 1999
Estoy definiendo Q. Falta resinchronizar Q, ordenandolo. 

%===Thu Nov 25 05:45:53 ART 1999
Habia un error 

%===Fri Nov 26 11:34:53 ART 1999
Cambio download_vector a un miembro de la clase Elemset. Defino una
funcion compute_this_elem que retorna un flag si hay que calcular este
elemento o no. 

%===Thu Dec  2 19:26:58 ART 1999
Retomo despues de Fluidos99. Hay que debuggear porque hace mallocs()
despues en la etapa de poisson y en las de momento. Ver que pasa con
los chunks!!

%===Thu Dec  2 19:39:42 ART 1999
Con el lapli.dat (2x2) anda al pelo. 0 unneeded y 0 mallocs. 
Tambien anda bien con chunk_size=1

%===Thu Dec  2 20:20:26 ART 1999
Cavidad chiquita de 3x3 elementos. Poisson y proyeccion andan
bien. Momento da muchos unneeded  0 mallocs en la primera llamada y
unneded y mallocs>0 en la segunda. A partir de la tercera da todo bien.

%===Fri Dec  3 11:35:29 ART 1999
Dimensiono correctamente matlocmom2. Ahora con un procesador y cuando
parto de una corrida anterior hace todas las alocaciones bien, no
mallocs y 0 unneedeed. Cuando parto de 0 ahi da unneeded en iter 1 y
mallocs y unneded en iter 2. Voy a probar a comentar el zeroe-mat.

%===Fri Dec  3 11:41:34 ART 1999
Tambien hace mallocs y unneeded cuando saco el zeroe_mat.

%===Fri Dec  3 12:08:20 ART 1999
Voy a verificar si con la cavidad cubica 5x5x5 tambien anda todo bien
al continuar una corrida anterior. 

%===Fri Dec  3 12:16:52 ART 1999
Con la cavidad cubica ocurre que en Poisson tambien hay unneeded y
mallocs. 

%===Fri Dec  3 12:30:47 ART 1999
Largo la cavidad cubica 41x41x41 (media malla) relanzando de la
corrida que hizo 72 iteraciones. 

%===Fri Dec  3 12:33:02 ART 1999
Mientras tanto voy a probar que pasa con la cavidad cubica 5x5x5 con
uns solo procesador para ver si tambien hace mallocs en el poisson. 

%===Fri Dec  3 12:36:14 ART 1999
Si! Tambien anda mal el poisson con un solo procesador. 

%===Fri Dec  3 15:28:21 ART 1999
Bueno, la matriz del laplaciano en una malla cubica da cero entre un
nodo y el nodo que da con el centro de las caras. O sea que la
estructura del stencil es

     1   2    1 
  2    0    2
1    2   1 

     2   0    2 
  0  -32    0
2    0   2 

     1   2    1 
  2    0    2
1    2   1 

Con lo cual hay ceros (eso no ocurre con el cudrilatero 2D) y puede
traer dramas al momento de alocar. 

%===Fri Dec  3 16:15:11 ART 1999
Hice un jobinfo = comp_mat_poi_prof para calcular el perfil del
poisson. Ahora da 0 mallocs cuando se reinicializa. 

%===Sat Dec  4 19:42:21 ART 1999
Separe las aplicaciones NS y LAPLA de las librerias. la libreria la
llame libpetscfem.a

%===Sun Dec  5 10:28:02 ART 1999
Estan andando tetras con npg=1 y npg=4. 

%===Wed Dec  8 18:26:43 ART 1999
El elmento de carga generico esta funcionando. A veces la resolucion
del sistema lineal pareceria que le cuesta converger. Pero la matriz
parece estar bien armada. 

%===Wed Dec  8 23:22:55 ART 1999
El flujo es el que llega a la capa u1, es decir que debe ser de la
forma hfilm*(u2-u1) entonces en el caso de ser u2=0, debe ser
-hfilm*u1. 

%===Wed Dec  8 23:30:12 ART 1999
No anda bien el newton cuando el intercambio convectivo es no-lineal. 

%===Fri Dec 10 12:15:52 ART 1999
Ahora anda bien el intercambio convectivo no-lineal. No hacia zero la
matriz del jacobiano antes de empezar a recalcularla en el lazo. 

%===Fri Dec 10 13:44:32 ART 1999
Andan las opciones generales.

%===Sat Dec 11 14:51:30 ART 1999
Reescribi la lectura de datos. Ahora no hace mas lo de chunkptr's para
leer nodos, etc.. y por lo tanto puede leer nodos y elementos de
tamanho ilimitado. 

%===Sun Dec 12 10:34:42 ART 1999
Junto Nodedata y elemsetlist en un solo objeto `mesh'. 

%===Sun Dec 12 20:08:04 ART 1999
Ahora chunk_size se puede poner como un dato del elemento. 

%===Mon Dec 13 10:08:25 ART 1999
Anda file_stack. Es un objeto al cual le vas pidiendo lineas con
get_line y te las va dando. Si hay includes te va pasando al arhivo
siguiente y asi... 

%===Wed Dec 15 17:21:51 ART 1999
Retomo en casa. Anda bien la lectura de archivos, y `lapla' y `ns'
estan reescritos con la estructura `Mesh'. Estoy debugueando porque no
anda en 3D. Da un pico de presion y consecuente singularidad en el
campo de velocidad cerca de donde se impone la presion a cero. Estoy
verificando que el residuo sea igual ante un cambio de constante en el
campo de presion. Inicializo de un vector de estado arbitrario e
imponiendo la presion en dos nodos diferentes (el 1 y el 2) despues de
dos iteraciones da un estado diferente. La diferencia tiene mucho
ruido cerca de donde se impone la presion. 

%===Wed Dec 15 17:38:28 ART 1999
Pense que podria estar relacionado con la solucion iterativa de los
subpasos. Pero aumentando la precision (bajando a tolerancia a 1e-8)
da igual. 

%===Wed Dec 15 17:39:57 ART 1999
Vamos a ver si en 2D pasa lo mismo. 

%===Wed Dec 15 18:24:14 ART 1999
En 2D parece que SI da lo mismo. Volvamos a verificar el 3D. 

%===Wed Dec 15 19:40:05 ART 1999
Efectivamente parece que al pasar a 3D no es invariante al agregar una
constante en la presion. 

%===Wed Dec 15 20:19:41 ART 1999
La matriz de poisson parece que esta bien. 

%===Wed Dec 15 20:25:05 ART 1999
Despues de un paso tambien la solucion es diferente. 

%===Wed Dec 15 20:43:09 ART 1999
La diferencia se produce en el paso de Poisson.

%===Thu Dec 16 10:22:53 ART 1999 
Encontrado el error! La suma de los residuos no daba nula porque el
termino div.u en el miembro derecho del problema de Poisson no estaba
escrito en forma conservativa. Debilitando es conservativo y se
arregla. Pero queda un termino de contorno que es no nulo cuando hay
una entrada de masa por el contorno y que en el caso general no se de
de donde lo vamos a calcular. Tal vez si podria ser tratado como
BCCONV.

%===Thu Dec 16 10:47:05 ART 1999
Voy a hacer el jacobiano analitico del Poisson y de la proyeccion. 

%===Thu Dec 16 12:13:58 ART 1999
Anda el jacobiano analitico para Poisson y de la proyeccion. 
Voy a hacer una clase "sparse" para despues usar con los ghost nodes y
poder eliminar los vectores `seq'. Despues haria lo de pasar
como argumentos de entrada y salida una lista arbitraria de vectores y
matrices. 

%===Sun Dec 19 15:26:31 ART 1999
Corriendo en casa. Problemas al hacer el upgrade a RedHat 6.1. Emacs
no andaba, se colgaba. Mpich sigue sin compilar el serv_p4.c. Libretto
sigue teniendo problemas al compilar. El debugger se cuelga al hacer
`p da_length(algo)' o parece colgarse al ejecutar cualquier funcion. 
#
Problemas especificos del programa: Habia un lio al leer las
conectividades propiedades. Fundamentalmente venia cuando nelprops=0 y
era un probelma con libretto. Ahora leo conectividades y props en el
mismo darray. Por otra parte leerlo en el mismo darray probablemente
era bastante ineficiente. Para hacerlo hay que ir copiando a un buffer
especial como si fueran "unsigned char". Ahora tengo que probarlo con
un numero par de enteros, a ver si no se corrompe el double. 

%===Sun Dec 19 15:36:02 ART 1999
Anda bien tambien con un numero impar de enteros en icone. 

%===Sun Dec 19 17:12:30 ART 1999
Voy debuggear el shallow water. 

%===Mon Dec 20 09:47:59 ART 1999
Empieza a andar el shallow water. Voy a ponerle el termino fuente. 

%===Mon Jan  3 13:09:04 ART 2000
Retomo despues de un tiempo de no anotar en la bitacora. Estoy
escribiendo una clase para reemplazar al dofmap. Primero estoy
escribiendo una clase de matrices "block_matrix" tipo sparse. Ya
esvribi las operaciones basicas, hasta `axpy'. Ahora tengo que
escribir un `split'. 

%===Mon Jan  3 19:31:01 ART 2000
Hay un error en la concepcion de block_matrix(), n esta intercambiado
con m. 

%===Tue Jan  4 12:16:10 ART 2000
Anda la funcion `split' que divide una block_matrix en
sub-bloques. Voy a chequear que no pierda memoria. 

%===Wed Jan  5 20:18:02 ART 2000
Hice que contemple el caso de matrices `nulas'. 

%===Fri Jan  7 13:07:37 ART 2000
Estoy escribiendo la clase `idmap' que permite tratar matrices que son
`casi' una permutacion. Ahora voy a escribir la inversion. 
Salvo ver: 1.4

%===Sat Jan  8 09:32:05 ART 2000
Anda la resolucion de sistemas con "solve". 
Salvo  version: 1.4

%===Sat Jan  8 22:18:10 ART 2000
Esta verificada la resolucion son solve. 
Salvo version: 1.5

%===Sun Jan  9 12:07:19 ART 2000
Los templates tienen que ir en los header si no no se pueden hacer las
instanciaciones apropriadas. 

%===Sun Jan  9 13:25:44 ART 2000
Habia un error con icur en la generacion de la matriz. Ahora parece
andar bien. 
Salvo version: 1.6

%===Fri Jan 14 16:20:43 ART 2000
Escribi bastante de documentacion. Ahora voy a poner las condiciones
periodicas con `idmap'. 
Salvo spider version: 1.9

%===Fri Jan 14 18:00:22 ART 2000
Voy a empezar a implementar el manejo de grados de libertad con
`idmap'. Que hacer con las fijaciones? En principio la idea es
asignarle a los valores fijos una posicion en el 

%===Thu Jan 20 20:57:59 ART 2000
Anda bien en read_mesh la puesta de fijaciones y numeracion de grados
de libertad on `idmap' y `dofmap'. 
Salvo spider version: 1.11

%===Fri Jan 21 10:08:58 ART 2000 
Al hacer que `dofmap' es creado con `new' en read_mesh, entonces hay
que pasarlo por referencia `Dofmap *&dofmap' en los argumentos de
read_mesh, sino no queda definida. Antes dofmap era definido en el
main() y era pasada a readmesh.
%
Ojo con los vectores de STL (fixed, fixed_remapped) que no se puede
referenciar a un elemento si previamente no fue definido. Al querer
manipularlo con vec[k] no canta nada y puede dar un error de memoria. 

%===Fri Jan 21 10:53:29 ART 2000
Esta andando!! Corre un pequenho caso sin condiciones periodicas en un
solo procesador. 
Salvo spider version: 1.12

%===Fri Jan 21 18:09:30 ART 2000 
Habia dos errores. get_nodal_value() estaba acumulando sin inicializar
a cero. Tambien habia un error en el direccionamiento en
get_dofval(). Ahora da bien un problema con condiciones Dirichlet. 
Salvo spider version: 1.13

%===Fri Jan 21 18:11:21 ART 2000
Voy a probar con 2 procesadores.

%===Sat Jan 22 18:19:35 ART 2000
No andaba con 2 procesadores. Era porque ahora get_row devuelve una
lista con free dof's y fijaciones. En la parte ne que define dof_here
hay que evitar las fijaciones. Para dejarlo bien ahora voy a hacer un
get_row_free que en la lista no ponga las fijaciones. 

%===Sat Jan 22 20:23:50 ART 2000
Anda bien en dos procesadores. 

%===Sun Jan 23 10:17:32 ART 2000
Tambien en 4. 
Salvo 1.14 2000/01/23 (spider)

%===Sun Jan 23 20:27:20 ART 2000
Andan bien las condiciones periodicas, ahora como "constraints". Voy a
probar en dos procesadores. 
Salvo 1.15 2000/01/23 (spider)

%===Sun Jan 23 20:32:24 ART 2000
Anda bien con dos procesadores.

%===Mon Jan 24 20:55:24 ART 2000
Anda bien un problema con condiciones periodicas. Ahora voy a probar
con un sector de corona circular. 
Salvo 1.16 2000/01/24 (spider)

%===Tue Jan 25 10:35:18 ART 2000
Anda bien el ejemplo de uns sector circular $2.7 < r < 4.5$ y $0 <
\theta < \pi/4$. Se resuelve $\Delta !u=0$ donde $!u$ es un vector
(velocidad?), con condiciones $!u=0$ en $r=r_\in$ y $!u=U\etheta$ en
$r=r_\ext$ y condiciones periodicas $\at{u_{r,\theta}}{r_\in} =
\at{u_{r,\theta}}{r_\ou}$. 

%===Tue Jan 25 10:45:30 ART 2000
Anda bien tambien calculando la matriz por diferencias finitas. Ahora
voy a hacer lo de pasar "reacciones" por los "retval"'s. 

%===Wed Jan 26 09:34:11 ART 2000
Anda bien poner las restricciones mediante una clase Constraint. 
Salvo version 1.17 2000/01/26 (spider)

%===Fri Jan 28 17:24:53 ART 2000
Estoy transformando el pasaje de ghost-values. Antes se hacia haciendo
un scatter a un vector con todos los valores (locales y ghost). Ahora
lo hcemos haciendo scatter solo de los valores ghost. Tambien, los
argumentos se van a pasar a elemset por una lista de argumentos (clase
`arg_list'). 
%
Originalmente pense en construir un vector con VecCreateGhost(), pero
resulta que (parece ser un bug de PETSc) no se puede hacer un
duplicate de vectores cuando no hay ghost values (por ejemplo cuando
se corre en un solo procesador). 
%
En este momento esta andando el pasaje de argumentos por arg_list y
hacer el scatter solo para los ghost values. 
Salvo version 1.18 2000/01/28 (spider)

%===Sat Jan 29 20:23:18 ART 2000
Esta empezando a andar la adaptacion de ghost_dofs. 
Salvo version 1.19 2000/01/29 (spider)

%===Sun Jan 30 15:23:55 ART 2000
Anda bien el pasaje de argmentos IN_VECTOR y OUT_VECTOR. Calcula bien
el residuo ("comp_res" en Laplace). Verifique que no pierde memoria,
corriendolo muchas veces y monitoreando con `top'. 
Salvo version 1.20 2000/01/30 (spider)

%===Sun Jan 30 15:26:28 ART 2000
Ahora voy a definir el OUT_MATRIX. 

%===Sun Jan 30 20:50:18 ART 2000
Primero tuve que definir PROFILE. Parece que anda. 
Salvo version 1.21 2000/01/30 (spider)

%===Sun Jan 30 20:59:10 ART 2000
Despes hay que verificar que pasa con varios procesadores y en un lazo
grande si pierde memoria. 

%===Mon Jan 31 09:54:23 ART 2000
Anda el laplaciano con calculo de matriz y residuo en la misma llamada
a assemble. Tambien anda de nuevo `print_vector()'. 
Salvo version 1.22 2000/01/31 (spider)

%===Mon Jan 31 10:33:11 ART 2000
Anda en dos procesadores!! Tuve que hacer un scatter a un vector
secuencial, pero que tiene "neq" valores en el procesador `0' y 0
valores en los otros. 
Salvo version 1.23 2000/01/31 (spider)

%===Mon Jan 31 20:44:08 ART 2000
Anda bien la inicializacion desde un vector "ini.dat". Se lee todo el
vector, se resuelve con la matriz Q y se setean los valores en el
vector global con VecSetValue(). 
Salvo version 1.24 2000/01/31 (spider)

%===Mon Jan 31 20:48:00 ART 2000
Eventualmente habria que hacer que verifique si al resolver Q*x = y
para x, despues cual es la norma de ||Q x-y||. 
Ahora voy a hacer la opcion de vector IN/OUT. 

%===Fri Feb  4 09:48:51 ART 2000
Estoy tratando de hacer el shallow water con condiciones de contorno
periodicas. Converti a shallow.cpp en una rutina generica que llama a
`flux_fun.cpp', y ahora estoy debugeando con adveccion lineal.
Salvo version 1.25 2000/02/04 (spider)

%===Sat Feb  5 12:30:20 ART 2000
Voy a escribir la rutina de condiciones absorbentes para shallow. 

%===Sat Feb  5 13:18:40 ART 2000
Cambie `shallow' a advective y `sw' a `adv'. 
Salvo version 1.26 2000/02/05 (spider)

%===Tue Feb  8 12:51:41 ART 2000
Andan las condiciones absorbentes. Por lo menos en 1D. Ahora voya a
probar con un fondo variable. 
Salvo version 1.27 2000/02/08 (spider)

%===Thu Feb 10 09:53:20 ART 2000
Anda bien el shallow water unidimensional. Captura bien la solucion
simetrica subcritica alrededor de una loma. Ahora voy a probar con la
solucion sub->super->sub-critica. 

%===Thu Feb 10 11:21:49 ART 2000
Con <newmat.h> tambien anda en spider. 

%===Fri Feb 11 11:04:46 ART 2000
Tengo problemas con el shallow water. No consigo capturar una solucion
sub-super-subsonica. 

%===Tue Feb 15 15:13:10 ART 2000
Dejo el shallow water. Hice un cambio en los Makefile, ahora puede
correr en trantor para Ruperto, etc... 
%
Ahora voy a tratar de llevar el Navier-Stokes al formato nuevo. 

%===Fri Mar 10 09:34:45 ART 2000
Lo de llevar Navier-Stokes al formato nuevo anduvo bien, pero el tema
de la memoria esta peor que antes. Despues me fui de vacaciones y
ahora estoy debuggeando shallow-water. 
%
Ahora lo hice andar con condiciones de contorno periodicas en las dos
direcciones, version debilitada. Quiero ver el tema de la
estabilidad. 

%===Mon Mar 13 13:35:27 ART 2000
Para el problema de un escalon con condiciones periodicas andan bien
tanto la version debilitada como la no debilitada. 

%===Mon Mar 13 13:37:46 ART 2000
Dan bastante iguales pero no exactamente iguales (lo cual esta bien). 

%===Mon Mar 13 13:55:28 ART 2000
Con condiciones absorbentes tambien da bien. 

%===Tue Mar 14 15:01:22 ART 2000
Descubrimos que en el KSPMonitor habia que destruir el vector de
residuos. 
%
Estoy escribiendo una rutina print_vector_rota() que va guardando
vectores en archivos rotandolos. 

%===Fri Mar 24 13:39:01 ART 2000
En NS, vimos que el precondicionador Jacobi debe ser aplicado a la
derecha y no a la izquierda.
%
Escribimos un flux_fun para Euler y estoy debuggeando. La condicion
absorbente anda bien, pero ahora 

%===Mon Apr 10 09:41:21 ART 2000
Estoy convirtiendo nsi_tet a fastmat. 
%
El tema de la memoria, en la cual la nueva version "aparentemente"
consumia mucho mas memoria, no parecia dar problemas en spider, con
una malla de 60x60. Esto lo dejo para despues. 

%===Mon Apr 17 09:58:34 ART 2000
La conversion ya anda. Mientras tanto tuve que escribir bastantes
nuevasrutinas de fastmat y corregir otras. Ahora voy a comparar
directamente con la version existente en el cluster. 

%===Mon Apr 17 19:03:26 ART 2000
Anda la version fastmat!! Converge identicamente con la version
".tempo" que corria en el cluster cuando desactivo el
precondicionador. 

%===Mon Apr 17 19:18:36 ART 2000
Parece que anda bien con precondicionador y tambien version fastamt
pura. Antes estaba probando con newmat emulando a fastmat. Voy a tomar
tiempos. 

%===Mon Apr 17 20:21:09 ART 2000
Tome tiempos. La version fastmat es 2.8 veces mas rapida que la
newmat. Esta relacion dio igual para quads 2D (elemento nsi_tet) para
chunk_size 20 que para 100 y los tiempos dierons practicamente igual,
lo cual indica que el "over_head" por pocesar por chunks es bastante
bajo. 

%===Mon Apr 17 20:31:53 ART 2000
Ya que estamos tocando el tema tiempos, con chunk_size=1 da 5.34
segundos contra 4.66 para chunk_size=1=100, lo cual dice que el
overhead es pequenho con respecto a elementos 'costosos' como
Navier-Stokes. 
%
Tambien la relacion 2.8 de newmat/fastmat podria mejorar en elementos
menos costosos. 

%===Mon Apr 17 21:55:16 ART 2000
Estoy tratando de debuggear el tema de la memoria. La nueva version
pierde memoria por paso de tiempo. 
%
Nueva version:
%
Total memory (sbrk) [arena]: 208056 b
Total memory (sbrk) [arena]: 961720 b
Total memory (sbrk) [arena]: 1178808 b
Total memory (sbrk) [arena]: 1932472 b
Total memory (sbrk) [arena]: 2677944 b
Total memory (sbrk) [arena]: 3423416 b
Total memory (sbrk) [arena]: 4172984 b
%
%==============================
Vieja version;
%
Total memory (sbrk) [arena]: 211732 b
Total memory (sbrk) [arena]: 944916 b
Total memory (sbrk) [arena]: 1698580 b
Total memory (sbrk) [arena]: 1698580 b
Total memory (sbrk) [arena]: 1698580 b
Total memory (sbrk) [arena]: 1698580 b
Total memory (sbrk) [arena]: 1698580 b
%
(Eso es medio curioso, porque no
pierde en Euler? Sera porque no se ensamblan matrices?) Una cosa que
puedo probar facilmente es cambiar el chunk_size. Ahora esta en 100.

%===Mon Apr 17 21:59:30 ART 2000
No, definitivamente no es el chunk_size... Estaba en 1. 
%
Bueno... Voy a probarlo en 100, a ver si cambia...

%===Mon Apr 17 22:06:32 ART 2000
El chunk_size no es. Pero si al bajar la dimension del espacio de
Krylov, baja la memoria que se pierde por iteracion!!

%===Mon Apr 17 22:12:01 ART 2000
Ahora no pierde memoria. 
%
Version nueva:
%
Total memory (sbrk) [arena]: 962360 b
Total memory (sbrk) [arena]: 1232696 b
Total memory (sbrk) [arena]: 1232696 b
Total memory (sbrk) [arena]: 1232696 b
Total memory (sbrk) [arena]: 1232696 b
Total memory (sbrk) [arena]: 1232696 b
%
La version vieja se comporta en forma similar pero con una alocacion
total de 1338292 b. La diferencia se puede deber a los chunks, que
ahora se aloca mejor. (???)

%===Thu Apr 20 09:33:23 ART 2000
Ahora coinciden bien la version nueva (con newmat) con la vieja,
incluyendo el termino de shock-capturing para la
incompresibilidad. (Termino nuevo que agrego Beto). 

%===Thu Apr 20 12:34:34 ART 2000
Ahora la cavidad chica anda bien hasta la ultima cifra decimal. 

%===Fri Apr 21 20:46:24 ART 2000
Voy a mofidifcar assemble para que calcule jacobianos por diferencias
finitas. Por ahora los hago en una rutina aparte
assemble_prof. Despues voy a unificar las dos funciones. 
%
Modifique el Makefile para que tenga un target depend: que corre el
makedepend. 

%===Sun Apr 23 11:27:27 ART 2000
Reestructure los makefiles. Ahora hay un Makefile.base en el
directorio raiz, y en este o bien se definen las variables o se
pueden definir en un  Makefile.defs en el directorio padre del raiz. 
%
Ahora voy a probar si anda el jacobiano numerico con Navier Stokes. 
%
Tambien hay que probar que ande con varios procesadores. 

%===Sun Apr 23 15:44:38 ART 2000
Parece andar bien el Jacobiano con Navier-Stokes. No da exactamente
igual, pero me imagino que se debe a que en realidad el jacobiano
analitico no es completamente exacto. Voy a probar que el
assemble_prof() sin calcular jacobianos numericos de igual que el
assemble(). 
Salvo version 1.2 2000/04/23 (spider)

%===Sun Apr 23 17:28:45 ART 2000
La version assemble_prof() da igual que la assmeble(). 
Salvo version 1.3 2000/04/23 (spider)

%===Sun Apr 23 17:37:06 ART 2000
Con dos procesadores tambien da bien. 

%===Sun Apr 23 21:47:15 ART 2000
Calcula bien perfiles llenos con assemble_prof() (jacobiano por
dif. finitas). Voy a probar con dos procs.

%===Sun Apr 23 23:07:24 ART 2000
Reiventa el calculo del jacobiano por diferencias finitas en mas de un
procesador. 
Salvo version 1.4 2000/04/24 (spider)

%===Tue Apr 25 18:22:55 ART 2000
Encontre una perdida de memoria. SI SE ALOCA CON NEW ENTONCES HAY QUE
BORRAR CON DELETE!!! En el destructor de Gpdata hacia  
 FM_shape[ipg]->~FastMat();
 FM_dshapexi[ipg]->~FastMat();
y habia que hacer
 delete FM_shape[ipg];
 delete FM_dshapexi[ipg];
al borrar las componentes FastMat. 

%===Tue Apr 25 19:47:13 ART 2000
Tiempos:
%
Para el cilindro (30000 elementos cuadrangulares) a Re=5000. Con nnwt=3
(iteraciones de Newton):
%
Version nueva: 1' por paso de tiempo, 20'' por subiteracion de
Newton. 12'' evaluacion del residuo y matriz (una sola llamada en la
nueva version) y 8'' de resolucion. evaluacion/total = 60%
%
Version vieja: 2'10'' por paso de tiempo, 43'' por subiteracion de
Newton. 35'' evaluacion del residuo y matriz (dos  llamadas en la
vieja version) y 8'' de resolucion. evaluacion/total = 80%
%
evaluacion del residuo (vieja/nueva) -> 3.0
%
total de la subiteracion de Newton (vieja/nueva) -> 2.15

%===Wed Apr 26 10:17:03 ART 2000
Todas las corridas anteriores fueron con el cluster desbalanceado (6
procesadores, de los cuales uno es PII 350Mhz, otro PIII 450 Mhz y el
resto PIII 500 MHz). Ahora trajeron el 7mo procesador PIII 500
Mhz. Con lo cual probe a correr con 6 procesadores (todos los PIII y
dejando afuera el PII) y el tiempo baja a 52 sec por paso de tiempo,
17.5 por subiteracion de Newton. 

%===Sun May  7 10:33:09 ART 2000
Estoy introduciendo el balance de carga entre procesadores. Para eso
Metis tiene una serie de rutinas de mas bajo nivel como
METIS_WPartGraphKway donde la W indica que es pesado y se le pasa un
peso por procesador. La macana es que en esta serie de rutinas hay que
pasarle el grafo y no solamente las conectividades. De todas formas
aprovecho y eso me permite hacer que particione tomando la informacion
de varios elemsets, ya que voy a armar el grafo yo. 

%===Sun May  7 14:14:48 ART 2000
Anda la division con peso con METIS_WPartGraphKway. En la cavidad de
20x20, poniendo dos procesadores con peso 0.25,0.75 da una division
que agrupa unos 97 elementos en la esquina para el proc 1 y otros 303
para el otro proc 2. 
%
Ahora hay que particionar los nodos. 

%===Sun May  7 18:40:07 ART 2000
Anda bien la particion de nodos inducida. Ahora hay que verificar que
ande bien Navier Stokes en un solo procesador. Despues en dos
procesadores sin y con diferencia de carga. Despues con dos
procesadores y dos elemsets. 

%===Sun May  7 18:59:24 ART 2000
Coincide el calculo en un procesador y en dos procesadores. Ahora voy
a comparar con la vieja rutina read_mesh. 

%===Sun May  7 19:07:42 ART 2000
Coincide con la vieja read_mesh. Ahora voy a probar con dos
procesadores y dos fat elemset. 

%===Sun May  7 19:22:27 ART 2000
Anda bien con dos procesadores y dos fat_elemsets.

%===Fri May 12 11:53:09 ART 2000
Lo termine de implementar y lo corro en el cluster. En read_mesh busco
una opcion `proc_weights' en el thash `global_options' y eso define un
archivo donde estan los pesos de los diferentes procesadores
(normalmente el archivo se llama weights.dat'). Un
script de Perl llamado procsel lee una tabla `proctable' que contiene
procesador/peso (valocidad) y escribe `weights.dat' y `machi.dat' para
MPI. 

%===Fri May 12 12:01:44 ART 2000
Con balanceo de carga da 30 seg/iteracion mientras que sin balanceo da
39 seg/iter. Lo curioso es porque tarda 39 seg/iter cuando antes
tardaba 20 seg/iter. (nos referimos a subiters de Newton). 

%===Fri May 12 12:44:56 ART 2000
La discordancia de tiempos que pasaba antes es que habia un
emacs corriendo que se chupaba toda la CPU. Ahora, balanceado da 15
seg/iter (contra 18seg sin balancear). 

%===Fri May 12 19:25:24 ART 2000
Todavia tengo un quilombo con el assemble_prof y el assemble. Lo voy a
unificar. Hago un save en .tgz en geronimo.

%===Sat May 13 08:26:28 ART 2000
Habia algo que estaba medio mal. En read_mesh la parte en la que
hacia el da_sort() de los ghost_elems se hacia fuera del lazo sobre
los elemsets, de manera que lo hacia solo para el ultimo elemset
visitado y podria ocasionar que para los otros elemsets los
ghost_elems quedaran desordenados. 

%===Sat May 13 17:33:23 ART 2000
En read_mesh.cpp: hace falta que line sea pasado como argumento de
read_hash_table? Porque no que quede definido adentro?

%===Sat May 13 17:37:23 ART 2000
Voy a mover filestack a un archivo fstack.cpp/h

%===Sun May 14 20:48:03 ART 2000
Implemente las condiciones de contorno dependientes del tiempo. La
idea es que ahora cada `fixation_entry' contiene un registro
`Amplitude *amp'. Las fijaciones que no dependen del tiempo tienen un
puntero nulo. Para las que si dependen del tiempo el objeto apuntado
*amp contiene un string `char *amp_function_key' que identifica a la
funcion apropiada y una serie de propiedades en una hash table
`TextHashTable *thash'. Por ejemplo si la condicion es c+A*\sin(\omega
t+\phi) entonces la entrada en el archivo de datos es del tipo
%
fixa_amplitude sine
const_val <c>
amplitude <A>
omega <\omega>
phase <\phi>
__END_HASH__
<node> <field> <val>
<node> <field> <val>
<node> <field> <val>
...
__END_FIXA__
%
values enclosed in <> are appropriate  numeric values for the
quantities enclosed. El string en el objeto amplitude seria
`sine'. Cuando se llama a la rutina que calcula residuos, esta va
localizando los valores del vector de estado en la rutina
`download_vector'. Cuando los valores nodales estan fijos, estos se
calculan de las fijaciones. Si dependen del tiempo, el tiempo es
pasado desde el main a las rutinas de mas bajo nivel por un `void
*time_data' y finalmente es pasado a la funcion. Las funciones
dependientes del tiempo se registran en una table `function_table'
(miembro estatico de la clase `Amplitude'). Los de la amplitud
dependiente del tiempo se calculan por `amp->eval(time_data)', donde
`eval' es un miembro de la clase amplitude. 
%
Renegue bastante con un problema que tuve con el miembro estatico
`static FunctionTable *function_table' de la clase `Amplitude'. La
regla es que para acceder a estos miembros estaticos sin tener que
referenciar a ningun objeto de la clase hay que hacerlo a traves de
funciones tambien definidas como estaticas. Por otra aprte, las
funciones estaticas no pueden acceder a los miembros no
estaticos. Para llamarlos hay que usar el scope operator, por ejemplo
`Amplitude::ad_entry("sine",&sine_function);". Ademas recordar que los
miembros estaticos hay que inicializarlos!! Sino no los crea y despues
en la etapa de linkedicion da un "undefined reference", por ejemplo
`FunctionTable *Amplitude::function_table=NULL;' en `dofmap.cpp'. 
No hay que inicializarlos en un header, porque sino da como una
definicion multiple. O sea, la inicializacion hay que pensarla como la
definicion de una funcion. Como no sabia como inicializar un
`map<string,AmplitudeFunction *>' defini al miembro `function_table'
de la clase `Amplitude' como un puntero a una `FunctionTable' que se
puede inicializar simplemente como `NULL'. 

%===Sun May 21 19:41:20 ART 2000
Voy a agregar un tipo de argumento "USER_DATA" de tipo (void *).

%===Wed May 24 13:18:07 ART 2000
Fijado un "leak memory" en advective.cpp. Al final del archivo, al
borrar A_jac y A_jac_av hay que hacer `delete' en vez de `~Matrix()' y
ademas el contador iba hasta `j<ndim' en vez de `j<=ndim'
%
  for (int jd=1; jd<=ndim; jd++) {
    delete A_jac[jd-1];
    delete A_jac_av[jd-1];
  }
. 

%===Thu May 25 18:24:43 ART 2000
Implementado el auto_time-step. Ahora voy a implementar el 
local_time_step. 

%===Thu May 25 19:42:57 ART 2000
Parece andar el local_time_step. Ahora voy a pasar las 
opciones globales (mesh->global_options) como una
variable global. 

%===Thu May 25 20:11:09 ART 2000
Abandono con lo de pasar las opciones en forma global. Voy a tratar de
poder usar todos lo sistemas advectivos al mismo tiempo con clases
derivadas.

%===Sat May 27 10:11:25 ART 2000
Finalmente pude hacer andar el euler de manera que ahora se puede
correr euler, shallow-water, etc... dentro del mismo programa. Eso lo
hice con clases derivadas. 

%===Sat May 27 11:21:50 ART 2000
Changed `read_mesh.cpp' to `readmesh.cpp' in order to have filenames
8.3 compatible and flux_fun...cpp o ff....cpp. 

%===Sat May 27 11:51:34 ART 2000
Rewritten the TAGS target in the makefiles. 

%===Wed Jun  7 20:47:17 ART 2000
Added description of advective elemsets to the doc. 

%===Fri Jun  9 12:02:34 ART 2000
Escribiendo la documentacion de funciones temporales. 

%===Sat Jun 10 14:11:16 ART 2000
Cambio los nombres de los archivos en laplace: el main es `laplace.cpp' y
el elemento es `lapla.cpp'. 

%===Sun Jun 11 13:40:59 ART 2000

Corregi un bug serio en la lectura de datos con get_double and
friends. En realidad el problema era en la alocacion con `new
char'. En varios lados estaba `new_string = new char[strlen(string)]'
para despues copiar `string' en `new_string'. No se estaba reservando
un lugar para el trailing NULL. (Esto es un error clasico!!!). Lo
reemplaze en todos lados por `new_string = new
char[strlen(string)+1]'.

%===Sun Jun 11 13:44:02 ART 2000
Estoy escribiendo un caso test para funciones dependientes del
tiempo. 

%===Sun Jun 11 22:00:51 ART 2000
Cambie "read_mesh.h" por "readmesh.h". Estoy recuperando la rutina
`print_some'. 

%===Mon Jun 12 21:30:25 ART 2000
Estoy debuggeando un efecto extranho. En geronimo no anda bien el test
`sector.dat'. Mientras que en spider anda bien. En geronimo
aparentemente no procesa 3 de los 100 elementos. Antes estaba con
jacobiano por diferencias y ahora lo puse analitico. Sera eso?

%===Mon Jun 12 21:37:46 ART 2000
No. En casa da bien con jacobiano analitico y numerico. Tambien con
chunks de 2, 200 y default (creo que 20).

%===Tue Jun 13 13:40:03 ART 2000
Encontre el error. Los datos de `sector.dat' estaban medio mal, es
decir en ciertos nodos que estaban fijos ademas le imponia condiciones
periodicas. 

%===Tue Jun 13 22:47:31 ART 2000
Arregle el error, pero de todas formas no lo pude verificar bien
porque lo corri en minerva, pero donde andaba mal era en geronimo. Lo
que hago ahora es que cuando perm[] del nodo fijado da 0, entonces no
remapeo la fijacion. 
%
Defino una clase TimeData que es general y otra Time que es un
doble. TimeData es mas general y podria tener cualquier cosa
adentro. Pero de todas formas me da la sensacion que no lo estoy
haciendo bien. Deberia hacerlo con clases virtuales o algo por el
estilo.

%===Fri Jun 23 09:45:16 ART 2000
Estoy escribiendo la libreria FastMat2. 
%
Hasta ahora tenia una rutina sync_dims que sincronizaba las
dimensiones de la matriz. Ahora voy a ser que devuelva las
dimensiones, asi no hay que dejar de declarar `const' al objeto. 

%===Tue Jun 27 21:16:22 ART 2000
Borro location_abs(). Creo que no se esta usando. 

%===Wed Jun 28 10:47:41 ART 2000
Anda bien ir() e is(). Ahora voy  a hacer que is(j) sin mas argumentos
limpia el filtro de la coordenada j. 

%===Wed Jun 28 22:45:31 ART 2000
Estaba mal el dimensionamiento de A_jac
%
A_jac_FM2(3,ndim,ndof,ndof);

%===Fri Jun 30 21:50:09 ART 2000
Ahora desde el advecive.cpp escrito en Newmat corre el ffeuler escrito
en FM2 a traves de una rutina de adaptacion ffadapfm2.cpp. Ahora voy a
probar que ande bien con Newmat y despues con FM2. 

%===Sat Jul  1 20:35:42 ART 2000
Andan los cache!! Hasta ahora andan para las operaciones`'one to
one'. El paso siguiente es escribirlas para las operaciones setel,
addel, scaleel. 
%
Para una operacion de 3x3 de copy, (set) y mult (multiplicacion
elemento a elemento) da unos 8 Mflops. Para chunks de 10 baja a 7. 

%===Wed Jul  5 11:45:53 ART 2000
Anduve mirando tiempos en FastMat2 y anda bien, en algunos casos es
unas 10 veces mas rapida que Newmat. Pero el overhead es realmente
malo, hay que ir hasta 100 o 1000 veces ejecutar con cache para que
deje de incidir el tiempo de armado del cache. Voy a escribir una
mejor clase Indx. 

%===Wed Jul  5 19:07:22 ART 2000
La clase Indx esta mejor escrita. Es un vector de longitud MAXINDX,por
ahora no hay chequeo de "out of bounds". FM2 da en el problemita de
hacer c = a(:,1)*a(1,:) 5.55 Mflops y Newmat 0.64Mflops. 
El punto de corte anda en Nin=100. (Antes estaba en 1000 o peor). 

%===Thu Jul  6 12:55:26 ART 2000
Agregue un conteo de operaciones en el cache_list. 

%===Sun Jul  9 12:59:57 ART 2000
Andan las listas de cache con branching, anda el conteo de
operaciones.

%===Mon Jul 10 19:44:07 ART 2000 mstorti@node1.beowulf.gtm
;(defalias 'notas-insert-date (read-kbd-macro
;			      "=== <<date>> C-e SPC C-u ESC !uname SPC -n RET <down> C-k \
;<up> C-e 3*<C-left> C-u ESC !whoami RET <down> C-a DEL @ C-e <down>"))
Nueva definicion de notas-insert-date. (ver arriba). 
%
En set(double *) y export(double *) tuve que hacer que si la matriz no
esta definida entonces no exporta nada. 

%===Wed Jul 12 06:18:24 ART 2000 mstorti@localhost.localdomain
;(defalias 'notas-insert-date (read-kbd-macro
;         "%=== <<date>> C-e SPC C-u ESC !whoami RET C-e @ C-u ESC !uname \
;SPC -n RET C-a <down> 2*C-k"))
New-new version of notas-insert-date. 
%
tau_supg is not reshaped now. If it is scalar it is passed in position
1,1, the rest of the matrix is ignored, and this is flagged by the
writer of the flux_fun routine through a bit filed in int variable
'ret_options' (options for return values).

%===Wed Jul 12 06:50:16 ART 2000 mstorti@localhost.localdomain
Voy a escribir una clase de matrices derivada de FastMat2 pero solo
para dos dimensiones. La llamo FMatrix. 

%===Wed Jul 12 22:26:23 ART 2000 mstorti@localhost.localdomain
Escribo double FastMat2::sum_square_all(). El truco es tener una
matriz estatica escalar A adentro y entonces adentro se llama a
A.sum_square(*this). Ahora lo voy a hacer para todos los otros. 

%===Thu Jul 13 10:42:33 ART 2000 mstorti@node1.beowulf.gtm
Acepto que una de las dimensiones al crear una matriz sea nula. En ese
caso hago todo menos hacer el `new' y setear la bandera `defined' a 1.

%===Thu Jul 13 12:58:07 ART 2000 mstorti@node1.beowulf.gtm
Habia un bug en cuanto a que no se deleteaba el cache en toda una
serie de rutinas cuando no se estaba usando la opcion de guardar el
cache. 

%===Thu Jul 13 13:10:05 ART 2000 mstorti@node1.beowulf.gtm
FM2 sin cache da una iteracion del naca (2429 elementos) en 2' (a 50%
CPU), es decir 24 [sec/1000 elem]. 
%
No pierde memoria! 
%
Ahora voy a ver cuanto da Newmat y despues FastMat con Cache.

%===Thu Jul 13 13:15:00 ART 2000 mstorti@node1.beowulf.gtm 
Con Newmat da 38'' el mismo caso, con la misma distribucion de
carga. Es decir una relacion 3 a 1. Newmat estaria dando 8 [sec/1000
elem].

%===Thu Jul 13 18:30:21 ART 2000 mstorti@node1.beowulf.gtm
Con FastMat2 cached da 12 seg (a 50% CPU), es decir 2.5
[sec/1000elem].

%===Fri Jul 14 08:48:09 ART 2000 mstorti@node1.beowulf.gtm
Algo anda mal en las medidiones de tiempo. Por empezar el upload y
download de vectores se lleva mucho tiempo, asi que voy a tratar de
medir tiempo haciendo muchas llamadas a assemble solamente. 

%===Fri Jul 14 09:38:26 ART 2000 mstorti@node1.beowulf.gtm
%
Dentro del debugger y con -pg activado da 1 sec/Ke para assemble fon
FM2. Fuera del debugger  0.968273 sec/Ke.
%
Newmat: 7.404418 sec/Ke. 
%
FM2 sin el -pg para profiler 0.609000 sec/Ke. 
%
Newmat sin el -pg:
total 32.270000, ntimes 10, nelems 500, rate 6.454000 [sec/1000/elems/iter]
%
Overhead en FM2: para chunk_size=100 la tasa baja a 0.825000 sec/Ke.
Para chunk_size=30, la tasa es de  1.433333 sec/Ke. 
%
El ajuste por minimos cuadrados da t [sec] = 0.026 + 5.55e-3 * (Nro de
elementos), lo cual da un overhead equivalente a 47 elementos. La
eficiencia para unos 500 elementos estaria en 10/11 = 91% aprox.
%
Toda la rutina assemble en FM2:
total 58.310000, ntimes 10, nelems 2429, rate 2.400576
[sec/1000/elems/iter]. Quiere decir que le agrega unos 1.8 sec sobre
los .6 que requiere la rutina advective con fm2.
%
Toda la rutina assemble en Newmat:
total 195.340000, ntimes 10, nelems 2429, rate 8.041993
[sec/1000/elems/iter]. Quiere decir que le agrega unos 1.6 sec a los
6.45 de la rutina de Newmat. Coincide con la estimacion del overhead
de upload/download para FM2.

%===Fri Jul 14 13:14:34 ART 2000 mstorti@node1.beowulf.gtm
%
Voy a escribir versiones "rapidas" de los containers `map' y
`vector'. Vector ya casi esta, (lo tomo de Indx) y map se va a basar
en vector. 

%===Sat Jul 15 12:10:17 ART 2000 mstorti@localhost.localdomain
Ya andan las versiones rapidas. Para adv con chunk_size=500 da 0.93
sec/Ke el assemble cuando el advective solamente requiere 0.63 sec/Ke
significando que el overhead de element.cpp (upload/download) es de .3
sec/ke (32%). Esta mucho mejor pero de todas formas es muy elevado. 

%===Sat Jul 15 14:06:43 ART 2000 mstorti@localhost.localdomain
Con gprof:
total 423.870000, ntimes 100, nelems 2429, rate 1.745039 [sec/1000/elems/iter]
%
Quiere decir que con gprof se achancha bastante (sin gprof da .93
sec/Ke). Pero esto va a cambiar cuando ponga el shock-captring. 
%
El gprof canta que todavia estamos gastando la mitad del tiempo en
upload/download contra el tiempo de calculo puro. 

%===Sat Jul 15 17:12:27 ART 2000 mstorti@localhost.localdomain
%
Ahora el target $(PROG).bin esta en Makefile.base. Guarda! que si en
vez de `$(PROG).bin' pones `%.bin' entonces los `*.o' se convierten en
secundarios y los borra despues de compilar. Una posibilidad es poner
`.SECONDARY: $(MYOBJS)' pero parece ser mejor poner directamente como
target `$(PROG).bin'. 

%===Sat Jul 15 17:14:12 ART 2000 mstorti@localhost.localdomain
%
Verifico que la version rapida da bien los resultados en
euler/naca.epl. 
%
Sacando el Mat/VecSetValue() se llega a 0.897900 sec/Ke. Lo cua quiere
decir que el overhead por carga de valores no es sensible. 

%===Sat Jul 15 17:47:53 ART 2000 mstorti@localhost.localdomain
%
Verifico la ganancia con la nueva version de upload/download:
vieja version: 2.122684 sec/Ke. 
nueva version: 0.929601 sec/Ke. Factor de ganancia: 2.3 

%===Sat Jul 15 18:45:10 ART 2000 mstorti@localhost.localdomain
%
Verifico que no tiene perdidas de memoria.

%===Sun Jul 16 10:17:23 ART 2000 mstorti@localhost.localdomain
Con optimizacion da 0.474434 sec/Ke. Factor de ganancia: 1.98

%===Sun Jul 16 12:36:52 ART 2000 mstorti@localhost.localdomain
%
Implemento la funcion de flujo para shallow water.
%
Performance para shallow water:
total 16.160000, ntimes 50, nelems 200, rate 1.616000 [sec/1000/elems/iter]
%
Guarda que el numero de elementos es bajo. Despues voy a probar a
llevarlo a 500. Esta compilado sin profiler y con debugger.
%
Con Newmat
%
total 37.780000, ntimes 50, nelems 200, rate 3.778000 [sec/1000/elems/iter]
%
Mas de dos veces mas lento.
%
Guarda que el chunk_size estaba en 20!! Paso el CHUNK_SIZE a 200 en
`fem.h' y a 500 en plano.dat.
Con Newmat:
total 7.070000, ntimes 10, nelems 200, rate 3.535000
[sec/1000/elems/iter]
Con FastMat2 [chunk_size de 200]
total 0.820000, ntimes 10, nelems 200, rate 0.410000 [sec/1000/elems/iter]
Con FastMat2 [chunk_size de 500]
total 1.960000, ntimes 10, nelems 500, rate 0.392000 [sec/1000/elems/iter]

%===Sun Jul 16 19:55:00 ART 2000 mstorti@localhost.localdomain
%
Escribi el shock capturing en FastMat2. Da igual (a precision de la
maquina) que con Newmat. Ahora tengo que activar el cache. 

%===Sun Jul 16 20:22:18 ART 2000 mstorti@localhost.localdomain
%
Anda le euler con FastMat2 y caches. Es 5.7 veces mas rapido. 
Ahora voy a probar con la version optimizada. 
%
Hay un problema con los caches. Si hay un `if' sin `else' entonces
cuando no entra en el `if' entonces queda un cache desfasado. Por ahora
se arregla poniendo siempre un `else'.

%===Sun Jul 16 20:34:22 ART 2000 mstorti@localhost.localdomain
La version optimizada da 1.274000 sec/iter para FM2 y 
14.010000 sec/iter para Newmat. Pero guarda que Newmat no esta
compilado con optimizacion. 
%
Con optimizacion Newmat baja a 7.57 sec/iter con lo cual la relacion
baja a un factor 6.
%
La llamada a `assemble' solo da 3.03 sec/Ke con Newmat y 0.50 sec/Ke
con FastMat2,  o sea tambien una relacion de 6 a 1.
%
La llamada al elemento solo (`advective' o `advectiveFM2') da 2.98
sec/Ke con Newmat y 0.4 sec/Ke con FastMat2 o sea que la relacion sube
a 7.45. 
%
Tambien se ve que para FastMat2 el overhead de upload y download es de
un 20%. 

%===Mon Jul 17 18:12:33 ART 2000 mstorti@node1.beowulf.gtm
Habia un bug en los puntos de Gauss para triangulos. 

%===Tue Jul 18 13:26:18 ART 2000 mstorti@node1.beowulf.gtm
%
Habia dos bugs que morfaban memoria. 
%
1./ En fem.cpp compute_prof() habia que hacer el `da_destroy(da)'
*antes* de definir la matriz, sino estan ambas definidas al mismo
tiempo. 
%
2./ En `fstack.cpp' habia que poner la linea compiled=1 para que no
recompilara cada vec, lo cual ademas daba lugar a una perdidad de
memoria. 

%===Fri Jul 21 12:23:22 ART 2000 mstorti@node1.beowulf.gtm
%
Cuando quise compilar NS con el optimizador pow() me daba uns
SIGSEGV. Lo arregle usando cbrt() (raiz cubica) y un macro SQ() que
hace el cuadrado. 

%===Fri Jul 21 12:24:42 ART 2000 mstorti@node1.beowulf.gtm
%
Perdida de memoria en remap_cols(). Despues de renegar y renegar no
encontre el problema y resolvi escribir un nuevo remap_cols() que haga
el remap copiando en un nuevo idmap y entonces despues borra el viejo
y deja el nuevo. Esto va a ser mejor siempre, pero de todas formas no
queda en claro que porque se pierde memoria con el set_elem() y esto
puede ser peligroso en alguna aplicacion donde se use mucho. 

%===Fri Jul 21 17:43:16 ART 2000 mstorti@node1.beowulf.gtm
%
Hago un directorio `PETSC/RUPERTO/advective.ruperto' donde Ruperto
puede poner sus funciones temporales. 

%===Sat Jul 22 11:50:15 ART 2000 mstorti@localhost.localdomain
%
Informe final sobre el remap_cols() bug:
========================================
%
%
El efecto de este bug es que, despues de definir el idmap dofmap->id,
en readmesh, cuando hace la rutina `remap_cols()' que remapea las
columnas (grados de libertad) de forma de dejar las que corresponden
al procesador 0 primero, despues las del 1, etc... hasta nptoc-1, y
despues los fijos, entonces despues de remapear las columnas se morfa
una gran cantidad de memoria que despues no es liberada. No he llegado
a la raiz del problema pero he logrado los siguientes diagnosticos:
%
*./ La memoria perdida depende del tamanho maximo al cual ha llegado
los col_map y row_map y la suma de los row_t y col_t individuales, es
decir de todos los `map'. Esto parece ser un `bug' o por lo menos un
funcionamiento no deseado en el compilador o las librerias STL. 
%
*./ La memoria no se recupera ni siquiera destruyendo los objetos. Sin
embargo, si se vuelven a usar maps (eventualmente otros) la memoria es
reusada. Pareceria ser entonces que la implementacion de los maps
reserva una cantidad de memoria para para los maps que nunca es
liberada ni reducida, incluso si los maps son destruidos. 
%
*./ Hay un ejemplo tryme2.cpp que muestra esto.
%
*./ La solucion actual es cambiar el remap_cols() de manera que se van
cargando las filas del viejo idmap en un nuevo idmap y despues se
destruye el viejo. De esa forma la memoria perdida es nula porque no
se usan los maps ya que no hay columnas ni filas especiales. 
%
*./ Otra posibilidad seria reimplementar el idmap de otra forma, por
ejemplo tomando un `binary tree' de libretto, o algo mas pedestre
implementado por mi directamente. 

%===Sat Jul 22 12:48:05 ART 2000 mstorti@localhost.localdomain
%
Cambio la forma de actualizar makefile.d en los makefiles y como
salvarlos. Ahora los makefile.d no se guardan en `source.tara'. Y en
los Makefile se agrega dos lineas que dice "makefile.d: cat /dev/null
>makefile.d ; makedepend -f makefile.d ...". De esta forma se generan
los makefile.d cuando se necensitan. 

%===Sat Jul 22 20:37:38 ART 2000 mstorti@localhost.localdomain
%
Fixed a bug introduced during the change of remap_cols(). In
remap_cols the line `n = n_new;' should be `idnew.n = n_new;'. 

%===Sat Jul 22 20:46:59 ART 2000 mstorti@localhost.localdomain
%
Hay un problema en al correr el plano.dat en ./run. Se produce un
error al resolver por GMRES me parece que se debe a que en la nueva
version la matriz que debe retornar es nula. 

%===Sat Jul 22 21:18:38 ART 2000 mstorti@localhost.localdomain
%
Corregido el error. Habia quedado comentada una linea en advective.cpp
al agregar el beta_supg.

%===Sun Jul 23 06:29:41 ART 2000 mstorti@localhost.localdomain
%
Muevo el target %.depl de `run' y `test' a makefile.base.

%===Sun Jul 23 10:30:01 ART 2000 mstorti@localhost.localdomain
%
Agrego para det() e inv() el calculo via Newmat. Ahora voy a hacer
eig(). 

%===Wed Jul 26 20:06:12 ART 2000 mstorti@node1.beowulf.gtm
%
Descubrimos cual era el problema por el cual para problemas grandes
parecia que "duplicaba" la memoria. Se debia a que cuando haces
MatSetValues los elementos quedan en un cache que recien es mandado a
comunicacion cuando llamas a MatAssemblyBegin/End(). Entonces, si
tenes muchos elementos el cache es tan grande como la matriz. Es
preferible entonces hacer el MatAssemblyBegin/End() despues de cada
chunks o cada cierto numero de chunks. Pero eso obliga a sincronizar
el procesamiento de cada chunk si no se te desbalancea el calculo ya
que el MatAssemblyBegin/End impone una barrera MPI. Entonces
calculamos un tamanho local ideal del chunk de la forma
%
local_chunk_size = (int)(chunk_size*dofmap->tpwgts[myrank]/w_max) +1 ;
%
donde w_max es el maximo weight (maxima velocidad). Des esta forma el
tamanho de los chunks es proporcional a la velocidad del procesador y
en el de mayor velocidad (por lo tanto, el que tiene mayor tamanho de
chunk) es el deseado chunk_size. 
%
Ahora bien, por problemas de `redondeo entero' etc... puede ser que el
numero de chunks varie de procesador a procesador, pero todos tienen
que llamar a MatAssemblyBegin/End el mismo numero de veces asi que en
algunos procesadores hay que llamar a MatAssemblyBegin/End sin haber
procesado elementos en ese chunk, solo para sincronizar. Para detectar
si un dado procesador termino hacemos 
%
      int local_has_finished= (el_last==nelem-1);
%
Y despues para ver si todos terminaron hacemos un Allreduce
%
      int global_has_finished;
      ierr = MPI_Allreduce((void *)&local_has_finished,
			(void *)&global_has_finished,1,MPI_INT,
			MPI_LAND,PETSC_COMM_WORLD);

%===Fri Jul 28 11:04:08 ART 2000 mstorti@node1.beowulf.gtm
%
Habia un problema con el nuevo procesamiento por chunks. No andaba
(daba un SIGSEGV) cuando se corria en mas de un procesador y con
chunk_size grande de manera que todo entra en un solo chunk. Lo
arregle cambiando `local_chunk_size' por `chunk_size' en la linea
%
	if (iele_here == chunk_size-1) break;
%
en `elemset.cpp'.

%===Fri Jul 28 11:33:51 ART 2000 mstorti@node1.beowulf.gtm
%
Voy a resolver el problema del branch/leave en FM2. La idea es que
funcione asi;
%
...
spawn()
if (...) {
  choose(j1);
  ...
} else if (...) {
  choose(j2);
  ...
} 
leave();
...
%

%===Sat Jul 29 13:03:49 ART 2000 mstorti@localhost.localdomain
%
Convierto FastMat2 a listas variables de argumentos pero via
argumentos con valores defaults. La idea es que las llamadas son de la
forma
%
fun(int m_0=0,int m_1=0,int m_2=0,int m_3=0,int m_4=0)
%
Y se supone que nunca es llamada con un numero variable de argumentos
diferente de 0. Entonces el numero de argumentos se obtiene mirando
cual es el primer argumento diferente de 0. Escribo unos macros
ARG_LIST(type,name,default)  y
READ_ARG_LIST(name,indx,default,exit_label) que expanden a la lista de
argumentos y despues lo leen.

%===Tue Aug  8 22:08:08 ART 2000 mstorti@localhost.localdomain
Agrego la funcion `piecewise' (lineal a trozos) escrita por
Beto. Agrego un campo en la clase `Amplitude' de tipo `void *' que es
para que el usuario pueda calcular datos estaticos y dejarlos
ahi. Atencion que no es lo mismo que tener datos de tipo `static' ya
que entonces no podrian variar entre dos diferentes regiones que
tienen la misma funcion de tipo 'piecewise', por ejemplo, pero tienen
diferentes parametros.

%===Tue Aug 15 19:51:41 ART 2000 mstorti@node1.beowulf.gtm
Empezamos a implementar LES / Smagorinsky. 
%
Agregamos un coeficiente C_smag. Si es cero no se hace turbulencia.
Por defecto es 0.18
%
Problema del ducto ciruclar: Hay que agregarle una gradiente de
presion o fuerza por unidad de volumen. Le agrego una propiedad G_body
que es un vector de ndim componentes. Se define en la tabla de
propiedades globales. 
%
Defino un nuevo elemento `nsi_tet_les' que esta en el archivo
`nsitetles.cpp'. A la larga la version con y sin LES deberian convivir
en el mismo elemento pero por ahora mantengo los dos para poder
comparar etc. De todas formas no se si voy a tener que tocar el
elemento viejo por para agregarle el G_body.


%================================================================
%
% Mensaje enviado por beto.
%
Mario,

aqui te mando como attachment un fichero en Matlab que genera las
  - coordenadas [x3d]
  - conectividades [ico3d]
  - periodical bc [peri]
  - fijaciones [fixa]

Es una feta en z (dos capas, una entrante y la otra periodica saliente) y
dos fetas en theta (circumferencial) para evitar un elemento triangular en
el centro. En la direccion radial le especificas a la rutina Nr (nro de
elementos radiales).

Como condiciones de contorno,
 -  a la entrada u=v=0 en toda la feta entrante (por ende al ser periodica
la saliente tambien es asi)
 - a la entrada w = 0 en la pared
 - a la entrada w = 1 en el centro
 - presion libre en todos lados
 - a la salida todo es periodico con la entrada

OJO:
Al mediodia me avive que vos llamaste nu_t en el programa y eso habria que
sumarlo al VISCO, no?

Suerte

Beto

%===Wed Aug 16 13:36:37 ART 2000 mstorti@node1.beowulf.gtm
%
Esta empezando a andar el flujo en un ducto en regimen laminar. La
velocidad maxima me da 0.243 menor que la exacta (0.25). Pruebo a
sacarle el upwind (tau_fac=0.) y no da mejor. Voy a probar a disminuir
la abertura de la feta de elementos. 
%
Debemos tener un archivo de inicializacion con un perfil parabolico y
maxima velocidad 1 y entonces para un dado Re hay que poner nu=1/Re y
dp/dz= 4/Re

%===Wed Aug 16 17:43:13 ART 2000 mstorti@node1.beowulf.gtm
%
Ahora, normalizado e inicializado desde el perfil parabolico da perfecto.

%===Wed Aug 16 18:14:26 ART 2000 mstorti@node1.beowulf.gtm
%
Cuando usas pasos de tiempo grandes no podes seguir la evolucion
temporal. No es fisicamente correcta. Parecia que en el centro del
ducto evolucionaba mas lento que en r=0.5 pero se debia al efecto ese
del paso de tiempo demasiado grande. 

%===Wed Aug 16 19:02:20 ART 2000 mstorti@node1.beowulf.gtm
%
El lazo exterior de NS chequeaba convergencia para problemas
estacionarios a sobre ||delta u||. Lo pase a que chequee sobre ||R||. 

%===Thu Aug 24 13:19:00 ART 2000 mstorti@node1.beowulf.gtm
%
Habia un error en la implementacion del Crank-Nicholson. Primero, en
la linea 634 en vez de 	"FMaxpy(matlocmom,alpha*nu_eff,tmp7);" va sin
el alpha porque el alpha solo va en todas las lineas escaleando al
Dt. Ademas el tiempo que hay que pasarle a `assemble' es t* no
t^{n+1}, asi que definimos una variable  `Time time_star;' y en las
llamadas dentro del lazo hay que pasarle `time_star'. 
%
Guarda!! Ademas en realidad cada vez que se pasa un estado habria que
pasarlo con el tiempo asociado. Es decir pasar {u^n,t^n} y
{u^n+1,t^n+1}. 
%
De todas formas el Crank-Nicholson tiene ahora una convergencia
$\propto \Dt^2$. 

%===Thu Aug 24 18:57:41 ART 2000 mstorti@node1.beowulf.gtm
%
Cuando agregue LES dejo de converger en la primera subiteracion lineal
debido al tratamiento diferente que se le hace al tensor de tensiones
que en el caso tubulento se pone como 1/2 (grad_u+ grad_u_t). Ahora
corregi tambien el jacobiano y da todo bien. Por supuesto como el
problema es no-lineal la convergencia no es en una iteracion pero de
todas formas converge rapido (a Re pequenhos). 

%===Thu Aug 24 19:43:38 ART 2000 mstorti@node1.beowulf.gtm
%
Performance para NS: Problema del qbend, 
%* 51,129 elementos. 
%* Corriendo junto con el adv.bin de Ruperto. 
%* Krylov_dim 150
%
Da 6' por iteracion, es decir 284 elem/sec. 

%===Thu Aug 24 20:11:35 ART 2000 mstorti@node1.beowulf.gtm
%
Pruebo con la version 1.48g que supuestamente es la que estaba activa
cuando corria el qbend. Con esa da 5' contra 6'. Sera el chunk_size?

%===Thu Aug 24 20:35:59 ART 2000 mstorti@node1.beowulf.gtm
%
Con la version 1.45g y con un chunk_size de 1000 tambien da del mismo orden (4'50''). 

%===Thu Aug 24 20:53:25 ART 2000 mstorti@node1.beowulf.gtm
%
Con la version actual (seria la 1.63g) da 5'50'' por iteracion (igual
que antes) con chunk_size=1000 o sea que pareceria ser independiente
del chunk_size.

%===Fri Aug 25 08:47:46 ART 2000 mstorti@node1.beowulf.gtm
%
Con Dt=500 se va a la mierda. Claro, no hay que basarlo sobre el
Fourier sino sobre el Courant. 

%===Fri Aug 25 11:50:59 ART 2000 mstorti@node1.beowulf.gtm
%
Empiezo a escribir el nsi_tet con FM2. Escribo una funcion identidad
`eye()'. 

%===Fri Aug 25 19:56:26 ART 2000 mstorti@node1.beowulf.gtm
%
Tuve que corregir la funcion rs() en FM2 ya que no reseteaba la
permutacion de indices.

%===Sat Aug 26 15:29:31 ART 2000 mstorti@localhost.localdomain
%
Hice otro casito test que es la cavidad cuadrada con 5 elementitos por
lado. Ahora lo uso para verificar que nsi_tet_les de lo mismo que
nsi_tet_les_fm2. 
%
Ahora hay que agregar los cache para FM2. 

%===Wed Aug 30 12:28:17 ART 2000 mstorti@node1.beowulf.gtm
%
El cache anda al pelo.  Estoy mirando el tema eficiencia. En casa me
daba que FM2 no era mas eficiente que FM, pero ahora aca en geronimo
da 3 veces mas rapido (sin optimizacion). Voy a ver ahora con
optimizacion. 3 sec/Ke con FM y 1 sec/Ke con FM2.
%
Con optimizacion da .28 sec/Ke con FM2 y .655 sec/Ke con FM. 
%
Guarda que en estos resultados el `loop gordo' con los `matij' estaba
comentado!! 
%
Incluyendo el lazo `matij' da 1.72 sec/Ke con FM2 y 1.43 sec/Ke para
FM. Mas o menos como daba en casa.
%
Ahora voy a sacar el gprof (opcion -pg)
%
Sin -pg da .65 sec/Ke para FM y 1.1 con FM2. 

%===Wed Aug 30 13:40:40 ART 2000 mstorti@node1.beowulf.gtm
%
Con el lazo `matij' vectorizado FM2 pasa a ser ligeramente mas rapido
que FM. 

%===Fri Sep  1 18:58:45 ART 2000 mstorti@node1.beowulf.gtm
%
Retomo NS+LES en FM2. Parece haber un error en la version FastMat de
`nsitetles.cpp'. Ahora dan iguales, pero me preocupa ese error que
habia en la version FastMat2. 

%===Tue Sep  5 11:17:57 ART 2000 mstorti@node1.beowulf.gtm
%
Detecto bug: En advective se detecta que cuando se ponen constraints
con coeficientes nulos da mal.

%===Wed Sep  6 10:57:54 ART 2000 mstorti@node1.beowulf.gtm
%
Fixed a serious bug in `elemset.cpp'. Temporary arrays (locst, retval)
were not destroyed for all elemsets, but only for the last.
%
Los delete[] de locst, pref, retval, etc... hay que hacerlos para cada
elemset, despues del lazo sobre los chunks. 

%===Wed Sep  6 22:57:38 ART 2000 mstorti@localhost.localdomain
%
Escribo la version weak_form=1 en advecfm2. Agrego en bcconv_adv el
termino H para shallow water. Tambien lo agrego a absorb (antes pasaba
basura y generaba un hterm cualquier cosa, guarda esto era una
bug!!). 
%
Agrego opciones __REWIND__, __FORWARD__ y __BACKWARD__ en
`myexpect.pl'. 

%===Fri Sep  8 20:12:38 ART 2000 mstorti@node1.beowulf.gtm
%
Arreglado un bug serio en la clase `idmap'. En `void
idmap::get_row(const int,IdMapRow &)' no se ponia la fila a vacio en
el caso `j==0', es decir ahora es:
%
  } else if (j==0) {
    row.resize(0);   // linea agregada!!
    return;
  } else {

%===Mon Sep 11 12:37:40 ART 2000 mstorti@node1.beowulf.gtm
%
`ffeuler.cpp' no compila con optimizacion asi que agregue una regla de
compilacion especial para ese archivo en el Makefile para que siempre
lo compile sin optimizacion. 
%
ffeuler.o: ffeuler.cpp
	-${CC} -c ${COPTFLAGS} ${CFLAGS} ${CCPPFLAGS} -O0 $<

%===Tue Sep 12 11:23:20 ART 2000 mstorti@node1.beowulf.gtm
%
Hay una pequenha asimetria en el codigo advectivo/shallow. Eso se debe
a que, cuando se usa local_time_step, el Jacobiano para calcular el
paso local de la malla, para escalear la matriz de masa se tomaba del
ultimo punto de Gauss visitado. Ahora se hace con un promedio de los
jacobianos. Ahora da perfectamente simetrico. 
%
El ||delta u|| parecia no converger, porque faltaba eliminar la
proyeccion de las condiciones absorbentes. Ahora formo el delta_u
despues de la proyeccion y converge muy bien. 

%===Tue Sep 12 13:25:13 ART 2000 mstorti@node1.beowulf.gtm
%
Voy a hacer un test para el bug de los `constraint'.

%===Tue Sep 12 18:46:51 ART 2000 mstorti@node1.beowulf.gtm
%
BUG: No estaba declarado estatico el regex en get_string(). 
Debia ser "static regex_t regex[2];" en getprop.cpp linea 109. 
Se colgaba cuando se hacia mas de un `get_string'. 

%===Tue Sep 12 19:29:54 ART 2000 mstorti@node1.beowulf.gtm
%
Hecho el test para el constraint bug. Esta hecho en un subdirectorio
de /test (esto es nuevo). 

%===Fri Sep 15 19:17:46 ART 2000 mstorti@node1.beowulf.gtm
%
Instale ANN (Approximate Nearest Neighbor) version 0.2 para LES. 
Hice un programa de verificacion (nneighbor/tryme.cpp) para verificar
los tiempos. Lo comparo con fuerza bruta (comparar con todos). 
%
ANN: total cpu_time: 2.010000, per point: 2.01e-09 , nnod: 10000, npoints: 100000
Fuerza bruta: total cpu_time: 5.080000, per point: 5.08e-07 , nnod:
100, npoints: 100000
%
O sea un factor 250 por lo menos. 

%===Mon Sep 18 08:27:14 ART 2000 mstorti@localhost.localdomain
%
Agrego al elemset un `double * elemprops_add' e `int * elemiprops_add'
para poner `propiedades adicionales', esto es, que son usadas
internamente por el `application writer'. Cuantos dobles o enteros son
necesarios es manejado por el campo `additional_props' y
`additional_iprops'. 

%===Tue Sep 19 11:24:23 ART 2000 mstorti@node1.beowulf.gtm
%
Estadistica de tiempos para back_step: (corriendo Beto). 
10,400 elementos, 2D con nsi_tet_les_fm2. Tarda 20 seg/subiter de
Newton. Es decir 60seg por iter del lazo exterior. Compitiendo con
RUperto. Sin ek procesador 5. Es decir que 
%
* considerando una potencia instalada de 5.65 procs. (tomando como base
el PIII  500Mhz). 
* 50% de uso efectivo (el otro 50% para Ruperto). 
%
El costo es de 5.4 [sec*proc/subiter/1000 elem]
%
Quiere decir que 100,000 elementos con una potencia instalada de 30
(14 PIII 700Mhz + los 6.65 PIII 500Mhz actuales) tendriamos un tiempo
de 18 sec/subiter. 

%===Tue Sep 19 17:05:32 ART 2000 mstorti@node1.beowulf.gtm
%
Sin correr Ruperto da 30 seg/iter lazo exterior como previsto. De
todas formas esta raro que la carga da un poco desbalanceada, en el
sentido de que con `$ bw uptime' da cargas fluctuando entre 60% y 90%
entre los diferentes procesadore. 

%===Thu Sep 21 10:54:13 ART 2000 mstorti@node1.beowulf.gtm
%
Debuggeando el elemento de pared LES `wall' encontre que los tiempos
de ensamblaje de la matriz se disparaban (como cuando calculas mal el
perfil). El perfil estaba bien calculado pero por eror estaba llamando
a `assemble' con `jobinfo="comp_wall_stres"' pero con un argumento
"A_tet,OUT_MATRIX" lo cual hacia que ensamblara la matriz. 

%===Thu Sep 21 11:16:08 ART 2000 mstorti@node1.beowulf.gtm
%
Para que de un warning cuando no alocaste bien la matriz (esto es,
cuando no calculaste bien el perfil) hay que poner:
%
  ierr =  MatSetOption(A_tet,MAT_NEW_NONZERO_ALLOCATION_ERR);

%===Sun Sep 24 18:24:36 ART 2000 mstorti@localhost.localdomain
%
Parece estar andando la implementacion de LES, por lo menos en cuanto
a la condicion de pared vista como condicion de contorno mixta. Si la
funcion de pared es lineal, es decir $\tau_w \propto u$ entonces da
convergencia en una iteracion. Si es no lineal, entonces da
convergencia cuadratica. 

%===Fri Oct  6 14:11:36 ART 2000 mstorti@node1.beowulf.gtm
%
Hago que FastMat2::print retorne void. Agrego una funcion
FastMat2::printd() que imprime las dimensiones de la matriz. 

%===Fri Oct  6 19:25:43 ART 2000 mstorti@node1.beowulf.gtm
%
Found serious bugs in 'FastMat2::reshape()'. It was missing a
`set_indx = Indx(ndims,0);' in order to reset the index restrict
vector, and a
%
   IndexFilter pp;
   for (int jd=0; jd<ndims; jd++) {
     ...
     dims[jd] = pp;
     ...
   }
%
in order to reset the filters for each dimension. 
%
Comment: this stuff with the 'store' = the fixed store, etc.. is
somewhat dangerous!! It should be rewritten properly!!

%===Mon Oct  9 17:17:50 ART 2000 mstorti@node1.beowulf.gtm
%
Agrego una bandera `shock_capturing_threshold' para regular cuando se
hace shock_capturing o no. 

%===Mon Oct  9 19:45:40 ART 2000 mstorti@node1.beowulf.gtm
%
Habia un bug en el macro `SGETOPTDEF_ND' (fem.h). No seteaba la
variable al valor default. 

%===Sun Oct 22 19:02:42 ART 2000 mstorti@localhost.localdomain
%
Ahora get_int retorna 1 si la entrada no tiene "miembro derecho" es
decir `print_internal_loop_conv' es equivalente a
`print_internal_loop_conv 1'. 

%===Mon Oct 23 19:11:47 ART 2000 mstorti@node1.beowulf.gtm
%
Abandono AFS y retomo LES. Da un error extranho cuando NP>1. Ahora me
doy cuenta de que parece que no puedo hacer lo de "procesar todos los
elementos" en cada procesador porque algunos no van a tener los datos
apropiados. 

%===Tue Oct 24 08:36:15 ART 2000 mstorti@node1.beowulf.gtm
%
Volviendo a hacer los calculos para "comp_shear_vel" por separado y
despues haciendo un "communicate_shear_vel" anda bien. Daba un SIGSEGV
pero era porque habia un FastMat2::deactivate_cache() cuando el cache
no estaba activado. Eso habria que arreglarlo. 

%===Tue Oct 24 11:07:15 ART 2000 mstorti@node1.beowulf.gtm
%
Ahora chequeo 'if (use_cache ...' antes de borrar el cache en
'void_cache()'. 

%===Wed Oct 25 11:57:59 ART 2000 mstorti@node1.beowulf.gtm
%
El les.epl tiene un problema que da NaN. Ahora esta andando, puede que
sea el compilar con Optimizador. 

%===Wed Oct 25 19:44:41 ART 2000 mstorti@node1.beowulf.gtm
%
Parece que se confirma que es el optimizador. 

%===Wed Oct 25 19:48:40 ART 2000 mstorti@node1.beowulf.gtm
%
No, falsa alarma. Con el debugger tambien da NaN. 

%===Wed Oct 25 19:53:02 ART 2000 mstorti@node1.beowulf.gtm
%
El mismo caso con LES=1 da NaN con LES=0 no. 

%===Fri Oct 27 13:25:21 ART 2000 mstorti@node1.beowulf.gtm
%
Da NaN cuando la velocidad en la pared es 0. Hay una division por 0 en
la formula. Habria que corregirlo (regularizarlo). 
%
Escribi un jobinfo "comp_res". La idea es no evaluar el jacobiano
todas las veces sino solo algunas. Pero parece no andar bien. 

%===Sat Nov  4 10:18:06 ART 2000 mstorti@localhost.localdomain
%
Tratando de mejorar la eficiencia. Observo que el tiempo de "comp_mat"
es aprox. la mitad del de comp_mat_res, cuando deberia ser mucho
menor!!!. Verifico que esto ocurre tanto con debugger como sin. 

%===Sat Nov 11 10:19:10 ART 2000 mstorti@localhost.localdomain
%

%
Agregue una version "linear" a FastMat2::prod(). En los experimentos
se gana bastante, un factor 2 o mas. Por ejemplo da 200-220 Mflops con
la version lineal y menos de 100 con la version no-lineal. Esto se
puede deber a que la version no-lineal hace mas operaciones pero
ademas requiere mas informacion ya que requiere un direccionamiento
mas. Estos datos en un P-III 733Mhz. 
%
Puesto en Navier-Stokes no se gana nada. 
%
Lo que si que la version FastMat2 eficiente se gana como un 20% sobre
le evaluacion del residuo. (11.32 sec/Ke contra 9.2 sec/Ke,
nsi_tet_les_fm2, P-II 350Mhz(spider)). 
%
Ahora voy a probar que pasa con el "cache" de matrices. 

%===Sat Nov 11 10:46:32 ART 2000 mstorti@localhost.localdomain
%
Con el cache de matrices se pasa de 9.2 sec/Ke a 6.7 sec/Ke!! Eso
parece estar bastante bien!! 

%===Sat Nov 11 10:49:18 ART 2000 mstorti@localhost.localdomain
%
Con el cache_grad_div_u tampoco parece haber diferencia entre la
version linear y no-lineal. Ahora vamos a verificar si la version mas
eficiente de todas `linear + cache_grad_div_u' da bien. (i.e., si
coincide con versiones anteriores.)

%===Sat Nov 11 17:44:00 ART 2000 mstorti@localhost.localdomain
%
Los tests dan bien. `oscaplate2b' da SIGSEGV pero despues no es usado
en los tests, asi que no lo cuento como test.
%
Sin embargo corriendo `test/les.epl' no da lo mismo que con versiones
anteriores. Voy a investigar esto. 

%===Sun Nov 12 10:04:03 ART 2000 mstorti@localhost.localdomain
%
Lo que pasa es que se junta bastante incompatibilidad con la nueva
estructura de datos, ya que ahora las opciones de elemento se pueden
poner en las opciones generales. 

%===Sun Nov 12 10:12:43 ART 2000 mstorti@localhost.localdomain
%
Tomando el caso test/sqcav5 parece dar lo mismo que la version 1.69g. 

%===Sun Nov 12 10:22:40 ART 2000 mstorti@localhost.localdomain
%
Vamos a ver si el oscplate2b deja de andar cuando se compila con -O2. 

%===Sun Nov 12 10:58:47 ART 2000 mstorti@localhost.localdomain
%
Efectivamente, con -O2 da un SIGSEGV. 

%===Sun Nov 12 18:27:22 ART 2000 mstorti@localhost.localdomain
%
Found a bug in tempfun.cpp in spline_function and piecewise_function.
Habia un error en la llamada a la sutina Fortran. Hay que reemplazar 
% spline_(&npoints,sd->time_vals,ampl_vals,sd->b,sd->c,sd->d);
por
% spline_(&npoints,sd->time_vals,sd->ampl_vals,sd->b,sd->c,sd->d);

%===Mon Nov 13 18:24:32 ART 2000 mstorti@node1.beowulf.gtm
%
En casa parecia que la version actual coincidia bien con una anterior
a empezar a investigar la eficiencia (creo que probe con la 1.69g),
pero ahora aca en geronimo no me coincide con la 2.16g. Voy a
verificar esto. 

%===Mon Nov 13 19:05:20 ART 2000 mstorti@node1.beowulf.gtm
%
Coincide bien pero si se usa la version vieja de `fmat2ep.cpp'. Quiere
decir que hay algun drama con la version `lineal'. 

%===Mon Nov 13 19:08:53 ART 2000 mstorti@node1.beowulf.gtm
%
Con "force non-linear" anda bien. 

%===Mon Nov 13 19:27:31 ART 2000 mstorti@node1.beowulf.gtm
%
Desactivando el "cache de fm2" anda mal igual. 

%===Mon Nov 13 19:31:52 ART 2000 mstorti@node1.beowulf.gtm
%
Decididamente lo que anda mal es el "linear addressing" no los caches.

%===Mon Nov 13 20:15:18 ART 2000 mstorti@node1.beowulf.gtm
%
Encontre el error! Cuando la dimension contraida era 1 (por ejemplo
cuando prod() se usa para hacer un kron) entonces el inca e incb
quedaban en 0 y el lazo no se hacia. Ahora dejo que si la dimension
contraida es menor o igual que 1 entonces se usa la version
no-lineal. 

%===Mon Nov 13 20:20:22 ART 2000 mstorti@node1.beowulf.gtm
%
Ahora anda bien version lineal y tambien si activo
EFFICIENT_FM2_VERSION en nsitetlesfm2.cpp.

%===Mon Nov 13 20:23:58 ART 2000 mstorti@node1.beowulf.gtm
%
Con `cache_grad_div_u' tambien da bien. 

%===Mon Nov 13 20:27:24 ART 2000 mstorti@node1.beowulf.gtm
%
Tambien les.epl da bien ahora. Vamos a medir la performance ahora. 

%===Tue Nov 14 09:21:50 ART 2000 mstorti@node1.beowulf.gtm
%
Con `cache_grad_div_u', linear addressing en FM2 y
EFFICIENT_FM2_VERSION da un 33% mas rapido, es decir 4.1sec/Ke contra
6.2sec/Ke. 
%
Ahora no me vuelve a coincidir con la version 2.16!! A revisar...

%===Tue Nov 14 10:12:47 ART 2000 mstorti@node1.beowulf.gtm
%
les.epl no coincide pero `back_step_3d' si. Asumo que esta bien y
genero una nueva version estable. 

%===Tue Nov 14 11:05:58 ART 2000 mstorti@node1.beowulf.gtm
%
Generado un test para el bug en la opcion linear de
FastMat2::prod(). Ademas hay una doble proteccion ya que ahora
inicializo inca,incb a 1.

%===Tue Dec  5 17:40:10 ART 2000 mstorti@node1.beowulf.gtm
%
BUG: en ffswfm2.cpp (funcion de flujo para shallow water). La matrix
flux_mass estaba inicializada como `static FastMat2 flux_mass(u)' lo
cual no tiene demasiado sentido y originaba que el almacenamiento de
flux_mass era el mismo que el de u. Podria ser una idea para generar
mascaras? 

%===Tue Dec  5 18:58:12 ART 2000 mstorti@node1.beowulf.gtm
%
back_step_3d: Aparentemente ahora el programa corre, despues de haber
booteado el nodo6. Guarda que quedo la version upgradeada o sea que
ahora esta corriendo una version mas nueva. Los residuos parecen ser
un poco diferentes. 
%
BUG: en ffswfm2.cpp ponia `start_chunk=1' despues de la
inicializacion. Mientras que deberia ponerlo a 0 para que no vuelva a
entrar. 
%
advdif: empiezo a escribir un `ffadvfm2.cpp' con flujos para adveccion
difusion lineal. 

%===Sat Dec  9 19:06:02 ARST 2000 mstorti@localhost.localdomain
%
Anda el problema de difusion lineal. Ahora voy a probar con algo de
adveccion.

%===Tue Dec 12 19:26:29 ARST 2000 mstorti@localhost.localdomain
%
Dejo lo de la difusion lineal por ahora. Voy a trabajar en hacer el
jacobiano analitico. 

%===Sat Dec 16 19:45:21 ARST 2000 mstorti@localhost.localdomain
%
Anda bien el jacobiano de la parte advectiva pura. Ahora voy a ver el
del bcconv (bccadvfm2.cpp). 

%===Sat Dec 16 20:05:02 ARST 2000 mstorti@localhost.localdomain
%
BUG en `advective': en `bccadvfm2.cpp' hay que pesar con `wpgdet' no
con WPG?? 
%
NOOOOO!! En los bccadvfm2, el vector `normal' incluye el jacobiano de
la trnasformacion (de superficie en superficie) asi que solo hace
falta multiplicar por el peso de gauss (WPG). 
%
Ahora voy a hacer la version no debilitada.

%===Sat Dec 16 20:50:09 ARST 2000 mstorti@localhost.localdomain
%
OJO! Hay que probar si anda con Jacobianos no simetricos...

%===Sat Dec 16 21:28:50 ARST 2000 mstorti@localhost.localdomain
%
Anda bien la version no-debilitada. 

%===Sun Dec 17 09:27:31 ARST 2000 mstorti@localhost.localdomain
%
Probar con la ec. de Burgers.
Probar con alpha\neq 1

%===Sun Dec 17 20:13:25 ARST 2000 mstorti@localhost.localdomain
%
Voy a llevar la ec. de Burgers a 2D como f = 0.5*u0*phi^2 donde u0 es
un vector constante que se entra.

%===Sat Dec 23 03:52:00 ARST 2000 mstorti@localhost.localdomain
%
Parece andar bien la ec. de Burgers 2D.

%===Sat Dec 23 03:59:14 ARST 2000 mstorti@localhost.localdomain
%
Vamos a verificar ahora que ande bien $\alpha \neq 1$

%===Sat Dec 23 10:55:38 ARST 2000 mstorti@localhost.localdomain
%
Estaba de mas el ALPHA en una contribucion al residuo:
! 	veccontr.axpy(tmp8,wpgdet*ALPHA);
cambiar por
! 	veccontr.axpy(tmp8,wpgdet);
Ahora converge bien la ecuacion del calor (u=0), es decir converge en
una sola iteracion pero el problema global no converge. 

%===Sat Dec 23 11:33:22 ARST 2000 mstorti@localhost.localdomain
%
Esta todo bien, solo era que el Crank-Nicholson puede no converger
para llegar a una solucion estacionaria si se usa un paso de tiempo
muy grande. Con paso de tiempo 0.8 anda todo bien. 
%
Ahora voy a probar que la convergencia de Newton-Raphson es
cuadratica.

%===Sat Dec 23 12:34:32 ARST 2000 mstorti@localhost.localdomain
%
No parece dar convergencia cuadratica.

%===Sat Dec 23 18:55:05 ARST 2000 mstorti@localhost.localdomain
%
Si bien todavia hay un drama con el tema de que se debrian pasar
argumentos que son vectores+tiempos, de todas formas por lo menos para
hacer que el Crank-Nicholson de convergencia cuadratica hay que pasar
como tiempo en todas las evalucaiones del residuo el tiempo
$t^*=t+\alpha\Dt$. Ahora si da convergencia cuadratica. 

%===Sat Dec 23 18:56:54 ARST 2000 mstorti@localhost.localdomain
%
vamos a probar si todas estas cosas dan bien con Burgers ahora.

%===Mon Dec 25 18:04:03 ARST 2000 mstorti@localhost.localdomain
%
Pude construir la documentacion para `glib' y entonces ahora pude
poner funciones de hash y de comparacion apropiadas para texthash. 

%===Thu Dec 28 13:08:39 ARST 2000 mstorti@minerva
%
Porting to gcc 2.96-54. There were several syntax errors. 
%
* `export' is a keyword
* Templates for functions can't have default values
* Pointers to functions can't be declared as `const'
* No te deja poner `template<class T> class T random_pop(set<...'
     tiene que ser `template<class T> T random_pop(set<' 
* Para declarar que el puntero es constante:  int get_line(char * const& line );
* Ahora PETSc PCSetType() no anda asi nomas ya que el segundo
       arugumento es char * y si le pasamos un string.c_str() el valor
       de retorno es const char *. Lo tuve qu copiar a una string de C
       clasico. 
* En la compilacion de mpi tuve que cambiar lo siguiente en
mpe/slog_api/src/slog_irec_write.c, linea 1171

/* Antes */
/*          dest_node_id      = va_arg( ap, SLOG_nodeID_t ); */ 
/*          dest_cpu_id       = va_arg( ap, SLOG_cpuID_t ); */
/*          dest_thread_id    = va_arg( ap, SLOG_threadID_t ); */
/* despues */
        dest_node_id      = va_arg( ap, int );
        dest_cpu_id       = va_arg( ap, int );
        dest_thread_id    = va_arg( ap, int );

El mensaje de error lo sugeria.

%===Fri Dec 29 11:46:08 ARST 2000 mstorti@minerva
%
En ElementIterator::begin() tuve que avanzar el iterator hasta el
primer elemento valido. Sino, te daba un error ya que incluia al
primer elemento qu podia no estar en el chunk. Ahora hay que modificar
el iterator para los otros modos de iteracion (jacobianos numericos). 

%===Fri Dec 29 11:59:30 ARST 2000 mstorti@minerva
%
Esta complicado convertir todo elemset con iterators ya que
`compute_this_elem' es usado muchas veces. 

%===Fri Dec 29 12:20:38 ARST 2000 mstorti@minerva
%
Lo que voy a hacer es que en el elemento se llama al iterator y
'is_valid()' llama en realidad a `compute_this_elem' de manera que se
asegura una cierta compatibilidad. 

%===Fri Dec 29 13:21:33 ARST 2000 mstorti@minerva
%
Guarda, sigue habiendo un quilombo con el ElementIterator con ghost
elems. 

%===Fri Dec 29 19:40:56 ARST 2000 mstorti@localhost.localdomain
%
No puedo hacer que recicle imagenes el latex2html. 

%===Sun Dec 31 10:05:58 ARST 2000 mstorti@spider
%
Retomo sine.epl. Adveccion difusion en un medio semiinfinito periodico
en la direccion `y'. La condicion en $x=0$ es $\phi = \cos (2\pi x)
\sin(\omega t)$
%
Da resultados mas o menos buenos, pero curiosamente no da convergencia
en una iteracion del problema no-lineal con LU. 
%
La convergencia inicial esta bien (baja el residuo a 1e-14 en una
iteracion) pero poco a poco se empieza a deteriorar. 

%===Sun Dec 31 10:22:24 ARST 2000 mstorti@spider
%
No parece estar relacionado con las condiciones variables en el
tiempo. Poniendo un escalon en el tiempo tampoco converge bien. 

%===Sun Dec 31 12:56:05 ARST 2000 mstorti@spider
%
El problema no esta en el elemento bcconv. Esta directamente en el
'advdife' y aparece para u\neq 0. 
%
con tau_fax=0 da mal tambien. 

%===Sun Dec 31 13:02:53 ARST 2000 mstorti@spider
%
Parece que era la famosa historia de como contraer el A_jac. 

%===Mon Jan  1 21:33:37 ARST 2001 mstorti@spider
%
Ahora da la convergencia todo bien. Crank-Nicholson converge
cuadraticamente y implicito puro converge lineal. Voy a hacer un caso
test. 

%===Thu Jan  4 17:49:26 ARST 2001 mstorti@minerva
%
TAG: beta-1.3

%===Fri Jan  5 20:28:18 ARST 2001 mstorti@minerva
%
TAG: beta-1.4
TAG: beta-1.5
TAG: beta-1.6

%===Sat Jan  6 11:11:17 ARST 2001 mstorti@spider
%
Haciendo upgrade a RH7.0 en spider.
%
Compilando MPI, PETSC, ANN, meschach: sin problema. 
%
Compilando Newmat: parche par newmat1.cpp

========cut here
diff -c /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp.orig /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp
*** /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp.orig	Sun Sep  7 04:38:35 1997
--- /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp	Sat Jan  6 11:18:05 2001
***************
*** 87,95 ****
     case Valid+Band+Lower:                   return "LwBnd";
     default:
        if (!(attribute & Valid))             return "UnSp ";
!       if (attribute & LUDeco)
!          return (attribute & Band) ?     "BndLU" : "Crout";
!                                             return "?????";
     }
  }
  
--- 87,102 ----
     case Valid+Band+Lower:                   return "LwBnd";
     default:
        if (!(attribute & Valid))             return "UnSp ";
! //        if (attribute & LUDeco)
! //           return (
!       if (attribute & LUDeco) {
! 	if (attribute & Band) {
! 	  return "BndLU";
! 	} else {
! 	  return "Crout";
! 	}
!       }
!       return "?????";
     }
  }
========cut here


%===Sat Jan  6 11:24:35 ARST 2001 mstorti@spider
%
Patch para meschach: matrix.h

========cut here
diff -c /home/mstorti/SOFT/meschach-1.2/matrix.h.orig /home/mstorti/SOFT/meschach-1.2/matrix.h
*** /home/mstorti/SOFT/meschach-1.2/matrix.h.orig	Mon Nov 11 06:20:57 1996
--- /home/mstorti/SOFT/meschach-1.2/matrix.h	Sat Jan  6 11:23:51 2001
***************
*** 178,184 ****
  /* free (de-allocate) (band) matrices, vectors, permutations and 
     integer vectors */
  extern  int iv_free(IVEC *);
! extern	m_free(MAT *),v_free(VEC *),px_free(PERM *);
  extern   int bd_free(BAND *);
  
  #endif
--- 178,184 ----
  /* free (de-allocate) (band) matrices, vectors, permutations and 
     integer vectors */
  extern  int iv_free(IVEC *);
! extern	int m_free(MAT *),v_free(VEC *),px_free(PERM *);
  extern   int bd_free(BAND *);
  
  #endif
========cut here

%===Sat Jan  6 18:58:10 ARST 2001 mstorti@spider
%
Tratando de hacer que el conseguir las coordenadas y H del elemento
sea mas `OOP'. Creamos dos funciones 
%
  void element_node_data(const ElementIterator &element,
			 double *xloc,double *Hloc);
%  
  void element_connect(const ElementIterator &element,
		       int *connect);
%
que recuperan las conectividades y valores nodales de un dado
elemento. 

%===Sun Jan  7 13:21:21 ARST 2001 mstorti@spider
%
Para que ande bien el `make depend' los SRCS en los makefile deben ser
definidos antes del `include ....Makefile.base'. 
%
Escribo 
  const double *
  element_vector_values(const ElementIterator &element,
			arg_data &ad) const;

%===Tue Jan  9 21:48:25 ARST 2001 mstorti@spider
%
Hago que la funcion de flujo en advdif sea ahora una `function object'
es decir una clase con `operator()' sobrecargado. La estructura es que
`AdvDif' tiene un puntero a una clase pure virtual `AdvDifFF' y
sobrecargando `operator()' de esta clase logras cambiar la funcion de
flujo. Ya anda para la clase `advdif_advecfm2' ahora hay que hacerlo
para `bcconv_adv_advecfm2'. 

%===Wed Jan 10 08:38:25 ARST 2001 mstorti@minerva
%
El comando para crear el patch es 
%
   $ diff -cNr DIROLD DIRNEW
%
y para patchear te pones en el directorio a patchear y
%
   $ patch -E -p1 < ../petscfem.patch

%===Wed Jan 10 12:48:37 ARST 2001 mstorti@minerva
%
Ahora voy a correr burgers. 

%===Wed Jan 10 13:55:21 ARST 2001 mstorti@minerva
%
Anda burgers. Ahora voy a ver si puedo transformar a NS. 

%===Sat Jan 13 21:45:30 ARST 2001 mstorti@spider
%
Para correr las viejas versiones de PETSc-FEM. Usar el compilador
egcs. Reemplazar en petsc/bmake/linux/base_variables `g++' por
$(GNUCC). Despues en Makefile.base se define `GNUCC = egcs++'. Des
esta forma compila pero despues hay errores de linkedicion. 
%

%===Sun Jan 14 15:13:06 ARST 2001 mstorti@spider
%
Puedo compilar versiones viejas con egcs!! 
%
Instrucciones:
%
* Cambiar el compilador C++ en petsc/bmake/linux/base_variables de g++
  a egcs++ (hay que tener los paquetes compat-egcs y compat-egcs++
  instalados). 
* Agregar '-static' a las banderas de linkedicion
* Agregar '-L/usr/lib/gcc-lib/i386-redhat-linux/2.96/' a las librerias
%
Para poder seguir compilando con gcc tambien, conviene agregar
las siguientes variables en Makefile.base 
%-------- para compilar con egcs++
GNUCXX = egcs++
OCXX_COPTFLAGS_USR = -static
CXX_SYS_LIB_USR = -L/usr/lib/gcc-lib/i386-redhat-linux/2.96/ 
%--------pra compilar con g++
GNUCXX = g++
OCXX_COPTFLAGS_USR = 
CXX_SYS_LIB_USR = 
%-------------
%
y modificar un poco el base_variables con el siguiente parche:
%-----------ESTE PARCHE ES CON RESPECTO AL BASE_VARIABLES ORIGINAL
%---------- CUT HERE
cd ~/PETSC/petsc-2.0.24/bmake/linux/
diff -c /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables.bck /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables
*** /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables.bck	Wed Mar 31 15:38:47 1999
--- /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables	Sun Jan 14 14:51:45 2001
***************
*** 1,4 ****
! # $Id: notes.txt,v 1.130 2002/07/22 21:11:38 mstorti Exp $ 
  #
  #     See the file bmake/base_variables.defs for a complete explanation of all these fields
  #
--- 1,5 ----
! # -*- mode: makefile -*-
! # $Id: notes.txt,v 1.130 2002/07/22 21:11:38 mstorti Exp $ 
  #
  #     See the file bmake/base_variables.defs for a complete explanation of all these fields
  #
***************
*** 7,41 ****
  OMAKE            = make  --no-print-directory
  RANLIB           = ranlib
  SHELL            = /bin/sh
! SH_LD            = gcc
  # ######################### C and Fortran compiler ########################
  #
! C_CC             = gcc -fPIC
  C_FC             = g77 -Wno-globals
! C_CLINKER        = gcc ${COPTFLAGS} -rdynamic -Wl,-rpath,${LDIR}:${DYLIBPATH}
  C_FLINKER        = g77 ${FOPTFLAGS} -rdynamic -Wl,-rpath,${LDIR}:${DYLIBPATH}
  C_CCV            = ${C_CC} --version
  C_SYS_LIB        = -ldl -lc -lg2c -lm
  # ---------------------------- BOPT=g options ----------------------------
! G_COPTFLAGS      = -g 
  G_FOPTFLAGS      = -g
  # ----------------------------- BOPT=O options -----------------------------
  O_COPTFLAGS      = -O -Wall -Wshadow  -fomit-frame-pointer
  O_FOPTFLAGS      = -O
  # ########################## C++ compiler ##################################
  #
! CXX_CC           = g++ -fPIC
  CXX_FC           = g77 -Wno-globals
! CXX_CLINKER      = g++ ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
! CXX_FLINKER      = g++ ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
  CXX_CCV          = ${CXX_CC} --version
! CXX_SYS_LIB      = -ldl -lc -lg2c -lm
  # ------------------------- BOPT=g_c++ options ------------------------------
! GCXX_COPTFLAGS   = -g 
  GCXX_FOPTFLAGS   = -g
  # ------------------------- BOPT=O_c++ options ------------------------------
! OCXX_COPTFLAGS   = -O 
! OCXX_FOPTFLAGS   = -O
  # -------------------------- BOPT=g_complex options ------------------------
  GCOMP_COPTFLAGS  = -g
  GCOMP_FOPTFLAGS  = -g
--- 8,43 ----
  OMAKE            = make  --no-print-directory
  RANLIB           = ranlib
  SHELL            = /bin/sh
! SH_LD            = egcs
  # ######################### C and Fortran compiler ########################
  #
! C_CC             = egcs -fPIC
  C_FC             = g77 -Wno-globals
! C_CLINKER        = egcs ${COPTFLAGS} -rdynamic -Wl,-rpath,${LDIR}:${DYLIBPATH}
  C_FLINKER        = g77 ${FOPTFLAGS} -rdynamic -Wl,-rpath,${LDIR}:${DYLIBPATH}
  C_CCV            = ${C_CC} --version
  C_SYS_LIB        = -ldl -lc -lg2c -lm
+ #C_SYS_LIB        = -ldl -lc -lf2c -lm
  # ---------------------------- BOPT=g options ----------------------------
! G_COPTFLAGS      = -g
  G_FOPTFLAGS      = -g
  # ----------------------------- BOPT=O options -----------------------------
  O_COPTFLAGS      = -O -Wall -Wshadow  -fomit-frame-pointer
  O_FOPTFLAGS      = -O
  # ########################## C++ compiler ##################################
  #
! CXX_CC           = $(GNUCXX) -fPIC
  CXX_FC           = g77 -Wno-globals
! CXX_CLINKER      = $(GNUCXX) ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
! CXX_FLINKER      = $(GNUCXX) ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
  CXX_CCV          = ${CXX_CC} --version
! CXX_SYS_LIB      = $(CXX_SYS_LIB_USR) -ldl -lc -lg2c -lm
  # ------------------------- BOPT=g_c++ options ------------------------------
! GCXX_COPTFLAGS   = -gstabs+
  GCXX_FOPTFLAGS   = -g
  # ------------------------- BOPT=O_c++ options ------------------------------
! OCXX_COPTFLAGS   = -O2 $(OCXX_COPTFLAGS_USR)
! OCXX_FOPTFLAGS   = -O2
  # -------------------------- BOPT=g_complex options ------------------------
  GCOMP_COPTFLAGS  = -g
  GCOMP_FOPTFLAGS  = -g

Diff finished at Sun Jan 14 15:14:01
%---------- CUT HERE
%
%---------- ESTE ES CON RESPECTO AL ULTIMO (YA CONFIGURADO)
%---------- CUT HERE
diff -c /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables.bck2 /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables
*** /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables.bck2	Fri Dec  8 15:19:17 2000
--- /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables	Sun Jan 14 14:51:45 2001
***************
*** 1,3 ****
--- 1,4 ----
+ # -*- mode: makefile -*-
  # $Id: notes.txt,v 1.130 2002/07/22 21:11:38 mstorti Exp $ 
  #
  #     See the file bmake/base_variables.defs for a complete explanation of all these fields
***************
*** 25,42 ****
  O_FOPTFLAGS      = -O
  # ########################## C++ compiler ##################################
  #
! CXX_CC           = g++ -fPIC
  CXX_FC           = g77 -Wno-globals
! CXX_CLINKER      = g++ ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
! CXX_FLINKER      = g++ ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
  CXX_CCV          = ${CXX_CC} --version
! CXX_SYS_LIB      = -ldl -lc -lg2c -lm
! #CXX_SYS_LIB      = -ldl -lc -lf2c -lm
  # ------------------------- BOPT=g_c++ options ------------------------------
  GCXX_COPTFLAGS   = -gstabs+
  GCXX_FOPTFLAGS   = -g
  # ------------------------- BOPT=O_c++ options ------------------------------
! OCXX_COPTFLAGS   = -O2 
  OCXX_FOPTFLAGS   = -O2
  # -------------------------- BOPT=g_complex options ------------------------
  GCOMP_COPTFLAGS  = -g
--- 26,42 ----
  O_FOPTFLAGS      = -O
  # ########################## C++ compiler ##################################
  #
! CXX_CC           = $(GNUCXX) -fPIC
  CXX_FC           = g77 -Wno-globals
! CXX_CLINKER      = $(GNUCXX) ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
! CXX_FLINKER      = $(GNUCXX) ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
  CXX_CCV          = ${CXX_CC} --version
! CXX_SYS_LIB      = $(CXX_SYS_LIB_USR) -ldl -lc -lg2c -lm
  # ------------------------- BOPT=g_c++ options ------------------------------
  GCXX_COPTFLAGS   = -gstabs+
  GCXX_FOPTFLAGS   = -g
  # ------------------------- BOPT=O_c++ options ------------------------------
! OCXX_COPTFLAGS   = -O2 $(OCXX_COPTFLAGS_USR)
  OCXX_FOPTFLAGS   = -O2
  # -------------------------- BOPT=g_complex options ------------------------
  GCOMP_COPTFLAGS  = -g

Diff finished at Sun Jan 14 15:22:21
%---------- CUT HERE
%

%===Sun Jan 14 18:15:34 ARST 2001 mstorti@spider
%
Todo esto tiene el problema que despues no corre el programa. Entonces
la solucion es compilar con egcs++ pero linkeditar con g++. Para esto
%
* Tocar base_variables de manera de volver a que el linkeditor sea
   g++:
%
CXX_CLINKER      = g++ ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
%
* En tiempo de corrida no encuentra libpetscles.so, entonces hay que
agregar los directorios:
%
10	/home/mstorti/PETSC/petsc-2.0.24/lib/libO_c++/linux/
11	/home/mstorti/PETSC/petsc-2.0.24/lib/libg_c++/linux/
12	/home/mstorti/PETSC/petsc-2.0.24/lib/libg/linux/
%
a /etc/ld.so.conf y correr `ldconfig -v'.

%===Sun Jan 14 18:59:54 ARST 2001 mstorti@spider
%
Corro el sqcav5 en spider con las versiones beta-1.12 y 2.8s. Dan
igual a precision de la maquina a Re=400. 

%===Mon Jan 15 12:33:56 ARST 2001 mstorti@minerva
%
Correiendo en geronimo da diferente la version beta-1.13 con la
2.16g. En la salida dan
%
------------------ VERSION beta-1.13
Time step: 1, time: 0.005
Newton subiter 0, norm_res  =  8.687e-07, update Jac. 1
Newton subiter 1, norm_res  =  1.967e-07, update Jac. 1
============= delta_u =  1.636e+00
iter: 0, saving on rec 0, file outvector0.out
Writing vector to file "outvector0.out"
Time step: 2, time: 0.01
Newton subiter 0, norm_res  =  7.485e-07, update Jac. 1
Newton subiter 1, norm_res  =  1.915e-07, update Jac. 1
============= delta_u =  1.648e+00
------------------ VERSION 2.16g
Time step: 1, time: 0.005
|| R || =  2.703e-06
|| R || =  1.321e-07
============= delta_u =  2.002e+00
iter: 0, saving on rec 0, file outvector0.sal
Writing vector to file "outvector0.sal"
Time step: 2, time: 0.01
|| R || =  1.963e-06
|| R || =  7.997e-08
============= delta_u =  1.708e+00
------------------
%
y la diferencia entre los vectores de estado en Octave da
%
octave> un=aload("back_step_3d.state.new");
octave> un=aload("back_step_3d.state.new");
octave> merr(uo-un)
ans = 0.055116
octave> 
%
Ahora voy a probar con sqcav5 a ver si sigue dando igual (como daba en
casa).

%===Mon Jan 15 12:46:09 ARST 2001 mstorti@minerva
%
Para el sqcav siguen dando iguales. 

%===Mon Jan 15 12:51:28 ARST 2001 mstorti@minerva
%
Con Jacobi tambien sqcav da igual. 
Con 2 procesadores tambien da lo mismo. 
Con weak_form 1 tambien da lo mismo.
Con C_smag 0 da lo mismo.
Con LES 0 da lo mismo.
Con temporal_stability_factor 0. da lo mismo.
Con Re=40000 da lo mismo
Con inicializacion da lo mismo.
Con nnwt=2 da lo mismo.
Con maxits=10 da lo mismo.
Copiando el archivo de datos de back_step_3d y dejando todas las
    opciones de ese caso da lo mismo.
Si el paso de tiempo es muy chico hay que poner
   `temporal_stability_factor 0' para que de lo mismo. 

%===Tue Jan 16 13:21:59 ARST 2001 mstorti@minerva
%
Para que de igual hay que poner `temporal_stability_factor 1'!!

%===Tue Jan 16 19:10:12 ARST 2001 mstorti@minerva
%
Voy a poner en NS la posibilidad de tener estados "filtrados". 

%===Wed Jan 17 18:13:20 ARST 2001 mstorti@minerva
%
Escribi una clase simple de filtros. Anda, pero ahora voy a poner el
mixer que puede tener varias entradas. 

%===Fri Jan 19 10:45:13 ARST 2001 mstorti@minerva
%
Viendo que pasa con el cache_grad_div_u: La version beta--1-13
(brancheada a cache-gdu) con esto activado no converge. El elemento
nsi_tet_les_fm2 da muy parecido al nsi_tet (si no activamos el
cache_grad_div_u). 

%===Fri Jan 19 10:50:43 ARST 2001 mstorti@minerva
%
Hay dos posibilidades: que no guarde bien el operador grad_div_u en el
cache, o que no este bien el calculando grad_div_u y despues agregando
la contribucion al residuo y a la matrix al final. 

%===Fri Jan 19 11:18:42 ARST 2001 mstorti@minerva
%
Agregando una linea de forma que calcule cada vez el grad_div_u parece
que anda bien (da una pequena diferencia, a mi me parece que deberia
dar lo mismo a precision de la maquina). 

%===Fri Jan 19 12:15:11 ARST 2001 mstorti@minerva
%
Ya esta!! El problema es con los chunks... Habria que usar la posicion
del elemento dentro del procesador, pero no dentro del chunk. 

%===Fri Jan 19 13:03:44 ARST 2001 mstorti@minerva
%
Ahora a local_store_address(int global_elem) hay que pasarle el numero
global del elemento. 
%
Encontre un error en `elemset.cpp': tomaba `report_consumed_time' de
la thash global, no de la del elemento. 
%
Cambie el chunk_size default a 1000. 

%===Fri Jan 19 23:29:22 ARST 2001 mstorti@spider
%
Mergeadas las versiones `cache-gdu-fixed' y 'beta-1.15' en la version
`beta-1.16'. 
%
Ahora el problema es que dan SIGSEGV las corridas `plano...' y
contraint_bug, o sea en todas las que el programa es adv. 

%===Sat Jan 20 07:32:20 ARST 2001 mstorti@spider
%
PARA MERGEAR BRANCHES:
%
* Para crear branches: $ cvs rtag -b -r OLDTAG NEWBRANCH petscfem
* Cada vez que se salva en ese branch: $ cvs rtag -b -r NEWBRANCH NEWBRANCH2 petscfem
* Para mergear un branch con un directorio de trabajo $ cvs up -kk -j BRANCH <directory> 
* Si hay problemas de conflictos correr en Emacs: M-x vc-resolve-conflicts
* Hacer comit del working directory. Si da problemas con 'sticky tags' 
        correr $ cvs up -A .
* Hacer un nuevo tag

%===Sat Jan 20 09:10:57 ARST 2001 mstorti@spider
%
Resuelto el problema!! Era que Newmat no compila bien en spider con
g++. Lo compile con egcs++ y a partir de ahi anduvo bien. Sera el
parche ese que tuve que hacer en newmat1.cpp??? 
%
========cut here
diff -c /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp.orig /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp
*** /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp.orig	Sun Sep  7 04:38:35 1997
--- /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp	Sat Jan  6 11:18:05 2001
***************
*** 87,95 ****
     case Valid+Band+Lower:                   return "LwBnd";
     default:
        if (!(attribute & Valid))             return "UnSp ";
!       if (attribute & LUDeco)
!          return (attribute & Band) ?     "BndLU" : "Crout";
!                                             return "?????";
     }
  }
  
--- 87,102 ----
     case Valid+Band+Lower:                   return "LwBnd";
     default:
        if (!(attribute & Valid))             return "UnSp ";
! //        if (attribute & LUDeco)
! //           return (
!       if (attribute & LUDeco) {
! 	if (attribute & Band) {
! 	  return "BndLU";
! 	} else {
! 	  return "Crout";
! 	}
!       }
!       return "?????";
     }
  }
========cut here

%===Sat Jan 20 23:47:06 ARST 2001 mstorti@spider
%
Volviendo a trabajar con filtros. Logro hacerlos andar muy basicamente
en ns.cpp. 

%===Mon Jan 22 18:02:32 ARST 2001 mstorti@minerva
%
Hay una incongruencia en la forma en que se tratan los filtros. Solos
se filtra el estado y si hay un nodo fijado a un valor dependiente del
tiempo entonces se pone el valor inmediato del nodos lo cual es
inconsistente.

%===Tue Jan 23 09:20:29 ARST 2001 mstorti@minerva
%
Andan los filtros. Falta hacer lo siguiente
* Que filtros impriman delta_u
* Documentar filtro. OK
* salvar version filtro
* Configurar mejor en ns.cpp. Que pueda leer nombre de archivo para
     guardar. 

%===Tue Jan 23 10:29:42 ARST 2001 mstorti@minerva
%
Voy a agregar las cosas de Beto (shallow water turbulento). Hago el
siguiente merge

version al 5 Jan 12:00  ---->  petscfem.beto
                    +------->  petscfem.actual

%===Wed Jan 24 19:14:08 ARST 2001 mstorti@minerva
%
Echo el merge con turbulento (version beta-1.23). Pero advdif no
linkedita en minerva!! No se que era, el sistema estaba inestable,
sali de X y a partir de ahi empezo a andar todo. Tampoco compilaba
algunos de los .cpp (daba internal error, SIGSEGV etc...) .

%===Sun Feb  4 22:47:13 ARST 2001 mstorti@spider
%
Agregue documentacion de opciones ('odoc.pl') a las opciones de
shallow water, en ffswfm2t.cpp. 

%===Wed Feb  7 12:04:50 ARST 2001 mstorti@minerva
%
En shallow water vamos a agregar un "bottom_slope" (equivalente al
"G_body" en NS) que es la friccion con el fondo evitando de tener que
definir la forma del fondo. 

%===Fri Feb  9 10:40:01 ARST 2001 mstorti@minerva
%
Se obtiene un perfil de velocidades con Ly=10, N=100, Dt=2, wall_coef=
v_wall/v_(y\to\infty)=0.9. La idea es ahora refinar la malla hacia las
paredes e ir bajando la velocidad en la pared a 0.

%===Fri Feb  9 10:57:26 ARST 2001 mstorti@minerva
%
Refinando por 3 cerca de la pared con los mismos parametros da bien. 
Bajando el factor de pared a 0.5 deja de converger. 

%===Fri Feb  9 11:01:53 ARST 2001 mstorti@minerva
%
No lo haces andar ni con factor de relajacion 0.1 (pero guarda que hay
que aumentar los nnwt). 

%===Fri Feb  9 11:11:51 ARST 2001 mstorti@minerva
%
Aumentando nnwt a 10 tampoco anda. Aumentano el Dt tampoco
mejora. Sera por inestabilidad temporal? Usando un paso de tiempo
suf. chico deberia andar pero tambien se puede armar quilombo con las
oscilaciones tipo "termino de reaccion" [(SU+C)PG.]

%===Fri Feb  9 11:45:34 ARST 2001 mstorti@minerva
%
Vamos a ver si se puede avanzar algo por continuacion. 

%===Fri Feb  9 11:56:33 ARST 2001 mstorti@minerva
%
Con continuacion se puede pasar de 0.9 -> 0.8 -> 0.7 -> 0.6
tranquilo. Ya no se puede pasar de 0.6 a 0.5.

%===Sat Feb 10 11:46:31 ARST 2001 mstorti@spider
%
Anda bien matriz lumped en ecuacion del calor.

%===Sun Feb 11 09:12:48 ARST 2001 mstorti@spider
%
Andan bien las variables logaritmicas para la ec. del calor.

%===Sun Feb 11 21:29:03 ARST 2001 mstorti@spider
%
Andan log-vars en shallow water y en general.
%
Ahora voy a documentarlo. 

%===Thu Mar  1 13:54:19 ARST 2001 mstorti@spider
%
In TextHashTable: Print a warning if some option is not used and a doc
when an option was got the first time.

%===Thu Mar  1 16:23:54 ARST 2001 mstorti@spider
%
Escribo TextHashTable::read, eventualmente va a reemplazar a
read_hash_table(). 

%===Fri Mar  2 20:00:34 ARST 2001 mstorti@spider
%
Agregado el conteo de veces que es accedida una opcion en
TextHashTable's. 

%===Sat Mar  3 05:05:03 ARST 2001 mstorti@spider
%
Doc++ no da segmentation fault cuando pongo
%
//---:---<*>---:---<*>---:---<*>---:---<*>---:---<*>---:---<*>---: 
/** Makes a temporary copy of a string.
    @author M. Storti
    @param cstr (input) the string to be copied
    @return a pointer to the copied string
*/ 
char * local_copy(const char * cstr);
%
Pero si lo da si cambio el parametro 'cstr' a 's'. Curioso no?

%===Sat Mar  3 07:17:29 ARST 2001 mstorti@spider
%
Agregue tests para variables logaritmicas en ecuacion del calor y sw turb.

%===Sat Mar  3 12:46:28 ARST 2001 mstorti@spider
%
Agregado elemset ns+termico (nsi_tet_les_ther / Beto). 

%===Sat Mar 10 11:30:50 ART 2001 mstorti@spider
%
Creado un parser (con bison) para leer la linea 'props'.

%===Fri Mar 16 19:07:03 ART 2001 mstorti@spider
%
Andan (no lo chequee demasiado) las propiedades. get_prop etc... Ahora
voy a hacer mejor las funciones de flujo... 

%===Sat Mar 17 19:39:15 ART 2001 mstorti@spider
%
Agrego una mascara de diag a FM2. Funciona poniendo A.d(j1,j2) y
quiere decir que a partir de entonces el indice j2 copia al j1. O sea
que si A es de 3x5x5 entonces si hacemos A.d(3,2) entonces cuando
accedemos a A(1,2) estamos accediendo a A(1,2,2).
%
Esto se hace seteando la variable set_indx[] de la dimension j2 a
-j1. Las modificaciones a fastmat2 fueron pequenhas. Corri el test
testfm2 y anda bien, ahora voy a correr todos los otros tests. 

%===Sat Mar 17 19:54:02 ART 2001 mstorti@spider
%
Hace unos dias descubrimos un error en 'ns' que la opcion
'shock_capturing_factor' no estaba puesta por default a 0. 

%===Fri Mar 23 09:58:42 ART 2001 mstorti@minerva
%
Agrego jacobianos difusivos, convectivos y reactivos en forma OOP. 
Ahora voy a corregir el bug descubierto por Beto en 'mydetsur'. 

%===Fri Mar 30 17:01:47 ART 2001 mstorti@spider
%
Hay un bug en elnuevo advdif. Cuando corro el test 'full_full_jacs_t'
con mas elementos en la direccion y no anda. 
%
Descubri un error que no leia correctamente el tau_fac, es decir no
habia hecho el EGETOPTDEF. 

%===Fri Mar 30 17:24:48 ART 2001 mstorti@spider
%
Otro bug: 'ret_options' no estaba pasado por referencia en start_chunk
de manera que no era modificado desde la funcion de flujo. 
%
otro BUG: el 'ndim' de la funcion de flujo estaba definido con
EGETOPTDEF y no con EGETOPTDEF_ND por lo cual no quedaba definido y
quedaba como cero. 

%===Mon Apr  2 17:23:25 ART 2001 mstorti@spider
%
Voy a agregar una entalpia generalizada para advdif. 

%===Fri Apr  6 14:20:05 ART 2001 mstorti@minerva
%
Para compilar con optimizacion maxima
%
> # ------------------------- BOPT=O_c++ options ------------------------------
> OCXX_COPTFLAGS   = -O2 -funroll-loops
> OCXX_FOPTFLAGS   = -O2 -funroll-loops
%
en base_variables.

%===Fri Apr  6 15:49:16 ART 2001 mstorti@minerva
%
Habia un bug en 'newff.m' el archivo que controla los tests en
'test/newff'. La expresion
     uana = (beta \ (1-exp(-beta*nstep*Dt)))*(CP\S);
hay que cambiarla por
     uana = (beta \ (eye(ndof)-expm(-beta*nstep*Dt)))*(CP\S);

%===Sat Apr  7 19:11:54 ART 2001 mstorti@spider
%
Agregadas las funciones de entalpia. Ahora falta el termino SUPG. 

%===Tue May  1 09:40:20 ART 2001 mstorti@spider
%
Estuve trabajando en modificar completamente la clase 'nodedata',
'elemset', etc... eso fue hasta la version beta-1.69 ahora vuelvo a la
beta-1.67. 

%===Tue May  1 10:01:27 ART 2001 mstorti@spider
%
En 'onedstr' hay que reemplazar por
  if h(2)>max(h([1 3]))

%===Tue May  1 19:50:49 ART 2001 mstorti@spider
%
Added `report_option_access' to ns module. 
Added a `steady' option. If set to one, then it is equivalent 
to have Dt=inf, but Dt is used to report time instants, etc... 

%===Tue May  1 19:52:40 ART 2001 mstorti@spider
%
Esta version fue vuelta atras a la beta-1.67, dejando de lado por
ahora todos los 'avances' en cuanto a nueva interfase. Campos de
entrada salida, etc... 
%
Corre bien todos los test 'oscplate_all'. 

%===Thu May  3 18:33:27 ART 2001 mstorti@minerva
%
Para modificar el copyright hay que tocar:
doc/license.txt, src/version.cppin y doc/latex2html.init

%===Thu May  3 19:05:51 ART 2001 mstorti@minerva
%
El problema al linkeditar con /linux: ... se debe a que si en
Makefile.base llegas a poner 'BOPT = O_c++   ' entonces los trailing
spaces te joden!!!

%===Fri May  4 19:11:45 ART 2001 mstorti@node1.beowulf.gtm
%
STL vector bug
==============
%
No compila en geronimo (o sea RH 5.2.)!! Para mi es un error en las
STL del compilador. Hay problemas con la implementacion de vectores y
da un error en el assembler. Los errores aparecen solo en NS en
ns.cpp, wall.cpp, walldata.cpp  y nsi_tet.cpp. Por ahora la solucion
es pasar 'hmin' como 'USER_DATA' en donde este el compilador viejo. Y
hacer una serie de trucos para que no use ciertos vectores STL en esos
archivos. Espero que esto cambie cuando upgrademos el compilador en
geronimo... 

%===Sat May  5 07:46:26 ART 2001 mstorti@spider
%
Cambio los VOID_IT por '.clear()'. En realidad lo que hago es
redefinir el 'VOID_IT'... Todo anda bien lo que quiere decir que se
puede ir reemplazando los VOID_IT por los clear y finalmente eliminar
el VOID_IT. 

%===Fri May 11 20:35:37 ART 2001 mstorti@spider
%
Voy a corregir el bug: bug100. Parece que es cuando un elemset no
tiene elementos pero tiene ghost-elements. 

%===Sat May 12 15:06:17 ART 2001 mstorti@spider
%
Resuelto el problema, ahora aloca un chunk_size que es al menos el
numero de elementos locales + numero de elementos ghost locales.

%===Sat May 12 15:07:19 ART 2001 mstorti@spider
%
Cambio las definiciones de PETSCFEM_ERROR y PETSCFEM_ASSERT. Ahora
petscfem_error emite el error con PetscPrintf() y PETSCFEM_ERROR llama
a petscfem_error con , ademas el numero de linea y archivo. Lo mismo
para PETSCFEM_ASSERT y petscfem_assert. 

%===Sat May 12 18:26:40 ART 2001 mstorti@spider
%
El bug100 se producia cuando en 'elemset.cpp' al definir el chunk_size
para alocar las cantidades locales. Cuando `nelem_here' era 0 pero habia
`ghost_elements', entonces el 'chunk_size=0' y se ve que se armaba lio
porque no alocaba memoria y despues daba un SIGSEGV. 

%===Sat May 12 18:28:55 ART 2001 mstorti@spider
%
Le hice unos retoques esteticos a `myexpect.pl'. 

%===Mon May 21 12:29:26 GMT+3 2001 mstorti@minerva
%
Merged kepsilon from beto.

%===Mon May 21 13:15:59 GMT+3 2001 mstorti@minerva
%
Comienzo a trabajar en k-epsilon. 

%===Thu May 24 22:57:04 ART 2001 mstorti@spider
%
Paso `genload' a `advdif' (Despues habria que hacerlo en forma
generica). Parece que empieza a andar. 
%
Falta:
* agregar diferentes source term y que no haya double layer. 
* elementos 1D. 
* entalpias (termino temporal)
Hacer los siguientes tests: 
* muchos genload

%===Wed May 30 13:56:43 ART 2001 mstorti@minerva
%
En la documentacion no se debe usar el environment 'align' o cualquier
otro que genere mas de un numero de ecuacion por 'display' porque sino
sale mal el numero de ecs. 

%===Fri Jun  1 11:20:07 ART 2001 mstorti@minerva
%
Debuggeando k-e: Un flujo homogeneo se hace inestable. La velocidad
lateral se hace inestable... Con la version original de Beto,
tambien. O sea que no es la opcion 'steady' que agregue. 
%
Habra una llave para desactivar la turbulencia?

%===Fri Jun  1 11:36:09 ART 2001 mstorti@minerva
%
Faltaba definir la viscosidad en el archivo de datos... 
Tambien habia que fijar un nodo de presion paa que no se dispare.
%
No se porque cuando se impone velocidad, p, k y e  a la entrada
entonces la matriz da singular si la presion es impuesta en el mismo
nodo. Hay que hacerlo en otro nodo. Se arregla imponiendo la presion
en un nodo a la salida. Tal vez se arregle cuando usemos weak_form. 
%
A esta altura produce bien el perfil de k y epsilon para un flujo
homogeneo. Voy a empezar a ver la restriccion no-lineal. 

%===Fri Jun 15 13:08:46 ART 2001 mstorti@minerva
%
k-e con ley de pared: Ahora con los jacobianos agregados por Beto
se banca pasos de tiempo muchos mas grande. Puedo correr con Dt=30. 
%
Incluso se banca Dt=30 desde el principio!! Pero los resultados dan
para la mierda... 
%
Lo mismo pasa para Dt=10...
%
Aumentando nnwt de 3 a 5 tampoco anda mejor. 
%
Recien con Dt=3 parece andar bien...
%
Para ese Dt, con 80 iteraciones ya da buenos resultados. 

%===Fri Jun 15 19:05:29 ART 2001 mstorti@spider
%
Para canales (Poiseuille y Couette) da bien. Para pipes (axisimetrico)
Poiseuille da bien. 

%===Sun Jun 17 12:49:15 ART 2001 mstorti@spider
%
Debuggeando k-e para el caso del canal. Encuentro que hay un problema
con los jacobianos. Cuando se desactiva la llave 'turbulence_coef=0'
entonces se deberia deactivar la turbulencia con lo cual el problema
deberia ser casi lineal. En el caso de flujo paralelo deberia ser
lineal. Pero esto no pasa, da convergencias tipo 1e-3, -5, -10 -15. Y
cuando miro los jacobianos numericos y analiticos con
'verify_jacobian_with_numerical_one' me da que el jacobiano numerico
tiene muchos elementos diagonales un factor 3 los analiticos!!! 
%
Parece que hay un factor 'Dt' entre el jacobiano analitico y el
numerico, pero eso se debe a una cuestion de convencion. 
%
El caso canal Poisuille, periodico da perfectamente lineal. 

%===Thu Jun 21 10:48:50 ART 2001 mstorti@minerva
%
Corriendo el caso 2D. 
%
Se produce una inestabilidad en k y epsilon a la salida, cerca de la
pared. Supongo que es la restriccion, voy a poner que el ultimo valor
de la pared mire al anterior en k y en epsilon.
% 
Da un problema con la resolucion de la ecuacion de transporte en K y
en epsilon. 

%===Sat Jun 23 09:31:20 ART 2001 mstorti@spider
%
Habia un error con los jacobianos de los terminos de produccion al
desactivar las banderas de turbulencia (`turbulence_coef' y
`turb_prod_coef'). 
%
Ahora da bien para Re bajos y altos (hasta 1000) en laminar, con
turbulencia desactivada. Converge bien en estacionario y con Dt=1. 
Pero da un ruido en el refinamiento de malla, por ejemplo en el perfil
de velocidad en el centro del canal o en el caudal. Voy a investigar
esto. 

%===Sat Jun 23 12:07:54 ART 2001 mstorti@spider
%
Decididamente la violacion de continuidad se debe al termino PSPG
cuando hay curvatura en la presion o cuando hay malla
variable. Recordar que el termino de estabilizacion es del tipo a
d/dx(h^2/nu dp/dx) o sea que o bien se activa si h=cte y p,xx =
0. Esto quiere decir que en el canal o ducto, con h=cte no aparece,
pero de todas formas eso tiene patas cortas, por ejemplo si la seccion
no fuera constante con lo que habria un 'p,xx \neq 0'. 
%
Podemos pensar que u + h^2/nu p,x = cte, entonces la perdida de masa
sera mas o menos 'Du = D(h^2) p,x'. En el canal tenemos que 'p,x = nu
* umax / L^2' donde L es el ancho del canal y entonces ' Du/umax =
h^2/L^2'. O sea que si usamos elementos muy alargados se nota mas el
efecto. 
%
Probe con 'pspg_factor=0.1' y da una perdida de masa muy pequena pero
da la presion muy oscilatoria. Con 'pspg_factor=0.3' da una perdida de
masa del 0.3% y el campo de presion es razonable. 
%
Tambien agregue un termino 'pspg_advection_factor' para controlar la
incidencia de la advecccion en la estabilizacion PSPG, pero
(obviamente) a Re bajos no incide. 
%
Incluso con 'pspg_factor=1' (pero 'pspg_advection_factor=0') dan
oscilaciones en la esquina entrada/pared.
%
Tambien dan oscilaciones con 'pspg_advection_factor=1' (hay que tener
en cuenta que la cantidad de estabilizacion baja. 
%
Con 'pspg_factor=2', 'pspg_advection_factor=1' da todavia oscilatoria la
presion pero la continuidad da bastante bien (baje la longitud 'x' de la
malla de 20 a 10, con lo cual 'h' bajo a la mitad y el termino de
estabilizacion 1/4.

%===Sun Jun 24 20:41:09 ART 2001 mstorti@spider
%
Ahora da bien el problema con 'turbulence_coef=1' y
'turb_prod_coef=0', pero cuando activo toda la turbulencia, es decir
'turb_prod_coef=1' entonces ahi se va al diablo en la salida. 'k' toma
valores negativos. Tambien 'k' tiene oscilaciones donde hay
refinamientos, asi que sospecho que debe haber alguna problema. 
%
El ruido tambien se produce en la velocidad.  Voy a desactivar
'turb_prod_coef=0' para ver si es una realimentacion a partir de K y E
o si la ecuacion de momento sola produce un salto. 
%
Con 'turb_prod_coef=0' tambien da un ruido en la velocidad en la
primera iteracion de Newton. Parece estar relacionado con el elemento
de pared (el eallke). Voy a correr varias iteraciones de Newton a ver
si es el jacobiano o el residuo o los dos. 
%
El pico tambien se produce en la solucion convergida. 
%
Puede ser que haya que pesar la condicion de contorno (es decir el
wallke) con la funcion perturbada? Creo que no... 
%
Con Re=0.1 tambien da un poco oscilatorio asi que no tiene que ver con
la adveccion. 
%
Con 'steady=1' tambien da un poco oscilatorio asi que no parece tener
que ver con el paso de tiempo. 

%===Mon Jun 25 20:17:57 ART 2001 mstorti@spider
%
Ahora con la version lumped anda mucho mejor. Pero de todas en formas
en steady revienta (bueno eso seria demasiado pedir). Vamos a probar
con un paso de tiempo menor. 

%===Mon Jun 25 21:35:40 ART 2001 mstorti@spider
%
Revienta igual. El lumped no parece dar mejor. Voy a probar
desactivando el 'turb_prod_coef=0'. 

%===Sun Jul  1 17:39:27 ART 2001 mstorti@spider
%
Lo pude hacer converger, para Re=1e4, 
    $turbulence_coef=1;         # mask turbulence
    $turb_prod_coef=0;          # mask turbulence production terms
lumped=0; converge bien, si bien da `eps' negativo en dos puntos sobre
la pared a la entrada. 
%
Haciendo que 'uwall' crezca desde cierto valor no unitario mejora
bastante. Ahora vamos a probar si se puede reducir la longitud de
empalme. 
%
Con longitud de empalme 1 (`$uwall_match_len=1') tambien
converge. Ahora voy a probar prendiendo '$turbulence_coef=1'. 
%
Tambien da con `$uwall_match_len=1' prendido. Voy a probar a ver si se
puede bajar todavia mas la longitud de empalme. 
%
Con longitud de empalme por debajo de 0.5 da valores de E negativos a
la salida. Como si hubiera un problema con la condicion de contorno. 

%===Mon Jul  2 09:13:22 ART 2001 mstorti@spider
%
Con funciones 'cutoff' (1e-3 para k y 1e-4 para eps) converge con
longitud de empalme .3 y Re=1e4 y da perfiles suaves, etc... Cuando lo
bajo a longitud de empalme .1 y Re=4e5 se va a la mierda enseguida. 
%
Con LE=.3 y Re=4e5 tambien diverge.
%
Con Re=1e5 diverge. 
%
Con Re=5e4 converge inicializando de la solucion a Re=1e4. Los cutoff
values todos en `kap_ctff_val 1e-4', `eps_ctff_val 1e-4'.

%===Fri Jul  6 12:31:51 ART 2001 mstorti@minerva
%
Pruebo a correr con turbulencia y con jacobi a ver como afecta la
convergencia. 
%
En el problema 1D (con peri) la convergencia es muy buena. 

%===Sun Jul  8 11:22:23 ART 2001 mstorti@spider
%
Estoy jugando con los parametros 
%
lagrange_diagonal_factor (abrev. ldf)
lagrange_residual_factor (lrf)
lagrange_scale_factor (lsf)
%
Si `ldf=0' entonces el `lu' no anda y el iterativo no converge. 
Si `ldf>0' y 'lrf=1' entonces introducimos un error de consistencia. 
Si `ldf>0' y 'lrf=0' entonces hacemos Newton, converge bien GMRES pero
no converge el Newton. 

%===Sun Jul  8 19:43:25 ART 2001 mstorti@spider
%
Incluso con "lu", no converge para nada si usamos "lrf=0". No tengo en
claro porque. 
%
21:25:56 ART: Todo se arreglo al eliminar el elemento de restriccion
primero. Ya que todas las variables del primer nodo en la pared estan
fijas. De todas formas deberia converger!!!

%===Mon Jul  9 12:18:49 ART 2001 mstorti@spider
%
Ahora converge razonablemente pero no puedo empezar directamente con
iterativo y despues de un cierto numero de iteraciones se clava. Voy a
probar si pasa lo mismo con "lu". 
%
12:22:04 ART: Con "lu" converge bien. Voy a probar a ver si es el
`lagrange_diagonal_factor'. 
%
13:06:59 ART: Pareceria ser que si usas `ldf>lsf' entonces diverge. Si
usas `ldf<<lsf' entonces converge rapido pero despues se estanca. Si
usas `ldf<lsf' (pero no `ldf<<lsf') entonces converge lenta pero
monotamente. 
%
14:10:48 ART: Reviso un poco lo anterior. Corro con `lsf=1e-3' y
`ldf=1e-2,1e-3,1e-4,1e-5,1e-6'. Diverge para `ldf=1-2' y converge
mejor cuanto mayor es el `ldf'. En el limite se estanca  para
`ldf=0'. 

%===Mon Jul  9 17:30:21 ART 2001 mstorti@spider
%
Puedo obtener una solucion completa con metodo iterativo. 
Con 
> Dt 1
> steady 0
> lagrange_scale_factor 1e-3
> lagrange_diagonal_factor 1e-3
> lagrange_residual_factor 0 
> maxits 100
> nnwt 4
%
y empezando 10 iteraciones 
> newton_relaxation_factor 0.1
y a partir de ahi siguiendo con 
> newton_relaxation_factor 1
%
converge a 1e-15 en unas 1400 iteraciones. Al final tiene una tasa de
100 iteraciones por orden de magnitud. (100 iter quiere decir 25 Dt  *
nnwt=4). 
%
17:50:31 ART: Corro el problema completo (100 nodos en direcicon y,
Ly=60). No converge con los parametros anteriores. Pruebo a bajar el
`ldf' a 1e-4.

%===Mon Jul 16 10:36:09 ART 2001 mstorti@spider
%
Escribiendo el IISD (Interface Iterative, Subdomain Direct)
solver. Para un procesador esta dando lo mismo, curiosamente no esta
accediendo a las opciones de iteracion varias veces (tantas como
factorizaciones de la matriz) sino una sola vez. 
%
No!!! Es al reves, ahora lo lee varias veces, cada vez que va a
resolver. (No se si esto puede traer problemas despues...)
%
10:52:19 ART: Ahora voy a verificar que ande bien con mas de un
procesador y con metodos iterativos. 
%
Aparentemente anda bien, pero las versiones viejas no andan bien con
mas de un procesador. 

%===Wed Jul 18 10:44:00 ART 2001 mstorti@spider
%
No andaba bien, porque aparentemente no es cierto que si llamas a
`KSPSetMonitor' con PETSC_NULL entonces desactiva el monitoreo sino
que se cuelga. 

%===Wed Jul 18 18:41:27 ART 2001 mstorti@spider
%
Decubri un error en `read_vector': si no hay suficientes valores en el
archivo entonces ahora manda un mensaje de error, antes probablemente
quedaba basura. 
%
18:55:24 ART: Voy a debuggear porque a partir de la segunda iteracion
no converge tan rapidamente como debiera??? 
%
18:57:52 ART: Ahora anda bien. 
%
19:03:01 ART: Voy a empezar a debuggear con varios procesadores. 
%
20:30:57: El SLES hay que volverlo a crear cada vez, sino no factoriza
la matriz asociada. 

%===Thu Jul 19 23:28:02 ART 2001 mstorti@spider
Da un SIGSEGV cuando en `nsitetlesfem2' cuando se usa
cache. Desactivando el cache no hay problema (despues da error al
cargar con `set_value'). 
%
00:11:29: Desactivando el cache solo en `comp_mat' da SIGSEGV igual. 

%===Fri Jul 20 07:19:57 ART 2001 mstorti@spider
%
Desactivando el cache a partir de un cierto punto con
`deactivate_cache()' no da SIGSEGV. 
%
08:28:43: Activando cache y desactivando en ciertas partes de
`nsitetlesfem2' puedo hacer que de SIGSEGV o no pero no avanzo
nada. Voy a desactivar el cache y avanzar con el tema del IISD en si. 
%
11:29:40: Se bloquea (tipo bloqueo por MPI) en `assembly_begin' de las
matrices LI, IL o II . Con el debugger se ve que esto occurre al hacer
un MPI_Allreduce. . Voy a probar si se debe al tema de no llamar a
`assemble' cuando el numero de elmentos procesados es nulo. 
%
11:49:26: Probe con lo de sacar el `if (iele_here > -1) {' pero no
anduvo. Voy a probar a hacer solamente el `FINAL_ASSEMBLY'. 
%
14:45:33: Voy a ver si tampoco anda sacando el `set_value' para los
bloques no `LL'. Si saco los `set_value' para todos los bloques menos
el `LL' entonces se bloquea en el MPI_Allreduce de
`local_has_finished'. 
%
Habia un lio con el error que mandaba el `IISDMat::create' que al
tirar un error generaba un deadlock. Para arreglarlo, resolvemos el
problema del bloque `LL' parcialmente. Cambiamos la numeracion de
nodos en `readmesh' de tal forma que ahora los nodos que estan
conectados a mas de un procesador se asignan al procesador de
numeracion mas alta. Despues, en las matrices  `IISDMat' como
consideramos a los dof's como interface aquellos que estan conectados
con un dof en un procesador de numeracion menor, entonces esto hace
que los dof's marcados como locales reciben contribuciones solo de
elementos locales. 
%
Pero esto tiene un problema: si el elemset no es considerado FAT
entonces podria ser que depues genere elementos `locales' en otro
procesador. De todas formas vamos a tener que hacer algo despues para
mandar las contribuciones locales a otros procesadores.  

%===Mon Jul 23 13:28:17 ART 2001 mstorti@spider
%
IISD is working!! Verified that works well for solution of several
systems (several iterations and time steps).
%
%
13:28:53: Voy a chequear que no pierda memoria. 
%
13:58:40: Efectivamente pierde memoria. Voy a verificar...
%
Ya encontre el error, ahora no pierde memoria. Probe con una caso mas
grande. Ahora habria que verificar que no refactoriza la matriz. 
%
Converge razonablemente, hay que probarlo en el cluster. 

%===Mon Jul 23 19:46:25 ART 2001 mstorti@spider
%
Anda bien la implementacion del producto de la matriz traspuesta para
IISD de manera que permite usar metodos como CGS, BiCG, etc... 

%===Wed Jul 25 12:45:16 ART 2001 mstorti@minerva
%
Haciendo upgrade a RH 7.1: las librerias 'glib' van a parar a
`/usr/include/glib-1.2; y a `/usr/lib/glib/include'. Este ultimo lo
pongo en una variable CPPFLAGS definida en el `Makefile.defs'. 

%===Wed Jul 25 15:21:15 ART 2001 mstorti@minerva
%
Al compilarlo en minerva cn RH 7.1 no tuvo problemas con IISD en mas
de un procesador. Ahora voy a probar con algo mas grosso en el
cluster. 

%===Wed Jul 25 21:10:18 ART 2001 mstorti@spider
%
Con la nueva version del compilador (RH 7.1, anda bien con IISD, en 2 y 3
procesadores (gcc version 2.96 20000731 (Red Hat Linux 7.1 2.96-81)
virtuales. 

%===Sat Jul 28 11:09:27 ART 2001 mstorti@spider
%
Corregido un error en cuanto a que no se retornaba error en algunas 
rutinas de `IISDMat' y `PETScMat'. 
%
%
11:12:00: Con RH 7.1 en casa vuelven a aparecer los problemas en
`advdif' para NP>1 en la dealocacion de memoria de los vectores
STL. Voy a probar a hacer que ciertas variables locales sean
estaticas, asi cuando las borra no hay drama. 
%
%
11:15:49: declarando 
%
static vector<int> nnz[2][2][2],flag0,flag,*d_nnz,*o_nnz;
%
no anda tampoco. 

%===Sat Jul 28 16:20:03 ART 2001 mstorti@spider
%
Encontre el error. Al crear los `d_nnz' `o_nnz' para IISDMat entonces
estaba mal el calculo de `row' y escribia en partes no habilitadas de
`d_nnz' o `o_nnz'. 
%
Ahora voy a escribir una clase 'DistMap' (distributed map) o
`distributed container'. 

%===Tue Aug  7 20:46:00 ART 2001 mstorti@spider
%
Escrita la clase DistMap y funciona bien. Habia un bug, escribi un
test para ello.
%
Cree un nuevo particionamiento llamado `random' que asigna un
procesador aleatorio para cada elemento. Esto sirve para debuggear
cosas. Por ejemplo asi se detecta mas facil el bug mencionado
anteriormente. 

%===Wed Aug  8 14:17:17 ART 2001 mstorti@minerva
%
Hace falta incluir un precondicionamiento para el IISD, aunque sea los
elementos diagonales de la matriz de FEM (no la de SCHUR). 

%===Thu Aug  9 16:35:11 ART 2001 mstorti@spider
%
El precondicionamiento diagonal fue implementado y anda muy bien!! 
%
En geronimo da SIGSEGV al correr compilado con optimizador. En spider
no, anda con optimizador para 3 procesos. 

%===Thu Aug 16 16:24:41 ART 2001 mstorti@node1.beowulf.gtm
%
Evaluacion de performance con IISD. Corremos el auto (malla
auto_11.epl, 11873 nodos, 22600 Navier Stokes 2D, a Re=3e6). 
%
Con IISD: 
=========
Con NP=10 procesadores usa 
%
94.72user 6.16system 2:03.61elapsed 81%CPU (0avgtext+0avgdata 0maxresident)
%
para hacer 20 pasos de tiempo con nnwt=1 (o sea 20 resoluciones del
sistema lineal). En promedio consume unos 30 y pico de iteraciones
para cada resolucion).  La tolerancia interna (del lazo de GMRES SOBRE
LA INTERFACE) puesto a 1e-3.
%
Con GMRES sobre el sistema global:
=================================
%
Para la misma precision (guarda que el operador es otro!!, en IISD es
la matriz complemento de Schur, aqui es la global). En promedio no
converge a 1e-3 en 200 iteraciones. Como los vectores son grandes se
ralentiza bastante a medida que avanza en GMRES y tarda:
%
396.97user 22.72system 8:52.55elapsed 78%CPU (0avgtext+0avgdata 0maxresident)k
%
Es de esperar que la relacion entre IISD y GMRES global sea mas
favorable a IISD si bajamos la tolerancia ya que se ralentiza menos
(el grueso del trabajo todavia esta en la factorizacion y
retrosustitucion) y la tasa de convergencia es mejor.
%
Con LU
======
%
Por supuesto solo se puede correr en un solo procesador. Se hace una
sola iteracion de GMRES. Tarda
%
1097.67user 5.44system 18:34.64elapsed 98%CPU (0avgtext+0avgdata 0maxresident)k
%
Lo cual es un factor 10 sobre el IISD. Ademas esta casi en el limite
de la memoria (son 10000 nodos, 40000 dof's, ancho de banda medio 400,
con lo cual son unos 16e6 elementos, 120 Mb aprox.). 

%===Thu Aug 16 17:18:28 ART 2001 mstorti@node1.beowulf.gtm
%
Reporte escrito por Jorge D'Elia
================================
%
Gauss solution on the cluster:
%
system size                  -> n
RAM for matrix A             -> r = 8*n^2/1e6
processor number             -> z
RAM of matrix A on each node -> d = r/z
%
------------------------------------------------------------------ 
A on one node:
  n = 3000 number of unknowns
  z =    1 processor
  r =   72 MBytes for whole matrix A
  d =   72 MBytes of A on each node
%
  mpirun -machinefile machi.dat -np 1 gauss3.exe      
%
  times for array with leading dimension          3000
      factor     solve      total     mflops       unit      ratio
  6.760E+02  0.000E+00  6.760E+02  2.665E+01  7.504E-02  1.207E+04
  end of test 
%
------------------------------------------------------------------ 
B on three nodes
  n = 3000 number of unknowns
  z =    3 processors
  r =   72 MBytes for whole matrix A
  d =   24 MBytes of A on each node
%
  mpirun -machinefile machi.dat -np 3 gauss3.exe
%
  times for array with leading dimension          3000
      factor     solve      total     mflops       unit      ratio
  3.200E+02  1.400E+01  3.340E+02  5.395E+01  3.707E-02  5.964E+03
  end of test 
%
------------------------------------------------------------------ 
C speed_up_{np = 3} = 5.395E+01 / 2.665E+01 = 2.0244
%
------------------------------------------------------------------ 
D corrida con el maximo tamanio del sistema posible en el
  actual cluster:
%
  n = 6500 number of unknowns
  z =    3    processors
  r =  338    MBytes for whole matrix A
  d =  112.67 MBytes of A on each node
%
  pghpf -Mautopar -Mmpi -o gauss3.exe gauss3.hpf
%
  mpirun -machinefile machi.dat -np 3 gauss3.exe
%
  times for array with leading dimension          6500
      factor     solve      total     mflops       unit      ratio
  3.152E+03  3.800E+01  3.190E+03  5.742E+01  3.483E-02  5.696E+04
  end of test 
%
------------------------------------------------------------------ 

%===Thu Aug 16 18:14:56 ART 2001 mstorti@node1.beowulf.gtm
%
Reporte de eficiencia para IISD:
=================================
Se corre el canal a Re=1e4 con 
%
iisd: IISD + preco jacobi 10 procesadores
lu: LU en 1 proc.
gmresg: GMRES + Jacobi global en 10 procs.
%
Para N=16, Ny=60, Ly=1, (malla `medium') da:
%
iisd: (Porque da tan alto el system?). Convergencia siempre a 1e-3 
10.71user 8.58system 0:29.69elapsed 64%CPU (0avgtext+0avgdata 0maxresident)k
lu:
28.98user 0.67system 0:31.59elapsed 93%CPU (0avgtext+0avgdata 0maxresident)k
gmresg: (La convergencia es muy mala, no ceonverge en ningun caso)
13.67user 16.74system 1:07.31elapsed 45%CPU (0avgtext+0avgdata 0maxresident)k
%
Malla 'very_large' de N=32 por Ny=120, Ly=10 ((N=3993 nodos)
============================================
%
lu: 
191.73user 2.76system 3:17.78elapsed 98%CPU (0avgtext+0avgdata 0maxresident)k
iisd: (En promedio usa al principio entre 120 y 150 iteraciones y
               despues va bajando, porque salta por atol)
65.65user 11.38system 1:52.05elapsed 68%CPU (0avgtext+0avgdata 0maxresident)k
gmresg:
53.17user 23.63system 3:58.88elapsed 32%CPU (0avgtext+0avgdata 0maxresident)k
%
Malla de 64 x 250 (N=16315 nodos):
==================
gmresg: la convergencia es pesima!! En 200 iteraciones no baja un 20%
           el residuo!!
174.78user 20.53system 5:24.05elapsed 60%CPU (0avgtext+0avgdata 0maxresident)k
iisd: tarda entre 140 y 190 iteraciones al principio. Al final baja
       bastante porque salta por atol. 
388.86user 16.09system 9:01.87elapsed 74%CPU (0avgtext+0avgdata 0maxresident)k
lu:
1465.77user 11.82system 25:00.16elapsed 98%CPU (0avgtext+0avgdata 0maxresident)k
%
Malla de 128 x 500 (N=64629 nodos):
==================
%
gmresg: no converge ni a canhonazos
lu: no entra en memoria
iisd: converge bien, 4 ordenes de magnitud en approx 200 iter. Usa 66
Mb de memoria. Usa unos 4' por iteracion, pero se podria bajar
bastante ya que podriamos usar una tolerancia de 1e-2 con lo cual
bajaria el tiempo a la mitad. La tasa de convergencia es de 60
iter/orden. Los 20 pasos de tiempo tardarian unos 80'. Con respecto al
problema de 64x250 vemos que el tiempo por iteracion aumento un factor
9, con lo cual el costo re resolucion va como N^1.6, donde N es el
numero de nodos. N=64629 nodos en la malla de 128x500. Ahora vamos a
tratar de probar con un problema 3D. Creo que andaba por ahi el LES. 

%===Sat Aug 18 17:11:14 ART 2001 mstorti@spider
%
Sinopsis de uso de PFMat:
%
  PFMat *A;
  A = PFMat_dispatch(solver.c_str());
  argl.arg_add(A,PROFILE|PFMAT); 
  // call to `assemble' to define profile and setup matrices
  for (....) { // loop over Dt or whatever that reuses the matrix profile
    A->zero_entries(); // This must be present
    argl.arg_add(A,OUT_MATRIX|PFMAT);
    // call to assemble to set values
    A->view(matlab);
    A->build_sles(GLOBAL_OPTIONS);
    for (...) {  // Solve several times with the same matrix
       ierr = A->solve(res,dx); CHKERRA(ierr); 
    }
    ierr = A->destroy_sles(); CHKERRA(ierr); 
  }
  delete A;

%===Sun Aug 19 13:23:51 ART 2001 mstorti@spider
%
`test/distmap/tryme4.cpp': Studying how to make the solution of local
systems with LU decomposition more efficient. Tratando de usar
`KSPPREONLY', `PCLUSetFill()' y `PCLUSetUseInPlace(pc)'. 
%
con:
    ierr = KSPSetType(ksp,KSPGMRES); CHKERRA(ierr); 
    ierr = KSPSetTolerances(ksp,0,0,1e10,1); CHKERRA(ierr); 
marca 25740 Kb de memoria usada (con top). 
%
con: 
    ierr = KSPSetType(ksp,KSPPREONLY); CHKERRA(ierr); 
marca 21688 Kb de memoria usada.

%===Sun Aug 19 17:23:11 ART 2001 mstorti@spider
%
otra prueba: Con N=100000 y M=100 la matriz factorizada requiere unos
16Mb. Con `pc_lu_fill=1.7' puedo correr con `ulimit -v 30000' mientras
que sin setear `pc_lu_fill' necesito ir a mas de `ulimit -v 35000'. 
%
19:31:15: Separo `iisdmat.h' de `pfmat.h'. 

%===Mon Aug 20 10:17:09 ART 2001 mstorti@spider
%
Voy a empezar a escribir el precondicionamiento para IISD. 
Branchear a partir de beta-2.02 si se quiere evitar el lio generado
por esto. 
%
11:39:52: Fuerzo a que los perfiles sean simetricos, haciendo que en
`compute_prof' se inserten `j,k' y `k,j'. 

%===Tue Aug 21 22:08:28 ART 2001 mstorti@spider
%
Escribiendo la clase DistMap (distributed Container). Habia un error
que ponia hacia pack tambien para mandarse a si mismo (myrank ->
myrank). Entonces hay que poner:
%
      if (k!=myrank) pack(*iter,send_buff_pos[k]);
 
%===Tue Aug 21 22:25:56 ART 2001 mstorti@spider
%
Works well with changed names from `distmap2.xxx' to `distcont.xxx'

%===Sat Aug 25 12:08:44 ART 2001 mstorti@spider
%
Adding Finite Difference Jacobian support to PFMat.
%
En principio no lo pude hacer andar asi que me voy a una version
estable anterior para avanzar con el tema del sloshing.  Hay que
implementar la clase PFMat como una maquina de estados (el esbozo esta
en `doc/OBJ/pfmat.fig'. 
%
Vuelvo a la version `beta-1.82'. 

%===Fri Sep  7 15:12:01 ART 2001 mstorti@minerva
%
Me daban mal los jacobianos, habia un bug en la version beta-1.82 para
el calculo de jacobianos. Se pasaba mal el `glob_param' eso se
arreglaria con `dynamic_cast' etc... 

%===Fri Sep  7 16:04:47 ART 2001 mstorti@minerva
%
Con jacobiano numerico anda bien si en las contribuciones a la
ecuacion de momento `resmom' agrego solo el termino du/dt. Voy
agregando otros terminos y veo. Si agrego el convectivo sigue andando
bien. 
%
Se jode cuando agrego el termino de viscosidad $\Delta!u$. 
%
La viscosidad hay que ponerla muy pequenha porque si la pones a 0 te
da una singularidad en el Peclet, etc... 

%===Sat Sep  8 09:57:15 ART 2001 mstorti@spider
%
Comentando todo da Ainf=ADt=0. Ahora voy a probar a prender solo la
matriz de masa. 
%
10:13:32: Poniendo solo la masa da perfecto. Ainf es 0. ADt da igual
para todos los campos y la suma de los elementos da 29.800 que
dividido por 3 (por campos) y por el Dt que es 0.01 da 9.9335e-02 que
es el volumen del sector de cilindro 0<=z<=1, r<=1, 0\le\theta\le
.2. cuyo volumen es h*\Delta\theta*R^2/2=.1, pero el volumen del area
discretizada (por el error entre la cuerda y el arco de
circunferencia) es h*\cos(\Delta\theta)*\sin(\Delta\theta)*R^2 que da
exactamente eso. Ahora prendemos el termino de viscosidad y apagamos
el temporal. 
%
10:28:13: Apagando el termino temporal y prendiendo viscosidad
efectivamente dan diferentes ADt de Ainf (deberian dar iguales). 
%
10:31:00: Los dos dan suma 0, esta bien. 
%
10:32:42: Guarda!! que me parece que estaba mirando mal. 
%
10:40:33: Efectivamente, ahora pongo todo y da lo que tiene que
dar. Todo el error se debio al error en el calculo de los jacobianos
numericos. 
%
11:26:19: Con upwind (`tau_fac=1') tambien anda todo bien. 

=======
%===Fri Sep  7 15:12:01 ART 2001 mstorti@minerva
%
Me daban mal los jacobianos, habia un bug en la version beta-1.82 para
el calculo de jacobianos. Se pasaba mal el `glob_param' eso se
arreglaria con `dynamic_cast' etc... 

%===Fri Sep  7 16:04:47 ART 2001 mstorti@minerva
%
Con jacobiano numerico anda bien si en las contribuciones a la
ecuacion de momento `resmom' agrego solo el termino du/dt. Voy
agregando otros terminos y veo. Si agrego el convectivo sigue andando
bien. 
%
Se jode cuando agrego el termino de viscosidad $\Delta!u$. 
%
La viscosidad hay que ponerla muy pequenha porque si la pones a 0 te
da una singularidad en el Peclet, etc... 

%===Mon Sep 10 12:10:44 ART 2001 mstorti@minerva
%
Voy a probar a tomar la matriz sin imponer p=0 en un nodo y ver sobre
esa matriz que pasa al imponer p=0 en un nodo o el otro. 
%
15:41:52: Poniendo la velocidad impuesta a la salida (en r=Rint) da
bien. Es decir el perfil de velocidades da variando inversamente
proporcional al radio (por continuidad). 
%
16:31:20: El problema con velocidad circunferencial da perfecto. Con
velocidad proporcional al radio y proporcional a la inversa del radio
(las dos soluciones fundamentales). 

%===Tue Sep 11 08:29:58 ART 2001 mstorti@minerva
%
El problema para el modo 1 con velocidad uniforme segun x no anda
bien. Pasa que no puedo usar complejos con el paquete sparse. Entonces
escribo el rotconden de forma que ahora trabaja todo en full. Si eso
anda despues pasare las rutinas del paquete SPARSE a complejos. 
%
10:10:30: Verifico que rotando las matrices A22 da A11 y A12 da A21. 
%
12:14:58: Anda bien el caso velocidad uniforme segun x. 

%===Thu Sep 13 10:55:51 ART 2001 mstorti@minerva
%
Ayer andaban bien los modos de oscilacion, me concentre en el programa
de visualizacion y ahora no puedo recuperar bien que me calcule modos
de oscilacion a frecuencias relativamente altas. 
%
13:20:45: Ahora empieza a andar bien (todavia no se bien porque no
andaba). Voy a probar a refinar la malla y reducir el diametro
interno. 
=======
%===Sun Sep  9 10:19:42 ART 2001 mstorti@spider
%
El `rotconden' anda ma o menos bien. Para la matriz de masa preserva
la masa total. 
%
17:08:22: Flujo vertical: da bien. Flujo radial, no da bien. Incluso
dejando que el valor a la salida lo saque solo, es decir dejando
u_normal a la salida libre. 
%
18:50:02: El flujo radial da "mas o menos" bien. La velocidad aumenta
hacia el eje, pero no todo lo que deberia y la presion tiene una
oscilacion fuerte. Me imagino que es por la imposicion de la presion. 

=======
%===Mon Sep 10 12:10:44 ART 2001 mstorti@minerva
%
14:20:50: Lo que parecia ser una onda de alta frequency era un error
de no correr el `rock2' despues de volver a correr un problema
incrementando el numero de nodos. 

%===Tue Sep 18 09:07:15 ART 2001 mstorti@minerva
%
En la region anular parecio que daba buenas ondas, ahora yendo hasta
el origen tiene sus dramas, sobe todo para el modo 1. Voy a volver a
probar con el modo 0. 
%
10:14:15: Estaba mal el calculo del `h_pspg': estaba con la formula 3D
y va la 2D. 
%
10:15:15: Estamos tratando de ver el modo 0. 
%
10:15:57: El `h_pspg' parece ser un problema. Como hacemos un calculo
de fourier en la direccion azimutal, entonces no hay que estabilizar 

%===Mon Sep 24 10:07:45 ART 2001 mstorti@minerva
%
Start writing a Sparse::Mat class based on SuperLU. Wrote already all
algebraic part, remains to add the `solve()' member based on
SuperLU. There is a problem with the fact that `SuperLU' is based on
columnwise stored representation. 

%===Sat Sep 29 09:41:20 ART 2001 mstorti@spider
%
Sparse::Mat is working as a finite state machine. Now I will include
the new Sparse::Mat (with SuperLu) as a sequential solver for IISD. 

%===Thu Oct  4 13:12:19 ART 2001 mstorti@minerva
%
Voy a agregar una restriccion como nonlr, pero ahora que pueda
involucrar a varios nodos. 
%
16:08:49: Detecte un error en el test `test/lupart' target
`test_iisd'. 

%===Sun Oct  7 18:11:01 ARST 2001 mstorti@spider
%
Corregido el jacobiano para weak_form=1 en `nsitetlesfm2.cpp'. 

%===Thu Nov  1 10:59:01 ART 2001 mstorti@minerva
%
El calculo del jacobiano numerico era super-lento con rockns.bin
(jacobiano en frecuencias). Resulta que el `MatAssemblyEnd()' hace un
laburo de eliminar aquellos elementos que no fueron seteados. Esto
hace que si en la primera iteracion no fueron seteados algunos
elementos entonces despues al compactarlo no te queda espacio para
lugares que eventualmente se pueden llenar y te da error si, por
ejemplo, pones `MatSetOption(A, MAT_NEW_NONZERO_ALLOCATION_ERR)'. Las
posibles soluciones son:
%
* Arreglar PETSc para que no compacte. 
* Que en `upload_vector' no mire el valor en si, sino una mascara. Aca
    habria que tocar solo PETSc-FEM
%
Ahora quedo medio emparchado porque en `upload_vector' le puse que
cargue todos los elementos, sin mirar si son nulos o no, pero si en
algun programa se uso la opcion de perfiles entonces tambien tendria
que haber devuelto perfil lleno. 

%===Mon Nov 12 14:06:19 ART 2001 mstorti@minerva
%
Agregue un nuevo particionador que permite correr con muchos mas
elementos. La idea es que de los $\Nelem$ elementos en la malla se
eligen al azahar $\Nelem/f$ donde $f$ es un numero digamos del orden
de 10 y se hace el grafo de conectividad haciendo `coalescer' los
elementos hacia estos puntos de precipitacion. Esto permite obtener
particionamientos razonables con mucho menos recursos. 
%
Tambien se agrego una opcion \verb+iisd_subpart+ de tal manera que en
realidad se particiona en $m$=\verb+size*iisd_subpart+
subdominios. Los subdominios $S_0$ a $S_{m-1}$ viven en el procesador
$P_0$, los  subdominios $S_m$ a $S_{2m-1}$ en el $P_1$, de manera que
el $S_j$ pertenece al procesador $P_{[j/m]}$ ($[x]$ denota la parte
entera de $x$). En realidad esto se hace permutando los dominios en
forma aleatoria de manera que en lo posible los dominios que viven en
un msmo procesador no esten en contacto. De esta forma en vez de tener
un subdominio grande tenemos pequenhos dominios disconexos. De esta
forma para $m=1$ tenemos el IISD clasico y para $m$ grande tendriamos
que todos los nodos serian interfase y el metodo seria puramente
iterativo. 
%
Resultados: Corriendo el `auto_32.epl' (183001 elementos, 40655 nodos)
y poniendo $m=20$ con 11 procesadores es decir que en realidad Metis
esta particionando en 200 subdominios (cada uno con del orden de 830
elementos) tenemos unos 40secs. por paso de tiempo (1 iteracion de
Newton por Dt). Usado $m=5$ tenemos 1min 50secs para calculo de
residuo/matrices  y factorizacion y 10'' para iterar GMRES (sin
debugger). Para $m=20$ tenemos 
%
Tenemos (sin debugger) (secs.)
m		res/mat/fact		GMRES
5		110			10
20		50			10
40		50			10
%
Con debugger los tiempos bajan a 34+6=40 secs. 

%===Wed Nov 14 23:47:06 ARST 2001 mstorti@spider
%
Estoy tratando de agregar opciones a las matrices Sparse::Mat, pero me
encuentro con el problema de poner el `TextHashTable' en el
Sparse::Mat o en el PFMat. Esta a medio hacer.

%===Tue Nov 20 12:54:06 ART 2001 mstorti@minerva
%
Con el IISD y `iisd_subpart=20' puedo correr con 60,000 nodos la cavidad
cubica. Tarda unos 2'38'' por iteracion. 

%===Sun Nov 25 19:45:32 ARST 2001 mstorti@spider
%
Voy a trabajar en el subparticionamiento dentro de cada subdominio. 

%===Fri Nov 30 10:20:32 ARST 2001 mstorti@spider
%
Subparticionamiento OK. 
Pasando en limpio los `adaptors' y escribiendo un nuevo adaptor donde
solo se calculan contribuciones a nivel del punto de Gauss. 

%===Mon Dec  3 13:57:59 ART 2001 mstorti@minerva
%
Volvemos a reflotar el tema del `cache_grad_div_u'. Activado da (en
una malla de 1600 elementos (sqcav con 40x40)) con debugger 
%
########## CON CACHE_GRAD_DIV_U = 0, 2D
[proc 0]   1.65   1.03125
[proc 0]   1.74   1.0875
[proc 0]   1.73   1.08125
[proc 0]   1.72   1.075
[proc 0]   1.69   1.05625
%
total: 16.54user 0.47system 0:20.04elapsed 84%CPU
%
########## CON CACHE_GRAD_DIV_U = 1, EN 2D
[proc] - total[sec] - rate[sec/Kelement]  
[proc 0]   1.47   0.91875
[proc 0]   1.48   0.925
[proc 0]   1.51   0.94375
[proc 0]   1.45   0.90625
%
total: 15.71user 0.45system 0:27.11elapsed 59%CPU
%
EN 3D -------------------------------
%
Sin debugger. En el cluster. 
%
%%% 
########## CON CACHE_GRAD_DIV_U = 0, 3D
Performance report for elemset "nsi_tet_les_fm2" task "comp_mat_res"
[proc] - total[sec] - rate[sec/Kelement]
[proc 0]   31.09   0.829509
[proc 1]   52.15   1.44813
[proc 2]   56.21   1.59082
[proc 3]   64.54   1.7229
[proc 4]   61.47   1.6542
[proc 5]   62.98   1.68041
[proc 6]   63.31   1.75282
[proc 7]   56.86   2.09229
[proc 8]   60.37   2.18558
[proc 9]   59.11   2.13656
[proc 10]   60.51   2.22602
%
########## CON CACHE_GRAD_DIV_U = 1, 2D
[proc] - total[sec] - rate[sec/Kelement]
[proc 0]   22.81   0.608591
[proc 1]   43.36   1.20404
[proc 2]   47.4   1.34148
[proc 3]   48.49   1.29445
[proc 4]   51.99   1.39909
[proc 5]   48.33   1.28952
[proc 6]   53.3   1.47568
[proc 7]   65.02   2.39255
[proc 8]   64.57   2.33763
[proc 9]   62.01   2.24138
[proc 10]   64.44   2.3706
%
Al activar el `cache_grad_div_u' los tiempos disminuyen para los
procesadores 1-7 en promedio en un 20%. Mientras que para los
procesadores 8-11 aumenta de un 5% a un 15%. 

%===Wed Dec  5 09:39:45 ART 2001 mstorti@minerva
%
Escribimos con Beto una modificacion para usar el programa para
deformar mallas y anduvo. Creo que se toco solo el main `ns.cpp' y la
nueva version la voy a mantener como un `branch' que sale de la
`petscfem-beta-2.52' y empieza como la `mesh-move-branch'. La version
anterior la vuelvo para atras. 

%===Wed Dec  5 13:39:32 ART 2001 mstorti@minerva
%
Con la nueva version da `pure virtual method called' en tiempo de
ejecucion, pero despues no afecta a los tests. 

%===Thu Dec  6 22:23:20 ARST 2001 mstorti@spider
%
No hay caso. No puedo encontrar el error. Aparentemente esta
relacionado con la logica de la maquina de estado, pero por otra parte
no hay problema con la version PETSc (`iisd_petsc') pero si con la
SuperLU. Abandono, porque de todas formas la version SuperLU no la
estamos usando. Ahora voy a tratar de debuggear porque da error cuando
se reusa la factorizacion de `iisd_petsc', por ejemplo cuando usamos
`update_jacobian'. 

%===Fri Dec  7 21:39:41 ARST 2001 mstorti@spider
%
Estoy agregando la maquina de estados, pero de a poco. Primero estoy
descomponiendo la funcion `solve()' anterior, como `factor_and_solve()' y
`solve_only()'. Esto ya esta compilando. 

%===Fri Jan  4 08:20:01 ARST 2002 mstorti@spider
%
La nueva version de `PFMat' (por ahora `IISDMat' y `PETScMat')
funciona aparentemente bien, pero pierde memoria. Aparentemente no
esta relacionado con StoreGrpah (lgraph). Todas estas pruebas hechas
con `pfmat.cpp'. 
%
08:31:23: Multiples soluciones (nsl>>1) no produce perdida de memoria.
%
08:33:24: Multiples definiciones de la matriz (nmat>>1) si produce
perdida de memoria. 
%
09:04:55: Solo se produce perdida de memoria con `IISDMat' no con
`PETScMat'. 
%
09:09:58: Con `iisd_subpart>1' es cuando pierde memoria. 
%
12:22:38: Con `SuperLU' como `LocalSolver' `parecia' al principio que
perdia menos, pero ahora no estoy muy seguro. 
%
21:56:04: Sacando la parte de resolucion no pierde memoria.
%
21:59:15: La perdida de memoria esta en el solve.

%===Sat Jan  5 07:34:25 ARST 2002 mstorti@spider
%
No es en el `A_LL_other->scatter()'.
%
07:46:03: Pierde memoria con `nsl'=1 y 2.
%
07:54:38: La perdida esta en `factor_and_solve'.
%
08:07:29: La perdida es en `local_solve'.
%
08:17:12: La perdida esta en el
`SLESSolve(sles_ll,y_loc_seq,x_loc_seq,&its_);' en `local_solve'. 
%
17:16:33: En realidad no perdia con `SuperLU', sino que leia mal los
datos y en realidad siempre usaba PETSc. Ahora voy a tratar de evitar
las perdidas que reporta `LeakTracer'.

%===Thu Jan 10 20:23:44 ARST 2002 mstorti@spider
%
El solver `IISDMat(PETSc)' da error en PETSc al hacer el
SLESDestroy(sles)
%
20:30:01: Comentando todo 'local_solve_SLU' no da error.

%===Fri Jan 11 19:19:06 ARST 2002 mstorti@spider
%
Cuando el local solver `A_LL_SLU' es `PETSc' (no `SuperLU') de nuevo
anda bien. 
%
21:02:29: Abandono... Parece ser un quilobo relacionado puramente con
SuperLU. 

%===Mon Dec 10 08:51:01 ART 2001 mstorti@minerva
%
`update_jacobians' anda en el cluster para la cavidad (el ejemplo
`test/sqcav/') pero no para la cavidad cubica. Los tiempos obtenidos
en la cavidad cuadrada con 40000 elementos (200x200) es de 0.79 secs
para el assemble de residuo (`update_jacobians=0') y 3.5 secs para
assemble de residuo y matriz (`update_jacobians=1'). O sea una
relacion de 4.4. Sin embargo en este caso la resolucion es mas cara
(7 a 8 secs promedio). 
%
08:56:36: La convergencia se deteriora pero llega a bajar el residuo
desde 1e-3 hasta 3e-8 en unos 30 pasos de tiempo. 
Las dudas son: si la convergencia se termina deteriorando por
problemas de precision o por usar el IISD. 

%===Wed Dec 12 14:52:11 ART 2001 mstorti@minerva
%
Uno de los tests de `lupart' no anda y se trata del IISDMat con solver
PETSc cuando no hay `sles_ll'. 

%===Mon Jan 14 06:16:44 ARST 2002 mstorti@spider
%
Start working in elemsets for surface and sub-surface hidrology. First
starting with adding a transmisivity and storativity ptoportional to
$\phi-\eta$. 

%===Sat Feb  2 12:16:31 ARST 2002 mstorti@spider
%
Volviendo al trabajo despues de las vacaciones. El elemento de
corriente `stream' anda bien, lo probe con una ley de flujo muy
sencilla y verifique que de lo mismo para una corriente lineal (segun
el eje `x') y una ciruclar (media circunferencia de radio `Lx/pi'). 

%===Sun Feb  3 10:12:08 ARST 2002 mstorti@spider
%
Despues de agregarle a `stream' la clase la `channel_shape' y
`friction_law' compila pero no No me linkea:
%
> stream.o: In function `FrictionLaw::FrictionLaw(NewElemset const *)':
> /home/mstorti/PETSC/petscfem/applications/advdif/stream.cpp:187: \
>         undefined reference to `FrictionLaw virtual table'
> collect2: ld returned 1 exit status
%
10:35:51: Probe a poner  `#pragma implementation "Chezy"' pero no
corrige. Probe a poner explicitamente constructores y destructores de
`FrictionLaw' y `Chezy' y tampoco anda. Probe a eliminar `Chezy' y
tampoco anda. 
%
10:40:28: Tambien probe a sacar el `static' de la funcion `factory' y
no anda. 
%
11:15:40: Era facil: faltaba implementar la funcion `flow' o ponerle
`=0'. 

%===Sat Feb  9 23:48:43 ARST 2002 mstorti@spider
%
El `dlopen()' de funciones de extension anda. Resulta que hay que
linkear como si fuera para armar un programa. 
%
11:52:10: ... Es decir, poniendo en la regla para '%.efn' en
`Makefile.base': 
>	${CXX_CLINKER} -g -shared -Wl -o $@ $< $(LDFLAGS)

%===Thu Feb 14 19:42:53 ARST 2002 mstorti@spider
%
Compilado con -O2 tiene unos cuantos problemas. Primero parece haber
problema con la clase `finite state machine' de `pfmat'. Eso se
arregla poniendo que compile con `-O0'. Despues en un test en `lupart'
se queda colgado en un `debug.trace' del main. 
%
19:51:10: Parece que se arregla tambien compilando `debug.cpp' con
-O0. 

%===Fri Feb 15 21:40:26 ARST 2002 mstorti@spider
%
Para resolver el problema creo una rama  `petscfem-optlevel-branch
(branch: 1.1.1.1.24)'. Compilo todo en `src' con -O0 menos `fastmat2'
y `fm2eperl'. 
%
07:30:53: Es bastante desconcertante. Probe a extraer todas las
versiones estables desde la 1.48 a saber las 2.48, 2.60, 2.61, 2.64,
2.76 y 2.82 y correr los tests y hasta la 2.61 da todo bien. Incluso
la 2.61 corre todos los tests OK. Entre la 2.61 y la 2.64 empieza
el drama del error en el PFMat::solve() y justamente es cuando se
agrego todo el tema de la Finite State machine compiler para PFMat. 
%
Probe con compilar todos los archivos que se habian tocado (debug.o,
distmat.o, dofmap.o, elemset.o, fem.o, graph.o, iisdcr.o, pfmatFSM.o,
sparse.o,  spsolve.o,  texthash.o,  util2.o no se porque no esta
pfmat.o) con -O0 y no hay caso. Probe con compilaar con -O0 todos los
archivos en `applications/ns' y tampoco. Probe con linkeditar con -O0
y tampoco, finalmente probe con compilar todos los que faltaban de
`src' y ahi anduvo (sospecho porque recien ahi compile pfmat.o con
-O0). Ahora voy  probar a volver a compilar solo `pfmat.o' con -O0. 
%
08:04:56: Pruebo a compilar ns de 2.64 con -O0 solo para `src/pfmat.o' y
`applications/ns/ns.o' y parece que corre los `oscplate'
%
08:43:49: Voy a ver que pasa con lo `test/aquifer' que no corre,
mientras que los tests `newff' y `burgers' si. 
%
10:53:38: Encontre los errores:
> Basically, there where two problems. `src/pfmat.o' needs to be
> compiled with -O2 and `applications/advdif/diff.o' had a bug related
> to the `glob_param' pointer. Also `test/pfmat/pfmat2.cpp' also needs
> to be compiled with -O0.
Corregidos estos errores corre todos los tests con -O2. 
%
10:59:35: Voy a cerrar la rama `optlevel', voy a agregar las
correcciones a la version `beta-2.84.pl1' y voy a seguir de ahi. 

%===Sat, 30 Mar 2002 09:06:12 -0300 mstorti@spider
%
Corrijo un error en los elementos de Navier-Stokes. El div_u_star
estaba calculado con el `ucols_new', por ejemplo 
%
nsikeps.cpp:717:	  div_u_star = double(tmp10.prod(dshapex,ucols_new,-1,-2,-2,-1));
nsilesther.cpp:526:	  div_u_star = double(tmp10.prod(dshapex,ucols_new,-1,-2,-2,-1));
%
Los pongo a todos en `ucols_star'. 

%===Sat, 13 Apr 2002 17:47:47 -0300 mstorti@spider
%
First fictitious node in `nsi_tet_keps_rot' state is now acceleration
not velocity. 
Fixed a bug in `nsikepsrot.cpp'. `acel_lin' was divided by Dt. Fixed.

%===Wed, 24 Apr 2002 19:36:43 -0300 mstorti@spider
%
Extraniamente el advdif no compila. Comparar con la version que tiene
Rodrigo. 

%===Sat,  4 May 2002 17:25:29 -0300 mstorti@spider
%
En el elemset `nsi_tet_keps_rot' al particionar los elementos
ficticios traen problemas porque hace que todos los elementos esten
conectados con todos. Voy a agregar una funcion que sea `real_nodes'
que retorne el numero de nodos `reales'.

%===Mon May  6 17:17:25 ART 2002 mstorti@node1.beowulf.gtm
%
Corregi un error que al particionar el grafo en `Graph::part()',
(file graph.cpp) cuando habia partes desconectadas del grafo resultaba
que no incrementaba el numero de vertices del grafo no conectada lo
cual llevaba eventualmente a un error. Ahora anda bien, pero
extraniamente en el caso del cilindro lleva a un numero de `ghost
elements' demasiado alto para uno de los procesadores lo cual
desbalancea totalmente. Por otra parte esto es medio extranio porque
normalmente si mal no recuerdo los ghost-elements era solo para cuando
se calculaba el perfil de la matriz. 

%===Mon,  6 May 2002 22:17:32 -0300 mstorti@spider
%
Estudio de performance: Tomando un chunk_size de 1000,
(nsi_tet_keps_rot) en un solo procesador el tiempo de evaluacion del
rediuo y matriz (tomado a nivel del elemento) es de 14.5 secs/ke. La
parte de ensamblaje (los MatAssembly...) aparentemente es muy rapida,
y aparentemente el upload/download tarda unos 3 secs. Juntando todos
los chunks (son 6144 elementos) eso da un total de 105 secs. Despues
el reporte final (para todo el asemble tomado en ns.cpp) es de
115secs. Ahora habria que ver como da todo esto en paralelo. 

%===Tue May  7 08:30:30 ART 2002 mstorti@node1.beowulf.gtm
%
Corriendo en node1, da 9.40secs/Ke (O_c++, nsi_tet_keps_rot), y
despues un promedio global de 10.7secs/Ke. Los assembly time
reportados dan muy bajos. Probablemente lo que se lleva tiempo son los
upload/download. En node12 (Pentium IV da 2.64secs/Ke raw y
3.74secs/Ke promedio), usando chunks de 4000. Total elapsed 26.17
%
Para 6144 elementos. 
PIV  1.4GHz (node12):  raw 3.10secs/Ke, averg  3.8secs/Ke, elapsed: 27.6secs
PIII 866Mhz (node1 ):  raw 9.40secs/Ke, averg 10.7secs/Ke, elapsed: 70.3secs
PIII 500Mhz (node8 ):  no puedo correr porque no entra en la memoria
%
Para 3456 nodos
PIV  1.4GHz (node12):  raw  2.7secs/Ke, averg   3.8secs/Ke, elapsed: 15.2secs
PIII 866Mhz (node1 ):  raw  9.2secs/Ke, averg  10.4secs/Ke, elapsed: 38.4secs
PIII 500Mhz (node8 ):  raw 10.2secs/Ke, averg  12.2secs/Ke, elapsed: 45.9secs
%
10:45:44: Corriendo en paralelo, con pesos 1.3/1/4,
chunk_size=4000. Los chunk_size son 1300,1000,4000. Tenemos una malla
de n=20/nz=10 (12000 eleementos). Los particionamientos en cada
procesador hacen que este bastante bien balanceado porque da tiempos
de entre 8 y 10.2 sec. Las tasas son mas o menos las reportadas
arriba. Los tiempos de upload van desde 1.6-1.8 para todos, menos
10.7secs para el P4. Los assembly time van de 34 a 44 secs. El elapsed
es de 58secs. O sea que del optimo de 40 secs (si corrieramos todo en
el P4) vamos a 58 secs por sincronizacion etc. Vamos a ver si la
perdida por sincronizacion se debe al gran desbalance. Vamos a correr
poniendo un pesos iguales para todos y ver el assembly, upload en ese
caso... 
%
11:04:32: Poniendo todos los pesos a 1 no mejora. Los tiempos de
calculo de residuo van de 3.3 sec para el P4 gasta 12.6 para los
P3-500. Los upload estan en el orden de los 3secs y los assembly estan
todos parejos en el orden de los 58secs. 
%
11:50:36: Tenemos entonces: evaluacion "raw": 8-10 secs, upload 2secs
(10 para el P4), scatter 9secs, assembly 1.5secs, assembly-end 32
secs. Total assembly: 41secs. 
%
13:43:59: En geronimo puedo correr una malla de 26/10. 
%
17:51:57: El problema esta con la matriz A_II. Si no hago el set
values de A_II entonces el tiempo baja de 320secs a 120secs. 

%===Sat, 11 May 2002 10:21:00 -0300 mstorti@spider
%
Pareceria haber demasiados nodos de interfase. Por ejemplo con una
malla 20/1 hay 1200 elementos, particionado en dos procs. da un total
de 11000 grados de libertad de los cuales en proc0 hay approx. 6000
dof, todos locales y en proc1 hay 5000 dofs, 2700 de interfase. Ahora
voy a probar a sacar las condiciones de contorno (para hacer el
analisis mas simple). Desactivo las cond. de contorno con un `#if'. 
%
10:49:35: Sacando las condiciones de contorno tambien particiona muy
mal los nodos. Ahora voy a probar a sacar los elemset `no-fat'. 
%
10:53:17: Con un solo `no-fat' tambien da mal. Ahora voy a probar a
dejar un solo dof por nodo. 

%===Sun, 12 May 2002 12:01:37 -0300 mstorti@spider
%
Creo que encontre el problema. No era un error en el programa sino con
el quilombo de elementos ficticios, `is_fat', nodos ficticios,
etc... Al marcar los elementos de supeficie como `is_fat=0', entonces
los nodos de superficie quedaban desconectados y entonces los asignaba
arbitrariamente a un procesador, por lo tanto quedaba todo un monton
de nodos marcados como interfase. 
%
18:51:14: Con una malla de 20/1 (1200 elementos) en un procesador da
todo bien. Porque los tiempos de calculo de elementos son 28secs para
`nsi_tet_keps_rot' y 28 secs para `bcconv_ns_fm2' y despues reporta un
tiempo de ensamblaje total de 58secs. Despues tarda unos 20secs en
resolver. Ahora voy a probar con una malla con mas `nz' y menos `n'. 
%
19:31:43: Con una malla 12/6 (2592 elementos) da 65'' para evaluar
`nsi_tet_keps_rot' y 15'' para evaluar `bcconv'. No puedo correr con
optimizacion (no se porque). Ahora voy a probar en paralelo. 
%
19:38:19: Corriendo con dos procesadores da 95secs total,
`bcconv_ns_fm2' 17secs y `nsi_tet_keps_rot' 73secs. Lo cual quiere
decir que el ensamblaje y otros elementos se comen solo 5secs, lo cual
esta muy bien. La resolucion se come 100secs. En un solo proc. parece
converger mejor, probablemente porque al mantener el `iisd_subpart' al
pasar a dos procs. se reduce el tamanio de los subdominios y aumenta
el numero de nodos de interfase. 
%
20:15:07: Con optimizacion da total=50, `nsi_tet_keps_rot=45',
`bcconv_ns_fm2=5'. Con lo cual esta muy bien. La resolucion tarda unos
130secs. La verdad es que el perfil de velocidad en el waterline es
bastante ruidoso. 

%===Mon May 13 10:52:11 ART 2002 mstorti@node1.beowulf.gtm
%
Puedo correr `cylinder.epl' con una malla de 30/15 (46939 nodos, 40500
hexas) en el cluster. Tarda 1'40'' evaluacion del residuo y 2'30''
resolucion (300 iter GMRES, baja 4 ord. magn. el residuo). 
%
13:37:43: Guarda que desactivando el bwm da 3'10'' toda la
iteracion. 70 secs. ensamblaje y 2' resolucion. Voy a probar con
tetras. 
%
15:17:41: Con tetras da 75'' el ensamblaje y 2'30'' la
resolucion. Curiosamente da mas que con hexas!!.

%===Fri May 24 14:54:35 ART 2002 mstorti@node1.beowulf.gtm
%
Se corre PETSc-FEM con una malla de 3000 elementos nsi_keps_rot
compilado con -O2-funroll-loops , version beta-2.94.pl18
%
Performance en los Pentium 4:
[proc] - total[sec] - rate[sec/Kelement] rate[Kel/sec]
node1:     30.17      10.05
node3:     12.56       4.18
node9:      9.43       3.14
node12:    10.61       3.53

%===Sun,  2 Jun 2002 20:34:59 -0300 mstorti@spider
%
Atencion: no se porque no esta salvando bien las versiones y las esta
poniendo como 2.94 cuando en realidad son 2.96 o mas. 

%===Sun, 16 Jun 2002 12:14:57 -0300 mstorti@spider
%
Encuentro en un error en `fstack'. Si se hace `close()' y despues
`delete fstack'. 
%
12:22:54: Cuando se hacian dos `close()' seguidos se estaba haciendo
un `fclose()' de un archivo ya cerrado o algo por el estilo. Ahora
cuando se hace el `close()' se hace solo si `file_at_top' no es NULL y
al cerrarlo se pone en NULL. 

%===Sat, 13 Jul 2002 19:20:37 -0300 mstorti@spider
%
Mirando eficiencia corro una malla de 50^3=125,000 elementos hexas con
qharm con `-O2 -funroll-loops' en spider (spider: dyntest 83Mflops
para N=41, 22Mflops asint). Tarda 60 secs en ensamblar (0.5sec/Kelem)
y 160 secs. en resolver el sistema a toleacia rtol=1e-6 con
iisd_subpart=60. Si todo escaleara bien deberia tardar unas 80 veces menos
en un cluster de 10 procesadores P4, o sea 1sec para ensamblar y 2
secs para resolver. Ahora voy a tratar de hacerlo con tetras. (Solo
deberia cambiar significativamente el tiempo de ensamblaje). 
%
21:08:06: Con tetras la malla de 50^3=625K elementos da: 36 secs de ensamblaje
(0.058secs/Ke), 100 secs de resolucion. 
%
22:10:51: Con tetras la malla de 60^3=1.08Melementos da: 62 secs de ensamblaje
(0.058secs/Ke), 276 secs de resolucion. El tiempo de resolucion parece
ser bastante mas que lineal. 
%
22:18:35: Vamos a probar hasta donde se puede llegar. 

%===Sun, 14 Jul 2002 13:16:13 -0300 mstorti@spider
%
Reflotando el fractional-step. Compila. 
%
20:11:04: En un momento (circa Nov 1 2001) yo anule el tema de 
cargar los perfiles segun la matriz devuelta sino que directamente
usaba una mascara (la misma para todos los elementos). 

%===Sat, 20 Jul 2002 23:46:42 -0300 mstorti@spider
%
Estoy tratando de ver porque consume tanta memoria el armado del
grapho y encima despues no lo libera. 

%===Mon Jul 22 08:42:16 ART 2002 mstorti@minerva
%
Escribi una nueva clase grafo basada en una clase de vector
dynamic dvector<>. Ahora parece que consume mucho menos memoria pero
todavia no se si la libera o no. Verifique que los resultados son los
mismos en la cavidad cubica con una malla de 4x4x4. Ahora voy a
verifica si por lo menos entra en el liberador de memoria
(lgraph->clear()). 
%
15:05:26: Con graphdv_dis pasa de la etapa de calculo de perfile con
una RSS de 220MB pero tarda como 2horas y media. Ahora voy a probar
con `link_graph'. 
%
16:33:00: Corriendo con 30^3 con link_graph va a los pedos, pero
iniciamente aloca unos 90 megas que despues no se si los libera y
asintoticamente ocupa unos 270 Mb, cuando para mi deberia ocupar unos
60 MB las matrices sin factorizar. 
%
17:28:39: No estaba limpiando los grafos en
`link_graph::clear()'. Ahora la malla de 30^3 consume unos 158MB. 

%# Current line ===========  
# Local Variables: $
# eval: (local-set-key "\C-cD" (quote notas-insert-date)) $
# eval: (local-set-key "\C-ct" (quote notas-insert-time)) $
# eval: (setq paragraph-separate "[ \t\f%]*$") $
# eval: (setq paragraph-start "[ \t\n\f%]") $
# End: $
