$Id: notes.txt,v 1.350 2003/10/17 22:52:36 mstorti Exp $
OK. This is mostly in spanish...  Sorry :-)
================================================================

%===Mon Aug  2 23:29:29 ART 1999
PetscFEM es una aplicacion PETSC para hacer un programa FEM. 
Salvo. ver: 1.1

%===Wed Aug  4 23:06:31 ART 1999
habia unos cuantos errores en la lectura de datos y se colgaba al
llamar a MatCreateSeqAIJ(). Ahora no se cuelga. 
Salvo. ver: 1.3

%===Sat Sep  4 12:18:45 ART 1999
No andaba porque definia *elemset pero despues no le asignaba un valor
con new. Siempre que se define un puntero despues hay que asignarle un
valor!!
Salvo. ver: 1.4

%===Sat Sep  4 18:16:26 ART 1999
Anda calculo de la matriz y del residuo. 
Salvo. ver: 1.5

%===Sat Sep 11 19:36:04 ART 1999
Pongo toda la informacion de los puntos de Gauss en una estructura
GPdata. Tuve problemas cuando hacia el delete en el destructor de
GPdata, pero parece que ahora anda aunque no se bien porque. Lo probe
para el triangulo, ahora hay que probar el cuadrilatero. 
Salvo. ver: 1.6

%===Sat Sep 11 22:50:33 ART 1999
No es verdad que estaba andando bien el cuadrangulo. Estaba tomando el
triangulo y en el cuadrangulo todavia hay errores. Estoy debuggeando. 

%===Sat Sep 11 22:56:08 ART 1999
Ahora parece que esta andando para cuadrangulos. 
Salvo version: 1.7

%===Sun Sep 12 08:31:22 ART 1999
Parece que anda bien un ejemplito de adveccion difusion.
version: 1.8

%===Sun Sep 12 10:14:56 ART 1999
Le puse que el determinante e inversa del jacobiano se calcule con las
funciones de newmat. 
version: 1.9

%===Sun Sep 12 12:52:04 ART 1999
Anda bien adveccion-difusion con cuadrangulos. Ahora voy a probar con
triangulos. 
version: 1.10

%===Sun Sep 12 13:51:32 ART 1999
Anda bien adveccion-difusion con triangulos. 
version: 1.11

%===Sun Sep 12 22:46:02 ART 1999
Hice andar las GNU-hash-tables (try4.cpp). 
version: 1.12

%===Fri Sep 17 12:16:52 ART 1999
Voy a hacer una clase TextHashTable.
version: 1.13

%===Fri Sep 17 13:47:38 ART 1999
parece que anda la clase TextHashTable.
version: 1.14

%===Fri Sep 17 19:41:51 ART 1999
Hecha la tabla texthash (.cpp y .h). 
Ahora hay que meterla en fem.cpp
version: 1.15

%===Sat Sep 18 20:05:53 ART 1999
Leo nodos en forma directa (sin leer dos veces el archivo) pero con
alocacion dinamica, alocando en una serie de 'chunks" y despues se
copia todo en la memoria. 
version: 1.16

%===Sat Sep 18 20:39:01 ART 1999
Corregidos ciertos errores en la (buffereada) lectura de nodos. 
Ahora voy a leer elementos. 
version: 1.17

%===Thu Sep 23 13:09:35 ART 1999
Lee elememsets y los pone en una lista. Ahora tengo que hacer que lea
fijaciones y etc. 
Salvo. ver: 1.18

%===Sat Sep 25 10:07:34 ART 1999
Anda libretto!!! grande!!! Permite construir listas, etc... Hice una
lista de enteros y de punteros a dobles. try9.cpp
version: 1.19

%===Sat Sep 25 11:01:20 ART 1999
Estoy poniendo eleemsetlist en forma de un Darray. 
version: 1.20

%===Sat Sep 25 11:21:26 ART 1999
Voy a cambiar el nombre xnod por nodedata. Se supone que contiene
informacion de los nodos que no cambia. 

%===Sat Sep 25 22:20:33 ART 1999
Anda bien guardar los punteros a elemesets en un arregloe dinamico
Darray de "libretto". 
version: 1.21

%===Sat Sep 25 23:19:27 ART 1999
Tengo que ver como paso el valor de nu, el numero de columnas en
nodedata. Probablemente tendria que definir una clase nodedata y
entonces nu seria un miembro de la misma. 
version: 1.22

%===Sun Sep 26 20:46:42 ART 1999
Anda bien la llamada por elemsets. Ahora hay que definir que le asigne
el diferente tipo de elemset segun el string que esta en el archivo de
datos. 
version: 1.23

%===Sun Sep 26 21:58:02 ART 1999
falta debuggear print_vector
version: 1.24

%===Mon Sep 27 19:05:40 ART 1999
Anda bien como triangulo y quad. 
Salvo. ver: 1.25

%===Mon Sep 27 19:14:19 ART 1999
Voy a separar en mas archivos .cpp, .h.

%===Mon Sep 27 20:06:58 ART 1999
Estan splitteados los fuentes. Compila y corre bien.
Salvo. ver: 1.26

%===Mon Sep 27 21:05:08 ART 1999
Llama a diferentes elemsets dependiendo de un macro. 
CHECK_ELEMSET_TYPE.
Salvo. ver: 1.27

%===Tue Sep 28 08:43:12 ART 1999
Voy a probar con el jacobiano numerico. Primero componente a
componente. 
Salvo. ver: 1.28

%===Tue Sep 28 09:29:06 ART 1999
Hago un wrapper sobre internal, para poder llamar al jacobiano
numerico. 
Salvo. ver: 1.29

%===Wed Sep 29 19:10:30 ART 1999
El programa anda medio chancho. Verificamos que newmat es bastante
chancho para pequnhas dimensiones. 
Tiempos (en secs.) para 1e6 evaluaciones de un producto de dos matrices de NxN. 
-
N            Newmat        A pata (en C++)
----        --------     -------------------
3             84             12.4
6            129             84.3
12           462            607.
-
-
Se observa que para N=3 newmat es muy chancho (una relaicon 7 a 1),
para N=6 ya andan ahi nomas y para N=12 ya la relacion se
invierte. Conclusion: conviene seguir poe el momento ocn newmat. 
Salvo. ver: 1.30

%===Wed Sep 29 19:17:52 ART 1999
Proximo objetivo: pasar "assemble" a que calcule los residuos
desacoplados por elemento. Eso facilitaria el calculo del jacobiano
numerico y tambien la programacion de cada "elemset". 

%===Thu Sep 30 09:45:06 ART 1999
Calcula residuos desacoplados por elemento. Ahora voy a hacer que
ensamble matrices calculadas (no numericamente), despues viene el
jacobiano numerico. 
version: 1.31

%===Thu Sep 30 12:09:23 ART 1999
Calcula bien residuos y matrices con la forma "por elemento".
Ahora: a calcular el jacobiano numerico. 
version: 1.32

%===Thu Sep 30 19:32:01 ART 1999
Anda bien el calculo numerico del jacobiano!! Ahora, para
verificar voy a probar de empezar desde un vector random. 
version: 1.33

%===Thu Sep 30 19:42:00 ART 1999
Anda bien con el vector inicializado random!! 
version: 1.34

%===Thu Sep 30 19:44:08 ART 1999
Cambio a nuevo nivel de version 2.0
version: 2.1

%===Thu Sep 30 19:54:41 ART 1999
Vamos a hacer una prueba con un problema con
dos grados de libertad. 

%===Thu Sep 30 20:14:40 ART 1999
Hago una estrategia Newton Raphson. Antes estaba en una forma muy
lineal. Puede haber algunos quilombos con los signos. 

%===Thu Sep 30 20:22:33 ART 1999
Anda bien el Newton Raphson en lineal. 
version: 2.2

%===Thu Sep 30 20:51:33 ART 1999
calcula determinante del jacobiano con una rutina mydet()
version: 2.3

%===Thu Sep 30 20:53:06 ART 1999
Voy a tratar de resolver varios campos. Primero lo mas simple, dos
campos acoplados, con las mismas condiciones de contorno,
conductividad, etc... 

%===Thu Sep 30 20:56:00 ART 1999
Hay que chequear tambien como pone las cargas!! Hasta ahora no lo chequee!!

%===Thu Sep 30 22:20:00 ART 1999
Andan bien dos campos desacoplados. 
version: 2.4

%===Fri Oct  1 19:22:29 ART 1999
Anda el 3D. Objetivo siguiente: fuente no lineal
Salvo. ver: 2.5

%===Sat Oct  2 08:44:19 ART 1999
retomo en casa.

%===Sat Oct  2 09:15:45 ART 1999
Pequenhos cambios. 
version: 2.6

%===Sat Oct  2 17:49:59 ART 1999
agrego la propiedad por elementos. La lee bien y llega bien al
elemset. 

%===Sat Oct  2 19:28:36 ART 1999
Voy a agregar que lea una tabla de strings con las propiedades de los
elementos entonces despues puede pedir una propiedad
(por. ej. "conductividad")  y esta puede ser constante para todo el
elemset o por elemento. 

%===Wed Oct 13 12:53:06 ART 1999
Performance de newmat versus fastmat
Newmat: 6.9e-3 sec/hexa (Laplace,2 ptos de Gauss)
FastMat: 8.8e-4 sec/hexa (Laplace,2 ptos de Gauss)

%===Sun Oct 17 18:24:42 ART 1999
Unified PROJECTS file with notas.txt.

%===Sun Oct 17 18:25:22 ART 1999
Voy a pasar r y A como variable arguments. 

%===Sun Oct 17 18:45:11 ART 1999
No, es demasiado enquilombado, voy a pasar o un puntero generico, o
tantos argumentos como hagan falta!!

%===Sat Nov 20 19:54:02 ART 1999
Cambio de llamar a ident a llamar dofmap.get_row(), devuelve una fila
del mapeo. 

%===Mon Nov 22 10:27:01 ART 1999
Anda bien el cambio ident -> dofmap con un solo procesador para el
problemita del laplaciano. Lo voy a probar en dos procesadores.

%===Mon Nov 22 10:52:18 ART 1999
Anda bien en dos procesadores. Ahora a poner las condiciones
periodicas. 

%===Mon Nov 22 21:59:58 ART 1999
Anda bien las fijaciones. 

%===Mon Nov 22 23:25:00 ART 1999
Estoy definiendo Q. Falta resinchronizar Q, ordenandolo. 

%===Thu Nov 25 05:45:53 ART 1999
Habia un error 

%===Fri Nov 26 11:34:53 ART 1999
Cambio download_vector a un miembro de la clase Elemset. Defino una
funcion compute_this_elem que retorna un flag si hay que calcular este
elemento o no. 

%===Thu Dec  2 19:26:58 ART 1999
Retomo despues de Fluidos99. Hay que debuggear porque hace mallocs()
despues en la etapa de poisson y en las de momento. Ver que pasa con
los chunks!!

%===Thu Dec  2 19:39:42 ART 1999
Con el lapli.dat (2x2) anda al pelo. 0 unneeded y 0 mallocs. 
Tambien anda bien con chunk_size=1

%===Thu Dec  2 20:20:26 ART 1999
Cavidad chiquita de 3x3 elementos. Poisson y proyeccion andan
bien. Momento da muchos unneeded  0 mallocs en la primera llamada y
unneded y mallocs>0 en la segunda. A partir de la tercera da todo bien.

%===Fri Dec  3 11:35:29 ART 1999
Dimensiono correctamente matlocmom2. Ahora con un procesador y cuando
parto de una corrida anterior hace todas las alocaciones bien, no
mallocs y 0 unneedeed. Cuando parto de 0 ahi da unneeded en iter 1 y
mallocs y unneded en iter 2. Voy a probar a comentar el zeroe-mat.

%===Fri Dec  3 11:41:34 ART 1999
Tambien hace mallocs y unneeded cuando saco el zeroe_mat.

%===Fri Dec  3 12:08:20 ART 1999
Voy a verificar si con la cavidad cubica 5x5x5 tambien anda todo bien
al continuar una corrida anterior. 

%===Fri Dec  3 12:16:52 ART 1999
Con la cavidad cubica ocurre que en Poisson tambien hay unneeded y
mallocs. 

%===Fri Dec  3 12:30:47 ART 1999
Largo la cavidad cubica 41x41x41 (media malla) relanzando de la
corrida que hizo 72 iteraciones. 

%===Fri Dec  3 12:33:02 ART 1999
Mientras tanto voy a probar que pasa con la cavidad cubica 5x5x5 con
uns solo procesador para ver si tambien hace mallocs en el poisson. 

%===Fri Dec  3 12:36:14 ART 1999
Si! Tambien anda mal el poisson con un solo procesador. 

%===Fri Dec  3 15:28:21 ART 1999
Bueno, la matriz del laplaciano en una malla cubica da cero entre un
nodo y el nodo que da con el centro de las caras. O sea que la
estructura del stencil es

     1   2    1 
  2    0    2
1    2   1 

     2   0    2 
  0  -32    0
2    0   2 

     1   2    1 
  2    0    2
1    2   1 

Con lo cual hay ceros (eso no ocurre con el cudrilatero 2D) y puede
traer dramas al momento de alocar. 

%===Fri Dec  3 16:15:11 ART 1999
Hice un jobinfo = comp_mat_poi_prof para calcular el perfil del
poisson. Ahora da 0 mallocs cuando se reinicializa. 

%===Sat Dec  4 19:42:21 ART 1999
Separe las aplicaciones NS y LAPLA de las librerias. la libreria la
llame libpetscfem.a

%===Sun Dec  5 10:28:02 ART 1999
Estan andando tetras con npg=1 y npg=4. 

%===Wed Dec  8 18:26:43 ART 1999
El elmento de carga generico esta funcionando. A veces la resolucion
del sistema lineal pareceria que le cuesta converger. Pero la matriz
parece estar bien armada. 

%===Wed Dec  8 23:22:55 ART 1999
El flujo es el que llega a la capa u1, es decir que debe ser de la
forma hfilm*(u2-u1) entonces en el caso de ser u2=0, debe ser
-hfilm*u1. 

%===Wed Dec  8 23:30:12 ART 1999
No anda bien el newton cuando el intercambio convectivo es no-lineal. 

%===Fri Dec 10 12:15:52 ART 1999
Ahora anda bien el intercambio convectivo no-lineal. No hacia zero la
matriz del jacobiano antes de empezar a recalcularla en el lazo. 

%===Fri Dec 10 13:44:32 ART 1999
Andan las opciones generales.

%===Sat Dec 11 14:51:30 ART 1999
Reescribi la lectura de datos. Ahora no hace mas lo de chunkptr's para
leer nodos, etc.. y por lo tanto puede leer nodos y elementos de
tamanho ilimitado. 

%===Sun Dec 12 10:34:42 ART 1999
Junto Nodedata y elemsetlist en un solo objeto `mesh'. 

%===Sun Dec 12 20:08:04 ART 1999
Ahora chunk_size se puede poner como un dato del elemento. 

%===Mon Dec 13 10:08:25 ART 1999
Anda file_stack. Es un objeto al cual le vas pidiendo lineas con
get_line y te las va dando. Si hay includes te va pasando al arhivo
siguiente y asi... 

%===Wed Dec 15 17:21:51 ART 1999
Retomo en casa. Anda bien la lectura de archivos, y `lapla' y `ns'
estan reescritos con la estructura `Mesh'. Estoy debugueando porque no
anda en 3D. Da un pico de presion y consecuente singularidad en el
campo de velocidad cerca de donde se impone la presion a cero. Estoy
verificando que el residuo sea igual ante un cambio de constante en el
campo de presion. Inicializo de un vector de estado arbitrario e
imponiendo la presion en dos nodos diferentes (el 1 y el 2) despues de
dos iteraciones da un estado diferente. La diferencia tiene mucho
ruido cerca de donde se impone la presion. 

%===Wed Dec 15 17:38:28 ART 1999
Pense que podria estar relacionado con la solucion iterativa de los
subpasos. Pero aumentando la precision (bajando a tolerancia a 1e-8)
da igual. 

%===Wed Dec 15 17:39:57 ART 1999
Vamos a ver si en 2D pasa lo mismo. 

%===Wed Dec 15 18:24:14 ART 1999
En 2D parece que SI da lo mismo. Volvamos a verificar el 3D. 

%===Wed Dec 15 19:40:05 ART 1999
Efectivamente parece que al pasar a 3D no es invariante al agregar una
constante en la presion. 

%===Wed Dec 15 20:19:41 ART 1999
La matriz de poisson parece que esta bien. 

%===Wed Dec 15 20:25:05 ART 1999
Despues de un paso tambien la solucion es diferente. 

%===Wed Dec 15 20:43:09 ART 1999
La diferencia se produce en el paso de Poisson.

%===Thu Dec 16 10:22:53 ART 1999 
Encontrado el error! La suma de los residuos no daba nula porque el
termino div.u en el miembro derecho del problema de Poisson no estaba
escrito en forma conservativa. Debilitando es conservativo y se
arregla. Pero queda un termino de contorno que es no nulo cuando hay
una entrada de masa por el contorno y que en el caso general no se de
de donde lo vamos a calcular. Tal vez si podria ser tratado como
BCCONV.

%===Thu Dec 16 10:47:05 ART 1999
Voy a hacer el jacobiano analitico del Poisson y de la proyeccion. 

%===Thu Dec 16 12:13:58 ART 1999
Anda el jacobiano analitico para Poisson y de la proyeccion. 
Voy a hacer una clase "sparse" para despues usar con los ghost nodes y
poder eliminar los vectores `seq'. Despues haria lo de pasar
como argumentos de entrada y salida una lista arbitraria de vectores y
matrices. 

%===Sun Dec 19 15:26:31 ART 1999
Corriendo en casa. Problemas al hacer el upgrade a RedHat 6.1. Emacs
no andaba, se colgaba. Mpich sigue sin compilar el serv_p4.c. Libretto
sigue teniendo problemas al compilar. El debugger se cuelga al hacer
`p da_length(algo)' o parece colgarse al ejecutar cualquier funcion. 
#
Problemas especificos del programa: Habia un lio al leer las
conectividades propiedades. Fundamentalmente venia cuando nelprops=0 y
era un probelma con libretto. Ahora leo conectividades y props en el
mismo darray. Por otra parte leerlo en el mismo darray probablemente
era bastante ineficiente. Para hacerlo hay que ir copiando a un buffer
especial como si fueran "unsigned char". Ahora tengo que probarlo con
un numero par de enteros, a ver si no se corrompe el double. 

%===Sun Dec 19 15:36:02 ART 1999
Anda bien tambien con un numero impar de enteros en icone. 

%===Sun Dec 19 17:12:30 ART 1999
Voy debuggear el shallow water. 

%===Mon Dec 20 09:47:59 ART 1999
Empieza a andar el shallow water. Voy a ponerle el termino fuente. 

%===Mon Jan  3 13:09:04 ART 2000
Retomo despues de un tiempo de no anotar en la bitacora. Estoy
escribiendo una clase para reemplazar al dofmap. Primero estoy
escribiendo una clase de matrices "block_matrix" tipo sparse. Ya
esvribi las operaciones basicas, hasta `axpy'. Ahora tengo que
escribir un `split'. 

%===Mon Jan  3 19:31:01 ART 2000
Hay un error en la concepcion de block_matrix(), n esta intercambiado
con m. 

%===Tue Jan  4 12:16:10 ART 2000
Anda la funcion `split' que divide una block_matrix en
sub-bloques. Voy a chequear que no pierda memoria. 

%===Wed Jan  5 20:18:02 ART 2000
Hice que contemple el caso de matrices `nulas'. 

%===Fri Jan  7 13:07:37 ART 2000
Estoy escribiendo la clase `idmap' que permite tratar matrices que son
`casi' una permutacion. Ahora voy a escribir la inversion. 
Salvo ver: 1.4

%===Sat Jan  8 09:32:05 ART 2000
Anda la resolucion de sistemas con "solve". 
Salvo  version: 1.4

%===Sat Jan  8 22:18:10 ART 2000
Esta verificada la resolucion son solve. 
Salvo version: 1.5

%===Sun Jan  9 12:07:19 ART 2000
Los templates tienen que ir en los header si no no se pueden hacer las
instanciaciones apropriadas. 

%===Sun Jan  9 13:25:44 ART 2000
Habia un error con icur en la generacion de la matriz. Ahora parece
andar bien. 
Salvo version: 1.6

%===Fri Jan 14 16:20:43 ART 2000
Escribi bastante de documentacion. Ahora voy a poner las condiciones
periodicas con `idmap'. 
Salvo spider version: 1.9

%===Fri Jan 14 18:00:22 ART 2000
Voy a empezar a implementar el manejo de grados de libertad con
`idmap'. Que hacer con las fijaciones? En principio la idea es
asignarle a los valores fijos una posicion en el 

%===Thu Jan 20 20:57:59 ART 2000
Anda bien en read_mesh la puesta de fijaciones y numeracion de grados
de libertad on `idmap' y `dofmap'. 
Salvo spider version: 1.11

%===Fri Jan 21 10:08:58 ART 2000 
Al hacer que `dofmap' es creado con `new' en read_mesh, entonces hay
que pasarlo por referencia `Dofmap *&dofmap' en los argumentos de
read_mesh, sino no queda definida. Antes dofmap era definido en el
main() y era pasada a readmesh.
%
Ojo con los vectores de STL (fixed, fixed_remapped) que no se puede
referenciar a un elemento si previamente no fue definido. Al querer
manipularlo con vec[k] no canta nada y puede dar un error de memoria. 

%===Fri Jan 21 10:53:29 ART 2000
Esta andando!! Corre un pequenho caso sin condiciones periodicas en un
solo procesador. 
Salvo spider version: 1.12

%===Fri Jan 21 18:09:30 ART 2000 
Habia dos errores. get_nodal_value() estaba acumulando sin inicializar
a cero. Tambien habia un error en el direccionamiento en
get_dofval(). Ahora da bien un problema con condiciones Dirichlet. 
Salvo spider version: 1.13

%===Fri Jan 21 18:11:21 ART 2000
Voy a probar con 2 procesadores.

%===Sat Jan 22 18:19:35 ART 2000
No andaba con 2 procesadores. Era porque ahora get_row devuelve una
lista con free dof's y fijaciones. En la parte ne que define dof_here
hay que evitar las fijaciones. Para dejarlo bien ahora voy a hacer un
get_row_free que en la lista no ponga las fijaciones. 

%===Sat Jan 22 20:23:50 ART 2000
Anda bien en dos procesadores. 

%===Sun Jan 23 10:17:32 ART 2000
Tambien en 4. 
Salvo 1.14 2000/01/23 (spider)

%===Sun Jan 23 20:27:20 ART 2000
Andan bien las condiciones periodicas, ahora como "constraints". Voy a
probar en dos procesadores. 
Salvo 1.15 2000/01/23 (spider)

%===Sun Jan 23 20:32:24 ART 2000
Anda bien con dos procesadores.

%===Mon Jan 24 20:55:24 ART 2000
Anda bien un problema con condiciones periodicas. Ahora voy a probar
con un sector de corona circular. 
Salvo 1.16 2000/01/24 (spider)

%===Tue Jan 25 10:35:18 ART 2000
Anda bien el ejemplo de uns sector circular $2.7 < r < 4.5$ y $0 <
\theta < \pi/4$. Se resuelve $\Delta !u=0$ donde $!u$ es un vector
(velocidad?), con condiciones $!u=0$ en $r=r_\in$ y $!u=U\etheta$ en
$r=r_\ext$ y condiciones periodicas $\at{u_{r,\theta}}{r_\in} =
\at{u_{r,\theta}}{r_\ou}$. 

%===Tue Jan 25 10:45:30 ART 2000
Anda bien tambien calculando la matriz por diferencias finitas. Ahora
voy a hacer lo de pasar "reacciones" por los "retval"'s. 

%===Wed Jan 26 09:34:11 ART 2000
Anda bien poner las restricciones mediante una clase Constraint. 
Salvo version 1.17 2000/01/26 (spider)

%===Fri Jan 28 17:24:53 ART 2000
Estoy transformando el pasaje de ghost-values. Antes se hacia haciendo
un scatter a un vector con todos los valores (locales y ghost). Ahora
lo hcemos haciendo scatter solo de los valores ghost. Tambien, los
argumentos se van a pasar a elemset por una lista de argumentos (clase
`arg_list'). 
%
Originalmente pense en construir un vector con VecCreateGhost(), pero
resulta que (parece ser un bug de PETSc) no se puede hacer un
duplicate de vectores cuando no hay ghost values (por ejemplo cuando
se corre en un solo procesador). 
%
En este momento esta andando el pasaje de argumentos por arg_list y
hacer el scatter solo para los ghost values. 
Salvo version 1.18 2000/01/28 (spider)

%===Sat Jan 29 20:23:18 ART 2000
Esta empezando a andar la adaptacion de ghost_dofs. 
Salvo version 1.19 2000/01/29 (spider)

%===Sun Jan 30 15:23:55 ART 2000
Anda bien el pasaje de argmentos IN_VECTOR y OUT_VECTOR. Calcula bien
el residuo ("comp_res" en Laplace). Verifique que no pierde memoria,
corriendolo muchas veces y monitoreando con `top'. 
Salvo version 1.20 2000/01/30 (spider)

%===Sun Jan 30 15:26:28 ART 2000
Ahora voy a definir el OUT_MATRIX. 

%===Sun Jan 30 20:50:18 ART 2000
Primero tuve que definir PROFILE. Parece que anda. 
Salvo version 1.21 2000/01/30 (spider)

%===Sun Jan 30 20:59:10 ART 2000
Despes hay que verificar que pasa con varios procesadores y en un lazo
grande si pierde memoria. 

%===Mon Jan 31 09:54:23 ART 2000
Anda el laplaciano con calculo de matriz y residuo en la misma llamada
a assemble. Tambien anda de nuevo `print_vector()'. 
Salvo version 1.22 2000/01/31 (spider)

%===Mon Jan 31 10:33:11 ART 2000
Anda en dos procesadores!! Tuve que hacer un scatter a un vector
secuencial, pero que tiene "neq" valores en el procesador `0' y 0
valores en los otros. 
Salvo version 1.23 2000/01/31 (spider)

%===Mon Jan 31 20:44:08 ART 2000
Anda bien la inicializacion desde un vector "ini.dat". Se lee todo el
vector, se resuelve con la matriz Q y se setean los valores en el
vector global con VecSetValue(). 
Salvo version 1.24 2000/01/31 (spider)

%===Mon Jan 31 20:48:00 ART 2000
Eventualmente habria que hacer que verifique si al resolver Q*x = y
para x, despues cual es la norma de ||Q x-y||. 
Ahora voy a hacer la opcion de vector IN/OUT. 

%===Fri Feb  4 09:48:51 ART 2000
Estoy tratando de hacer el shallow water con condiciones de contorno
periodicas. Converti a shallow.cpp en una rutina generica que llama a
`flux_fun.cpp', y ahora estoy debugeando con adveccion lineal.
Salvo version 1.25 2000/02/04 (spider)

%===Sat Feb  5 12:30:20 ART 2000
Voy a escribir la rutina de condiciones absorbentes para shallow. 

%===Sat Feb  5 13:18:40 ART 2000
Cambie `shallow' a advective y `sw' a `adv'. 
Salvo version 1.26 2000/02/05 (spider)

%===Tue Feb  8 12:51:41 ART 2000
Andan las condiciones absorbentes. Por lo menos en 1D. Ahora voya a
probar con un fondo variable. 
Salvo version 1.27 2000/02/08 (spider)

%===Thu Feb 10 09:53:20 ART 2000
Anda bien el shallow water unidimensional. Captura bien la solucion
simetrica subcritica alrededor de una loma. Ahora voy a probar con la
solucion sub->super->sub-critica. 

%===Thu Feb 10 11:21:49 ART 2000
Con <newmat.h> tambien anda en spider. 

%===Fri Feb 11 11:04:46 ART 2000
Tengo problemas con el shallow water. No consigo capturar una solucion
sub-super-subsonica. 

%===Tue Feb 15 15:13:10 ART 2000
Dejo el shallow water. Hice un cambio en los Makefile, ahora puede
correr en trantor para Ruperto, etc... 
%
Ahora voy a tratar de llevar el Navier-Stokes al formato nuevo. 

%===Fri Mar 10 09:34:45 ART 2000
Lo de llevar Navier-Stokes al formato nuevo anduvo bien, pero el tema
de la memoria esta peor que antes. Despues me fui de vacaciones y
ahora estoy debuggeando shallow-water. 
%
Ahora lo hice andar con condiciones de contorno periodicas en las dos
direcciones, version debilitada. Quiero ver el tema de la
estabilidad. 

%===Mon Mar 13 13:35:27 ART 2000
Para el problema de un escalon con condiciones periodicas andan bien
tanto la version debilitada como la no debilitada. 

%===Mon Mar 13 13:37:46 ART 2000
Dan bastante iguales pero no exactamente iguales (lo cual esta bien). 

%===Mon Mar 13 13:55:28 ART 2000
Con condiciones absorbentes tambien da bien. 

%===Tue Mar 14 15:01:22 ART 2000
Descubrimos que en el KSPMonitor habia que destruir el vector de
residuos. 
%
Estoy escribiendo una rutina print_vector_rota() que va guardando
vectores en archivos rotandolos. 

%===Fri Mar 24 13:39:01 ART 2000
En NS, vimos que el precondicionador Jacobi debe ser aplicado a la
derecha y no a la izquierda.
%
Escribimos un flux_fun para Euler y estoy debuggeando. La condicion
absorbente anda bien, pero ahora 

%===Mon Apr 10 09:41:21 ART 2000
Estoy convirtiendo nsi_tet a fastmat. 
%
El tema de la memoria, en la cual la nueva version "aparentemente"
consumia mucho mas memoria, no parecia dar problemas en spider, con
una malla de 60x60. Esto lo dejo para despues. 

%===Mon Apr 17 09:58:34 ART 2000
La conversion ya anda. Mientras tanto tuve que escribir bastantes
nuevasrutinas de fastmat y corregir otras. Ahora voy a comparar
directamente con la version existente en el cluster. 

%===Mon Apr 17 19:03:26 ART 2000
Anda la version fastmat!! Converge identicamente con la version
".tempo" que corria en el cluster cuando desactivo el
precondicionador. 

%===Mon Apr 17 19:18:36 ART 2000
Parece que anda bien con precondicionador y tambien version fastamt
pura. Antes estaba probando con newmat emulando a fastmat. Voy a tomar
tiempos. 

%===Mon Apr 17 20:21:09 ART 2000
Tome tiempos. La version fastmat es 2.8 veces mas rapida que la
newmat. Esta relacion dio igual para quads 2D (elemento nsi_tet) para
chunk_size 20 que para 100 y los tiempos dierons practicamente igual,
lo cual indica que el "over_head" por pocesar por chunks es bastante
bajo. 

%===Mon Apr 17 20:31:53 ART 2000
Ya que estamos tocando el tema tiempos, con chunk_size=1 da 5.34
segundos contra 4.66 para chunk_size=1=100, lo cual dice que el
overhead es pequenho con respecto a elementos 'costosos' como
Navier-Stokes. 
%
Tambien la relacion 2.8 de newmat/fastmat podria mejorar en elementos
menos costosos. 

%===Mon Apr 17 21:55:16 ART 2000
Estoy tratando de debuggear el tema de la memoria. La nueva version
pierde memoria por paso de tiempo. 
%
Nueva version:
%
Total memory (sbrk) [arena]: 208056 b
Total memory (sbrk) [arena]: 961720 b
Total memory (sbrk) [arena]: 1178808 b
Total memory (sbrk) [arena]: 1932472 b
Total memory (sbrk) [arena]: 2677944 b
Total memory (sbrk) [arena]: 3423416 b
Total memory (sbrk) [arena]: 4172984 b
%
%==============================
Vieja version;
%
Total memory (sbrk) [arena]: 211732 b
Total memory (sbrk) [arena]: 944916 b
Total memory (sbrk) [arena]: 1698580 b
Total memory (sbrk) [arena]: 1698580 b
Total memory (sbrk) [arena]: 1698580 b
Total memory (sbrk) [arena]: 1698580 b
Total memory (sbrk) [arena]: 1698580 b
%
(Eso es medio curioso, porque no
pierde en Euler? Sera porque no se ensamblan matrices?) Una cosa que
puedo probar facilmente es cambiar el chunk_size. Ahora esta en 100.

%===Mon Apr 17 21:59:30 ART 2000
No, definitivamente no es el chunk_size... Estaba en 1. 
%
Bueno... Voy a probarlo en 100, a ver si cambia...

%===Mon Apr 17 22:06:32 ART 2000
El chunk_size no es. Pero si al bajar la dimension del espacio de
Krylov, baja la memoria que se pierde por iteracion!!

%===Mon Apr 17 22:12:01 ART 2000
Ahora no pierde memoria. 
%
Version nueva:
%
Total memory (sbrk) [arena]: 962360 b
Total memory (sbrk) [arena]: 1232696 b
Total memory (sbrk) [arena]: 1232696 b
Total memory (sbrk) [arena]: 1232696 b
Total memory (sbrk) [arena]: 1232696 b
Total memory (sbrk) [arena]: 1232696 b
%
La version vieja se comporta en forma similar pero con una alocacion
total de 1338292 b. La diferencia se puede deber a los chunks, que
ahora se aloca mejor. (???)

%===Thu Apr 20 09:33:23 ART 2000
Ahora coinciden bien la version nueva (con newmat) con la vieja,
incluyendo el termino de shock-capturing para la
incompresibilidad. (Termino nuevo que agrego Beto). 

%===Thu Apr 20 12:34:34 ART 2000
Ahora la cavidad chica anda bien hasta la ultima cifra decimal. 

%===Fri Apr 21 20:46:24 ART 2000
Voy a mofidifcar assemble para que calcule jacobianos por diferencias
finitas. Por ahora los hago en una rutina aparte
assemble_prof. Despues voy a unificar las dos funciones. 
%
Modifique el Makefile para que tenga un target depend: que corre el
makedepend. 

%===Sun Apr 23 11:27:27 ART 2000
Reestructure los makefiles. Ahora hay un Makefile.base en el
directorio raiz, y en este o bien se definen las variables o se
pueden definir en un  Makefile.defs en el directorio padre del raiz. 
%
Ahora voy a probar si anda el jacobiano numerico con Navier Stokes. 
%
Tambien hay que probar que ande con varios procesadores. 

%===Sun Apr 23 15:44:38 ART 2000
Parece andar bien el Jacobiano con Navier-Stokes. No da exactamente
igual, pero me imagino que se debe a que en realidad el jacobiano
analitico no es completamente exacto. Voy a probar que el
assemble_prof() sin calcular jacobianos numericos de igual que el
assemble(). 
Salvo version 1.2 2000/04/23 (spider)

%===Sun Apr 23 17:28:45 ART 2000
La version assemble_prof() da igual que la assmeble(). 
Salvo version 1.3 2000/04/23 (spider)

%===Sun Apr 23 17:37:06 ART 2000
Con dos procesadores tambien da bien. 

%===Sun Apr 23 21:47:15 ART 2000
Calcula bien perfiles llenos con assemble_prof() (jacobiano por
dif. finitas). Voy a probar con dos procs.

%===Sun Apr 23 23:07:24 ART 2000
Reiventa el calculo del jacobiano por diferencias finitas en mas de un
procesador. 
Salvo version 1.4 2000/04/24 (spider)

%===Tue Apr 25 18:22:55 ART 2000
Encontre una perdida de memoria. SI SE ALOCA CON NEW ENTONCES HAY QUE
BORRAR CON DELETE!!! En el destructor de Gpdata hacia  
 FM_shape[ipg]->~FastMat();
 FM_dshapexi[ipg]->~FastMat();
y habia que hacer
 delete FM_shape[ipg];
 delete FM_dshapexi[ipg];
al borrar las componentes FastMat. 

%===Tue Apr 25 19:47:13 ART 2000
Tiempos:
%
Para el cilindro (30000 elementos cuadrangulares) a Re=5000. Con nnwt=3
(iteraciones de Newton):
%
Version nueva: 1' por paso de tiempo, 20'' por subiteracion de
Newton. 12'' evaluacion del residuo y matriz (una sola llamada en la
nueva version) y 8'' de resolucion. evaluacion/total = 60%
%
Version vieja: 2'10'' por paso de tiempo, 43'' por subiteracion de
Newton. 35'' evaluacion del residuo y matriz (dos  llamadas en la
vieja version) y 8'' de resolucion. evaluacion/total = 80%
%
evaluacion del residuo (vieja/nueva) -> 3.0
%
total de la subiteracion de Newton (vieja/nueva) -> 2.15

%===Wed Apr 26 10:17:03 ART 2000
Todas las corridas anteriores fueron con el cluster desbalanceado (6
procesadores, de los cuales uno es PII 350Mhz, otro PIII 450 Mhz y el
resto PIII 500 MHz). Ahora trajeron el 7mo procesador PIII 500
Mhz. Con lo cual probe a correr con 6 procesadores (todos los PIII y
dejando afuera el PII) y el tiempo baja a 52 sec por paso de tiempo,
17.5 por subiteracion de Newton. 

%===Sun May  7 10:33:09 ART 2000
Estoy introduciendo el balance de carga entre procesadores. Para eso
Metis tiene una serie de rutinas de mas bajo nivel como
METIS_WPartGraphKway donde la W indica que es pesado y se le pasa un
peso por procesador. La macana es que en esta serie de rutinas hay que
pasarle el grafo y no solamente las conectividades. De todas formas
aprovecho y eso me permite hacer que particione tomando la informacion
de varios elemsets, ya que voy a armar el grafo yo. 

%===Sun May  7 14:14:48 ART 2000
Anda la division con peso con METIS_WPartGraphKway. En la cavidad de
20x20, poniendo dos procesadores con peso 0.25,0.75 da una division
que agrupa unos 97 elementos en la esquina para el proc 1 y otros 303
para el otro proc 2. 
%
Ahora hay que particionar los nodos. 

%===Sun May  7 18:40:07 ART 2000
Anda bien la particion de nodos inducida. Ahora hay que verificar que
ande bien Navier Stokes en un solo procesador. Despues en dos
procesadores sin y con diferencia de carga. Despues con dos
procesadores y dos elemsets. 

%===Sun May  7 18:59:24 ART 2000
Coincide el calculo en un procesador y en dos procesadores. Ahora voy
a comparar con la vieja rutina read_mesh. 

%===Sun May  7 19:07:42 ART 2000
Coincide con la vieja read_mesh. Ahora voy a probar con dos
procesadores y dos fat elemset. 

%===Sun May  7 19:22:27 ART 2000
Anda bien con dos procesadores y dos fat_elemsets.

%===Fri May 12 11:53:09 ART 2000
Lo termine de implementar y lo corro en el cluster. En read_mesh busco
una opcion `proc_weights' en el thash `global_options' y eso define un
archivo donde estan los pesos de los diferentes procesadores
(normalmente el archivo se llama weights.dat'). Un
script de Perl llamado procsel lee una tabla `proctable' que contiene
procesador/peso (valocidad) y escribe `weights.dat' y `machi.dat' para
MPI. 

%===Fri May 12 12:01:44 ART 2000
Con balanceo de carga da 30 seg/iteracion mientras que sin balanceo da
39 seg/iter. Lo curioso es porque tarda 39 seg/iter cuando antes
tardaba 20 seg/iter. (nos referimos a subiters de Newton). 

%===Fri May 12 12:44:56 ART 2000
La discordancia de tiempos que pasaba antes es que habia un
emacs corriendo que se chupaba toda la CPU. Ahora, balanceado da 15
seg/iter (contra 18seg sin balancear). 

%===Fri May 12 19:25:24 ART 2000
Todavia tengo un quilombo con el assemble_prof y el assemble. Lo voy a
unificar. Hago un save en .tgz en geronimo.

%===Sat May 13 08:26:28 ART 2000
Habia algo que estaba medio mal. En read_mesh la parte en la que
hacia el da_sort() de los ghost_elems se hacia fuera del lazo sobre
los elemsets, de manera que lo hacia solo para el ultimo elemset
visitado y podria ocasionar que para los otros elemsets los
ghost_elems quedaran desordenados. 

%===Sat May 13 17:33:23 ART 2000
En read_mesh.cpp: hace falta que line sea pasado como argumento de
read_hash_table? Porque no que quede definido adentro?

%===Sat May 13 17:37:23 ART 2000
Voy a mover filestack a un archivo fstack.cpp/h

%===Sun May 14 20:48:03 ART 2000
Implemente las condiciones de contorno dependientes del tiempo. La
idea es que ahora cada `fixation_entry' contiene un registro
`Amplitude *amp'. Las fijaciones que no dependen del tiempo tienen un
puntero nulo. Para las que si dependen del tiempo el objeto apuntado
*amp contiene un string `char *amp_function_key' que identifica a la
funcion apropiada y una serie de propiedades en una hash table
`TextHashTable *thash'. Por ejemplo si la condicion es c+A*\sin(\omega
t+\phi) entonces la entrada en el archivo de datos es del tipo
%
fixa_amplitude sine
const_val <c>
amplitude <A>
omega <\omega>
phase <\phi>
__END_HASH__
<node> <field> <val>
<node> <field> <val>
<node> <field> <val>
...
__END_FIXA__
%
values enclosed in <> are appropriate  numeric values for the
quantities enclosed. El string en el objeto amplitude seria
`sine'. Cuando se llama a la rutina que calcula residuos, esta va
localizando los valores del vector de estado en la rutina
`download_vector'. Cuando los valores nodales estan fijos, estos se
calculan de las fijaciones. Si dependen del tiempo, el tiempo es
pasado desde el main a las rutinas de mas bajo nivel por un `void
*time_data' y finalmente es pasado a la funcion. Las funciones
dependientes del tiempo se registran en una table `function_table'
(miembro estatico de la clase `Amplitude'). Los de la amplitud
dependiente del tiempo se calculan por `amp->eval(time_data)', donde
`eval' es un miembro de la clase amplitude. 
%
Renegue bastante con un problema que tuve con el miembro estatico
`static FunctionTable *function_table' de la clase `Amplitude'. La
regla es que para acceder a estos miembros estaticos sin tener que
referenciar a ningun objeto de la clase hay que hacerlo a traves de
funciones tambien definidas como estaticas. Por otra aprte, las
funciones estaticas no pueden acceder a los miembros no
estaticos. Para llamarlos hay que usar el scope operator, por ejemplo
`Amplitude::ad_entry("sine",&sine_function);". Ademas recordar que los
miembros estaticos hay que inicializarlos!! Sino no los crea y despues
en la etapa de linkedicion da un "undefined reference", por ejemplo
`FunctionTable *Amplitude::function_table=NULL;' en `dofmap.cpp'. 
No hay que inicializarlos en un header, porque sino da como una
definicion multiple. O sea, la inicializacion hay que pensarla como la
definicion de una funcion. Como no sabia como inicializar un
`map<string,AmplitudeFunction *>' defini al miembro `function_table'
de la clase `Amplitude' como un puntero a una `FunctionTable' que se
puede inicializar simplemente como `NULL'. 

%===Sun May 21 19:41:20 ART 2000
Voy a agregar un tipo de argumento "USER_DATA" de tipo (void *).

%===Wed May 24 13:18:07 ART 2000
Fijado un "leak memory" en advective.cpp. Al final del archivo, al
borrar A_jac y A_jac_av hay que hacer `delete' en vez de `~Matrix()' y
ademas el contador iba hasta `j<ndim' en vez de `j<=ndim'
%
  for (int jd=1; jd<=ndim; jd++) {
    delete A_jac[jd-1];
    delete A_jac_av[jd-1];
  }
. 

%===Thu May 25 18:24:43 ART 2000
Implementado el auto_time-step. Ahora voy a implementar el 
local_time_step. 

%===Thu May 25 19:42:57 ART 2000
Parece andar el local_time_step. Ahora voy a pasar las 
opciones globales (mesh->global_options) como una
variable global. 

%===Thu May 25 20:11:09 ART 2000
Abandono con lo de pasar las opciones en forma global. Voy a tratar de
poder usar todos lo sistemas advectivos al mismo tiempo con clases
derivadas.

%===Sat May 27 10:11:25 ART 2000
Finalmente pude hacer andar el euler de manera que ahora se puede
correr euler, shallow-water, etc... dentro del mismo programa. Eso lo
hice con clases derivadas. 

%===Sat May 27 11:21:50 ART 2000
Changed `read_mesh.cpp' to `readmesh.cpp' in order to have filenames
8.3 compatible and flux_fun...cpp o ff....cpp. 

%===Sat May 27 11:51:34 ART 2000
Rewritten the TAGS target in the makefiles. 

%===Wed Jun  7 20:47:17 ART 2000
Added description of advective elemsets to the doc. 

%===Fri Jun  9 12:02:34 ART 2000
Escribiendo la documentacion de funciones temporales. 

%===Sat Jun 10 14:11:16 ART 2000
Cambio los nombres de los archivos en laplace: el main es `laplace.cpp' y
el elemento es `lapla.cpp'. 

%===Sun Jun 11 13:40:59 ART 2000

Corregi un bug serio en la lectura de datos con get_double and
friends. En realidad el problema era en la alocacion con `new
char'. En varios lados estaba `new_string = new char[strlen(string)]'
para despues copiar `string' en `new_string'. No se estaba reservando
un lugar para el trailing NULL. (Esto es un error clasico!!!). Lo
reemplaze en todos lados por `new_string = new
char[strlen(string)+1]'.

%===Sun Jun 11 13:44:02 ART 2000
Estoy escribiendo un caso test para funciones dependientes del
tiempo. 

%===Sun Jun 11 22:00:51 ART 2000
Cambie "read_mesh.h" por "readmesh.h". Estoy recuperando la rutina
`print_some'. 

%===Mon Jun 12 21:30:25 ART 2000
Estoy debuggeando un efecto extranho. En geronimo no anda bien el test
`sector.dat'. Mientras que en spider anda bien. En geronimo
aparentemente no procesa 3 de los 100 elementos. Antes estaba con
jacobiano por diferencias y ahora lo puse analitico. Sera eso?

%===Mon Jun 12 21:37:46 ART 2000
No. En casa da bien con jacobiano analitico y numerico. Tambien con
chunks de 2, 200 y default (creo que 20).

%===Tue Jun 13 13:40:03 ART 2000
Encontre el error. Los datos de `sector.dat' estaban medio mal, es
decir en ciertos nodos que estaban fijos ademas le imponia condiciones
periodicas. 

%===Tue Jun 13 22:47:31 ART 2000
Arregle el error, pero de todas formas no lo pude verificar bien
porque lo corri en minerva, pero donde andaba mal era en geronimo. Lo
que hago ahora es que cuando perm[] del nodo fijado da 0, entonces no
remapeo la fijacion. 
%
Defino una clase TimeData que es general y otra Time que es un
doble. TimeData es mas general y podria tener cualquier cosa
adentro. Pero de todas formas me da la sensacion que no lo estoy
haciendo bien. Deberia hacerlo con clases virtuales o algo por el
estilo.

%===Fri Jun 23 09:45:16 ART 2000
Estoy escribiendo la libreria FastMat2. 
%
Hasta ahora tenia una rutina sync_dims que sincronizaba las
dimensiones de la matriz. Ahora voy a ser que devuelva las
dimensiones, asi no hay que dejar de declarar `const' al objeto. 

%===Tue Jun 27 21:16:22 ART 2000
Borro location_abs(). Creo que no se esta usando. 

%===Wed Jun 28 10:47:41 ART 2000
Anda bien ir() e is(). Ahora voy  a hacer que is(j) sin mas argumentos
limpia el filtro de la coordenada j. 

%===Wed Jun 28 22:45:31 ART 2000
Estaba mal el dimensionamiento de A_jac
%
A_jac_FM2(3,ndim,ndof,ndof);

%===Fri Jun 30 21:50:09 ART 2000
Ahora desde el advecive.cpp escrito en Newmat corre el ffeuler escrito
en FM2 a traves de una rutina de adaptacion ffadapfm2.cpp. Ahora voy a
probar que ande bien con Newmat y despues con FM2. 

%===Sat Jul  1 20:35:42 ART 2000
Andan los cache!! Hasta ahora andan para las operaciones`'one to
one'. El paso siguiente es escribirlas para las operaciones setel,
addel, scaleel. 
%
Para una operacion de 3x3 de copy, (set) y mult (multiplicacion
elemento a elemento) da unos 8 Mflops. Para chunks de 10 baja a 7. 

%===Wed Jul  5 11:45:53 ART 2000
Anduve mirando tiempos en FastMat2 y anda bien, en algunos casos es
unas 10 veces mas rapida que Newmat. Pero el overhead es realmente
malo, hay que ir hasta 100 o 1000 veces ejecutar con cache para que
deje de incidir el tiempo de armado del cache. Voy a escribir una
mejor clase Indx. 

%===Wed Jul  5 19:07:22 ART 2000
La clase Indx esta mejor escrita. Es un vector de longitud MAXINDX,por
ahora no hay chequeo de "out of bounds". FM2 da en el problemita de
hacer c = a(:,1)*a(1,:) 5.55 Mflops y Newmat 0.64Mflops. 
El punto de corte anda en Nin=100. (Antes estaba en 1000 o peor). 

%===Thu Jul  6 12:55:26 ART 2000
Agregue un conteo de operaciones en el cache_list. 

%===Sun Jul  9 12:59:57 ART 2000
Andan las listas de cache con branching, anda el conteo de
operaciones.

%===Mon Jul 10 19:44:07 ART 2000 mstorti@node1.beowulf.gtm
;(defalias 'notas-insert-date (read-kbd-macro
;			      "=== <<date>> C-e SPC C-u ESC !uname SPC -n RET <down> C-k \
;<up> C-e 3*<C-left> C-u ESC !whoami RET <down> C-a DEL @ C-e <down>"))
Nueva definicion de notas-insert-date. (ver arriba). 
%
En set(double *) y export(double *) tuve que hacer que si la matriz no
esta definida entonces no exporta nada. 

%===Wed Jul 12 06:18:24 ART 2000 mstorti@localhost.localdomain
;(defalias 'notas-insert-date (read-kbd-macro
;         "%=== <<date>> C-e SPC C-u ESC !whoami RET C-e @ C-u ESC !uname \
;SPC -n RET C-a <down> 2*C-k"))
New-new version of notas-insert-date. 
%
tau_supg is not reshaped now. If it is scalar it is passed in position
1,1, the rest of the matrix is ignored, and this is flagged by the
writer of the flux_fun routine through a bit filed in int variable
'ret_options' (options for return values).

%===Wed Jul 12 06:50:16 ART 2000 mstorti@localhost.localdomain
Voy a escribir una clase de matrices derivada de FastMat2 pero solo
para dos dimensiones. La llamo FMatrix. 

%===Wed Jul 12 22:26:23 ART 2000 mstorti@localhost.localdomain
Escribo double FastMat2::sum_square_all(). El truco es tener una
matriz estatica escalar A adentro y entonces adentro se llama a
A.sum_square(*this). Ahora lo voy a hacer para todos los otros. 

%===Thu Jul 13 10:42:33 ART 2000 mstorti@node1.beowulf.gtm
Acepto que una de las dimensiones al crear una matriz sea nula. En ese
caso hago todo menos hacer el `new' y setear la bandera `defined' a 1.

%===Thu Jul 13 12:58:07 ART 2000 mstorti@node1.beowulf.gtm
Habia un bug en cuanto a que no se deleteaba el cache en toda una
serie de rutinas cuando no se estaba usando la opcion de guardar el
cache. 

%===Thu Jul 13 13:10:05 ART 2000 mstorti@node1.beowulf.gtm
FM2 sin cache da una iteracion del naca (2429 elementos) en 2' (a 50%
CPU), es decir 24 [sec/1000 elem]. 
%
No pierde memoria! 
%
Ahora voy a ver cuanto da Newmat y despues FastMat con Cache.

%===Thu Jul 13 13:15:00 ART 2000 mstorti@node1.beowulf.gtm 
Con Newmat da 38'' el mismo caso, con la misma distribucion de
carga. Es decir una relacion 3 a 1. Newmat estaria dando 8 [sec/1000
elem].

%===Thu Jul 13 18:30:21 ART 2000 mstorti@node1.beowulf.gtm
Con FastMat2 cached da 12 seg (a 50% CPU), es decir 2.5
[sec/1000elem].

%===Fri Jul 14 08:48:09 ART 2000 mstorti@node1.beowulf.gtm
Algo anda mal en las medidiones de tiempo. Por empezar el upload y
download de vectores se lleva mucho tiempo, asi que voy a tratar de
medir tiempo haciendo muchas llamadas a assemble solamente. 

%===Fri Jul 14 09:38:26 ART 2000 mstorti@node1.beowulf.gtm
%
Dentro del debugger y con -pg activado da 1 sec/Ke para assemble fon
FM2. Fuera del debugger  0.968273 sec/Ke.
%
Newmat: 7.404418 sec/Ke. 
%
FM2 sin el -pg para profiler 0.609000 sec/Ke. 
%
Newmat sin el -pg:
total 32.270000, ntimes 10, nelems 500, rate 6.454000 [sec/1000/elems/iter]
%
Overhead en FM2: para chunk_size=100 la tasa baja a 0.825000 sec/Ke.
Para chunk_size=30, la tasa es de  1.433333 sec/Ke. 
%
El ajuste por minimos cuadrados da t [sec] = 0.026 + 5.55e-3 * (Nro de
elementos), lo cual da un overhead equivalente a 47 elementos. La
eficiencia para unos 500 elementos estaria en 10/11 = 91% aprox.
%
Toda la rutina assemble en FM2:
total 58.310000, ntimes 10, nelems 2429, rate 2.400576
[sec/1000/elems/iter]. Quiere decir que le agrega unos 1.8 sec sobre
los .6 que requiere la rutina advective con fm2.
%
Toda la rutina assemble en Newmat:
total 195.340000, ntimes 10, nelems 2429, rate 8.041993
[sec/1000/elems/iter]. Quiere decir que le agrega unos 1.6 sec a los
6.45 de la rutina de Newmat. Coincide con la estimacion del overhead
de upload/download para FM2.

%===Fri Jul 14 13:14:34 ART 2000 mstorti@node1.beowulf.gtm
%
Voy a escribir versiones "rapidas" de los containers `map' y
`vector'. Vector ya casi esta, (lo tomo de Indx) y map se va a basar
en vector. 

%===Sat Jul 15 12:10:17 ART 2000 mstorti@localhost.localdomain
Ya andan las versiones rapidas. Para adv con chunk_size=500 da 0.93
sec/Ke el assemble cuando el advective solamente requiere 0.63 sec/Ke
significando que el overhead de element.cpp (upload/download) es de .3
sec/ke (32%). Esta mucho mejor pero de todas formas es muy elevado. 

%===Sat Jul 15 14:06:43 ART 2000 mstorti@localhost.localdomain
Con gprof:
total 423.870000, ntimes 100, nelems 2429, rate 1.745039 [sec/1000/elems/iter]
%
Quiere decir que con gprof se achancha bastante (sin gprof da .93
sec/Ke). Pero esto va a cambiar cuando ponga el shock-captring. 
%
El gprof canta que todavia estamos gastando la mitad del tiempo en
upload/download contra el tiempo de calculo puro. 

%===Sat Jul 15 17:12:27 ART 2000 mstorti@localhost.localdomain
%
Ahora el target $(PROG).bin esta en Makefile.base. Guarda! que si en
vez de `$(PROG).bin' pones `%.bin' entonces los `*.o' se convierten en
secundarios y los borra despues de compilar. Una posibilidad es poner
`.SECONDARY: $(MYOBJS)' pero parece ser mejor poner directamente como
target `$(PROG).bin'. 

%===Sat Jul 15 17:14:12 ART 2000 mstorti@localhost.localdomain
%
Verifico que la version rapida da bien los resultados en
euler/naca.epl. 
%
Sacando el Mat/VecSetValue() se llega a 0.897900 sec/Ke. Lo cua quiere
decir que el overhead por carga de valores no es sensible. 

%===Sat Jul 15 17:47:53 ART 2000 mstorti@localhost.localdomain
%
Verifico la ganancia con la nueva version de upload/download:
vieja version: 2.122684 sec/Ke. 
nueva version: 0.929601 sec/Ke. Factor de ganancia: 2.3 

%===Sat Jul 15 18:45:10 ART 2000 mstorti@localhost.localdomain
%
Verifico que no tiene perdidas de memoria.

%===Sun Jul 16 10:17:23 ART 2000 mstorti@localhost.localdomain
Con optimizacion da 0.474434 sec/Ke. Factor de ganancia: 1.98

%===Sun Jul 16 12:36:52 ART 2000 mstorti@localhost.localdomain
%
Implemento la funcion de flujo para shallow water.
%
Performance para shallow water:
total 16.160000, ntimes 50, nelems 200, rate 1.616000 [sec/1000/elems/iter]
%
Guarda que el numero de elementos es bajo. Despues voy a probar a
llevarlo a 500. Esta compilado sin profiler y con debugger.
%
Con Newmat
%
total 37.780000, ntimes 50, nelems 200, rate 3.778000 [sec/1000/elems/iter]
%
Mas de dos veces mas lento.
%
Guarda que el chunk_size estaba en 20!! Paso el CHUNK_SIZE a 200 en
`fem.h' y a 500 en plano.dat.
Con Newmat:
total 7.070000, ntimes 10, nelems 200, rate 3.535000
[sec/1000/elems/iter]
Con FastMat2 [chunk_size de 200]
total 0.820000, ntimes 10, nelems 200, rate 0.410000 [sec/1000/elems/iter]
Con FastMat2 [chunk_size de 500]
total 1.960000, ntimes 10, nelems 500, rate 0.392000 [sec/1000/elems/iter]

%===Sun Jul 16 19:55:00 ART 2000 mstorti@localhost.localdomain
%
Escribi el shock capturing en FastMat2. Da igual (a precision de la
maquina) que con Newmat. Ahora tengo que activar el cache. 

%===Sun Jul 16 20:22:18 ART 2000 mstorti@localhost.localdomain
%
Anda le euler con FastMat2 y caches. Es 5.7 veces mas rapido. 
Ahora voy a probar con la version optimizada. 
%
Hay un problema con los caches. Si hay un `if' sin `else' entonces
cuando no entra en el `if' entonces queda un cache desfasado. Por ahora
se arregla poniendo siempre un `else'.

%===Sun Jul 16 20:34:22 ART 2000 mstorti@localhost.localdomain
La version optimizada da 1.274000 sec/iter para FM2 y 
14.010000 sec/iter para Newmat. Pero guarda que Newmat no esta
compilado con optimizacion. 
%
Con optimizacion Newmat baja a 7.57 sec/iter con lo cual la relacion
baja a un factor 6.
%
La llamada a `assemble' solo da 3.03 sec/Ke con Newmat y 0.50 sec/Ke
con FastMat2,  o sea tambien una relacion de 6 a 1.
%
La llamada al elemento solo (`advective' o `advectiveFM2') da 2.98
sec/Ke con Newmat y 0.4 sec/Ke con FastMat2 o sea que la relacion sube
a 7.45. 
%
Tambien se ve que para FastMat2 el overhead de upload y download es de
un 20%. 

%===Mon Jul 17 18:12:33 ART 2000 mstorti@node1.beowulf.gtm
Habia un bug en los puntos de Gauss para triangulos. 

%===Tue Jul 18 13:26:18 ART 2000 mstorti@node1.beowulf.gtm
%
Habia dos bugs que morfaban memoria. 
%
1./ En fem.cpp compute_prof() habia que hacer el `da_destroy(da)'
*antes* de definir la matriz, sino estan ambas definidas al mismo
tiempo. 
%
2./ En `fstack.cpp' habia que poner la linea compiled=1 para que no
recompilara cada vec, lo cual ademas daba lugar a una perdidad de
memoria. 

%===Fri Jul 21 12:23:22 ART 2000 mstorti@node1.beowulf.gtm
%
Cuando quise compilar NS con el optimizador pow() me daba uns
SIGSEGV. Lo arregle usando cbrt() (raiz cubica) y un macro SQ() que
hace el cuadrado. 

%===Fri Jul 21 12:24:42 ART 2000 mstorti@node1.beowulf.gtm
%
Perdida de memoria en remap_cols(). Despues de renegar y renegar no
encontre el problema y resolvi escribir un nuevo remap_cols() que haga
el remap copiando en un nuevo idmap y entonces despues borra el viejo
y deja el nuevo. Esto va a ser mejor siempre, pero de todas formas no
queda en claro que porque se pierde memoria con el set_elem() y esto
puede ser peligroso en alguna aplicacion donde se use mucho. 

%===Fri Jul 21 17:43:16 ART 2000 mstorti@node1.beowulf.gtm
%
Hago un directorio `PETSC/RUPERTO/advective.ruperto' donde Ruperto
puede poner sus funciones temporales. 

%===Sat Jul 22 11:50:15 ART 2000 mstorti@localhost.localdomain
%
Informe final sobre el remap_cols() bug:
========================================
%
%
El efecto de este bug es que, despues de definir el idmap dofmap->id,
en readmesh, cuando hace la rutina `remap_cols()' que remapea las
columnas (grados de libertad) de forma de dejar las que corresponden
al procesador 0 primero, despues las del 1, etc... hasta nptoc-1, y
despues los fijos, entonces despues de remapear las columnas se morfa
una gran cantidad de memoria que despues no es liberada. No he llegado
a la raiz del problema pero he logrado los siguientes diagnosticos:
%
*./ La memoria perdida depende del tamanho maximo al cual ha llegado
los col_map y row_map y la suma de los row_t y col_t individuales, es
decir de todos los `map'. Esto parece ser un `bug' o por lo menos un
funcionamiento no deseado en el compilador o las librerias STL. 
%
*./ La memoria no se recupera ni siquiera destruyendo los objetos. Sin
embargo, si se vuelven a usar maps (eventualmente otros) la memoria es
reusada. Pareceria ser entonces que la implementacion de los maps
reserva una cantidad de memoria para para los maps que nunca es
liberada ni reducida, incluso si los maps son destruidos. 
%
*./ Hay un ejemplo tryme2.cpp que muestra esto.
%
*./ La solucion actual es cambiar el remap_cols() de manera que se van
cargando las filas del viejo idmap en un nuevo idmap y despues se
destruye el viejo. De esa forma la memoria perdida es nula porque no
se usan los maps ya que no hay columnas ni filas especiales. 
%
*./ Otra posibilidad seria reimplementar el idmap de otra forma, por
ejemplo tomando un `binary tree' de libretto, o algo mas pedestre
implementado por mi directamente. 

%===Sat Jul 22 12:48:05 ART 2000 mstorti@localhost.localdomain
%
Cambio la forma de actualizar makefile.d en los makefiles y como
salvarlos. Ahora los makefile.d no se guardan en `source.tara'. Y en
los Makefile se agrega dos lineas que dice "makefile.d: cat /dev/null
>makefile.d ; makedepend -f makefile.d ...". De esta forma se generan
los makefile.d cuando se necensitan. 

%===Sat Jul 22 20:37:38 ART 2000 mstorti@localhost.localdomain
%
Fixed a bug introduced during the change of remap_cols(). In
remap_cols the line `n = n_new;' should be `idnew.n = n_new;'. 

%===Sat Jul 22 20:46:59 ART 2000 mstorti@localhost.localdomain
%
Hay un problema en al correr el plano.dat en ./run. Se produce un
error al resolver por GMRES me parece que se debe a que en la nueva
version la matriz que debe retornar es nula. 

%===Sat Jul 22 21:18:38 ART 2000 mstorti@localhost.localdomain
%
Corregido el error. Habia quedado comentada una linea en advective.cpp
al agregar el beta_supg.

%===Sun Jul 23 06:29:41 ART 2000 mstorti@localhost.localdomain
%
Muevo el target %.depl de `run' y `test' a makefile.base.

%===Sun Jul 23 10:30:01 ART 2000 mstorti@localhost.localdomain
%
Agrego para det() e inv() el calculo via Newmat. Ahora voy a hacer
eig(). 

%===Wed Jul 26 20:06:12 ART 2000 mstorti@node1.beowulf.gtm
%
Descubrimos cual era el problema por el cual para problemas grandes
parecia que "duplicaba" la memoria. Se debia a que cuando haces
MatSetValues los elementos quedan en un cache que recien es mandado a
comunicacion cuando llamas a MatAssemblyBegin/End(). Entonces, si
tenes muchos elementos el cache es tan grande como la matriz. Es
preferible entonces hacer el MatAssemblyBegin/End() despues de cada
chunks o cada cierto numero de chunks. Pero eso obliga a sincronizar
el procesamiento de cada chunk si no se te desbalancea el calculo ya
que el MatAssemblyBegin/End impone una barrera MPI. Entonces
calculamos un tamanho local ideal del chunk de la forma
%
local_chunk_size = (int)(chunk_size*dofmap->tpwgts[myrank]/w_max) +1 ;
%
donde w_max es el maximo weight (maxima velocidad). Des esta forma el
tamanho de los chunks es proporcional a la velocidad del procesador y
en el de mayor velocidad (por lo tanto, el que tiene mayor tamanho de
chunk) es el deseado chunk_size. 
%
Ahora bien, por problemas de `redondeo entero' etc... puede ser que el
numero de chunks varie de procesador a procesador, pero todos tienen
que llamar a MatAssemblyBegin/End el mismo numero de veces asi que en
algunos procesadores hay que llamar a MatAssemblyBegin/End sin haber
procesado elementos en ese chunk, solo para sincronizar. Para detectar
si un dado procesador termino hacemos 
%
      int local_has_finished= (el_last==nelem-1);
%
Y despues para ver si todos terminaron hacemos un Allreduce
%
      int global_has_finished;
      ierr = MPI_Allreduce((void *)&local_has_finished,
			(void *)&global_has_finished,1,MPI_INT,
			MPI_LAND,PETSC_COMM_WORLD);

%===Fri Jul 28 11:04:08 ART 2000 mstorti@node1.beowulf.gtm
%
Habia un problema con el nuevo procesamiento por chunks. No andaba
(daba un SIGSEGV) cuando se corria en mas de un procesador y con
chunk_size grande de manera que todo entra en un solo chunk. Lo
arregle cambiando `local_chunk_size' por `chunk_size' en la linea
%
	if (iele_here == chunk_size-1) break;
%
en `elemset.cpp'.

%===Fri Jul 28 11:33:51 ART 2000 mstorti@node1.beowulf.gtm
%
Voy a resolver el problema del branch/leave en FM2. La idea es que
funcione asi;
%
...
spawn()
if (...) {
  choose(j1);
  ...
} else if (...) {
  choose(j2);
  ...
} 
leave();
...
%

%===Sat Jul 29 13:03:49 ART 2000 mstorti@localhost.localdomain
%
Convierto FastMat2 a listas variables de argumentos pero via
argumentos con valores defaults. La idea es que las llamadas son de la
forma
%
fun(int m_0=0,int m_1=0,int m_2=0,int m_3=0,int m_4=0)
%
Y se supone que nunca es llamada con un numero variable de argumentos
diferente de 0. Entonces el numero de argumentos se obtiene mirando
cual es el primer argumento diferente de 0. Escribo unos macros
ARG_LIST(type,name,default)  y
READ_ARG_LIST(name,indx,default,exit_label) que expanden a la lista de
argumentos y despues lo leen.

%===Tue Aug  8 22:08:08 ART 2000 mstorti@localhost.localdomain
Agrego la funcion `piecewise' (lineal a trozos) escrita por
Beto. Agrego un campo en la clase `Amplitude' de tipo `void *' que es
para que el usuario pueda calcular datos estaticos y dejarlos
ahi. Atencion que no es lo mismo que tener datos de tipo `static' ya
que entonces no podrian variar entre dos diferentes regiones que
tienen la misma funcion de tipo 'piecewise', por ejemplo, pero tienen
diferentes parametros.

%===Tue Aug 15 19:51:41 ART 2000 mstorti@node1.beowulf.gtm
Empezamos a implementar LES / Smagorinsky. 
%
Agregamos un coeficiente C_smag. Si es cero no se hace turbulencia.
Por defecto es 0.18
%
Problema del ducto ciruclar: Hay que agregarle una gradiente de
presion o fuerza por unidad de volumen. Le agrego una propiedad G_body
que es un vector de ndim componentes. Se define en la tabla de
propiedades globales. 
%
Defino un nuevo elemento `nsi_tet_les' que esta en el archivo
`nsitetles.cpp'. A la larga la version con y sin LES deberian convivir
en el mismo elemento pero por ahora mantengo los dos para poder
comparar etc. De todas formas no se si voy a tener que tocar el
elemento viejo por para agregarle el G_body.


%================================================================
%
% Mensaje enviado por beto.
%
Mario,

aqui te mando como attachment un fichero en Matlab que genera las
  - coordenadas [x3d]
  - conectividades [ico3d]
  - periodical bc [peri]
  - fijaciones [fixa]

Es una feta en z (dos capas, una entrante y la otra periodica saliente) y
dos fetas en theta (circumferencial) para evitar un elemento triangular en
el centro. En la direccion radial le especificas a la rutina Nr (nro de
elementos radiales).

Como condiciones de contorno,
 -  a la entrada u=v=0 en toda la feta entrante (por ende al ser periodica
la saliente tambien es asi)
 - a la entrada w = 0 en la pared
 - a la entrada w = 1 en el centro
 - presion libre en todos lados
 - a la salida todo es periodico con la entrada

OJO:
Al mediodia me avive que vos llamaste nu_t en el programa y eso habria que
sumarlo al VISCO, no?

Suerte

Beto

%===Wed Aug 16 13:36:37 ART 2000 mstorti@node1.beowulf.gtm
%
Esta empezando a andar el flujo en un ducto en regimen laminar. La
velocidad maxima me da 0.243 menor que la exacta (0.25). Pruebo a
sacarle el upwind (tau_fac=0.) y no da mejor. Voy a probar a disminuir
la abertura de la feta de elementos. 
%
Debemos tener un archivo de inicializacion con un perfil parabolico y
maxima velocidad 1 y entonces para un dado Re hay que poner nu=1/Re y
dp/dz= 4/Re

%===Wed Aug 16 17:43:13 ART 2000 mstorti@node1.beowulf.gtm
%
Ahora, normalizado e inicializado desde el perfil parabolico da perfecto.

%===Wed Aug 16 18:14:26 ART 2000 mstorti@node1.beowulf.gtm
%
Cuando usas pasos de tiempo grandes no podes seguir la evolucion
temporal. No es fisicamente correcta. Parecia que en el centro del
ducto evolucionaba mas lento que en r=0.5 pero se debia al efecto ese
del paso de tiempo demasiado grande. 

%===Wed Aug 16 19:02:20 ART 2000 mstorti@node1.beowulf.gtm
%
El lazo exterior de NS chequeaba convergencia para problemas
estacionarios a sobre ||delta u||. Lo pase a que chequee sobre ||R||. 

%===Thu Aug 24 13:19:00 ART 2000 mstorti@node1.beowulf.gtm
%
Habia un error en la implementacion del Crank-Nicholson. Primero, en
la linea 634 en vez de 	"FMaxpy(matlocmom,alpha*nu_eff,tmp7);" va sin
el alpha porque el alpha solo va en todas las lineas escaleando al
Dt. Ademas el tiempo que hay que pasarle a `assemble' es t* no
t^{n+1}, asi que definimos una variable  `Time time_star;' y en las
llamadas dentro del lazo hay que pasarle `time_star'. 
%
Guarda!! Ademas en realidad cada vez que se pasa un estado habria que
pasarlo con el tiempo asociado. Es decir pasar {u^n,t^n} y
{u^n+1,t^n+1}. 
%
De todas formas el Crank-Nicholson tiene ahora una convergencia
$\propto \Dt^2$. 

%===Thu Aug 24 18:57:41 ART 2000 mstorti@node1.beowulf.gtm
%
Cuando agregue LES dejo de converger en la primera subiteracion lineal
debido al tratamiento diferente que se le hace al tensor de tensiones
que en el caso tubulento se pone como 1/2 (grad_u+ grad_u_t). Ahora
corregi tambien el jacobiano y da todo bien. Por supuesto como el
problema es no-lineal la convergencia no es en una iteracion pero de
todas formas converge rapido (a Re pequenhos). 

%===Thu Aug 24 19:43:38 ART 2000 mstorti@node1.beowulf.gtm
%
Performance para NS: Problema del qbend, 
%* 51,129 elementos. 
%* Corriendo junto con el adv.bin de Ruperto. 
%* Krylov_dim 150
%
Da 6' por iteracion, es decir 284 elem/sec. 

%===Thu Aug 24 20:11:35 ART 2000 mstorti@node1.beowulf.gtm
%
Pruebo con la version 1.48g que supuestamente es la que estaba activa
cuando corria el qbend. Con esa da 5' contra 6'. Sera el chunk_size?

%===Thu Aug 24 20:35:59 ART 2000 mstorti@node1.beowulf.gtm
%
Con la version 1.45g y con un chunk_size de 1000 tambien da del mismo orden (4'50''). 

%===Thu Aug 24 20:53:25 ART 2000 mstorti@node1.beowulf.gtm
%
Con la version actual (seria la 1.63g) da 5'50'' por iteracion (igual
que antes) con chunk_size=1000 o sea que pareceria ser independiente
del chunk_size.

%===Fri Aug 25 08:47:46 ART 2000 mstorti@node1.beowulf.gtm
%
Con Dt=500 se va a la mierda. Claro, no hay que basarlo sobre el
Fourier sino sobre el Courant. 

%===Fri Aug 25 11:50:59 ART 2000 mstorti@node1.beowulf.gtm
%
Empiezo a escribir el nsi_tet con FM2. Escribo una funcion identidad
`eye()'. 

%===Fri Aug 25 19:56:26 ART 2000 mstorti@node1.beowulf.gtm
%
Tuve que corregir la funcion rs() en FM2 ya que no reseteaba la
permutacion de indices.

%===Sat Aug 26 15:29:31 ART 2000 mstorti@localhost.localdomain
%
Hice otro casito test que es la cavidad cuadrada con 5 elementitos por
lado. Ahora lo uso para verificar que nsi_tet_les de lo mismo que
nsi_tet_les_fm2. 
%
Ahora hay que agregar los cache para FM2. 

%===Wed Aug 30 12:28:17 ART 2000 mstorti@node1.beowulf.gtm
%
El cache anda al pelo.  Estoy mirando el tema eficiencia. En casa me
daba que FM2 no era mas eficiente que FM, pero ahora aca en geronimo
da 3 veces mas rapido (sin optimizacion). Voy a ver ahora con
optimizacion. 3 sec/Ke con FM y 1 sec/Ke con FM2.
%
Con optimizacion da .28 sec/Ke con FM2 y .655 sec/Ke con FM. 
%
Guarda que en estos resultados el `loop gordo' con los `matij' estaba
comentado!! 
%
Incluyendo el lazo `matij' da 1.72 sec/Ke con FM2 y 1.43 sec/Ke para
FM. Mas o menos como daba en casa.
%
Ahora voy a sacar el gprof (opcion -pg)
%
Sin -pg da .65 sec/Ke para FM y 1.1 con FM2. 

%===Wed Aug 30 13:40:40 ART 2000 mstorti@node1.beowulf.gtm
%
Con el lazo `matij' vectorizado FM2 pasa a ser ligeramente mas rapido
que FM. 

%===Fri Sep  1 18:58:45 ART 2000 mstorti@node1.beowulf.gtm
%
Retomo NS+LES en FM2. Parece haber un error en la version FastMat de
`nsitetles.cpp'. Ahora dan iguales, pero me preocupa ese error que
habia en la version FastMat2. 

%===Tue Sep  5 11:17:57 ART 2000 mstorti@node1.beowulf.gtm
%
Detecto bug: En advective se detecta que cuando se ponen constraints
con coeficientes nulos da mal.

%===Wed Sep  6 10:57:54 ART 2000 mstorti@node1.beowulf.gtm
%
Fixed a serious bug in `elemset.cpp'. Temporary arrays (locst, retval)
were not destroyed for all elemsets, but only for the last.
%
Los delete[] de locst, pref, retval, etc... hay que hacerlos para cada
elemset, despues del lazo sobre los chunks. 

%===Wed Sep  6 22:57:38 ART 2000 mstorti@localhost.localdomain
%
Escribo la version weak_form=1 en advecfm2. Agrego en bcconv_adv el
termino H para shallow water. Tambien lo agrego a absorb (antes pasaba
basura y generaba un hterm cualquier cosa, guarda esto era una
bug!!). 
%
Agrego opciones __REWIND__, __FORWARD__ y __BACKWARD__ en
`myexpect.pl'. 

%===Fri Sep  8 20:12:38 ART 2000 mstorti@node1.beowulf.gtm
%
Arreglado un bug serio en la clase `idmap'. En `void
idmap::get_row(const int,IdMapRow &)' no se ponia la fila a vacio en
el caso `j==0', es decir ahora es:
%
  } else if (j==0) {
    row.resize(0);   // linea agregada!!
    return;
  } else {

%===Mon Sep 11 12:37:40 ART 2000 mstorti@node1.beowulf.gtm
%
`ffeuler.cpp' no compila con optimizacion asi que agregue una regla de
compilacion especial para ese archivo en el Makefile para que siempre
lo compile sin optimizacion. 
%
ffeuler.o: ffeuler.cpp
	-${CC} -c ${COPTFLAGS} ${CFLAGS} ${CCPPFLAGS} -O0 $<

%===Tue Sep 12 11:23:20 ART 2000 mstorti@node1.beowulf.gtm
%
Hay una pequenha asimetria en el codigo advectivo/shallow. Eso se debe
a que, cuando se usa local_time_step, el Jacobiano para calcular el
paso local de la malla, para escalear la matriz de masa se tomaba del
ultimo punto de Gauss visitado. Ahora se hace con un promedio de los
jacobianos. Ahora da perfectamente simetrico. 
%
El ||delta u|| parecia no converger, porque faltaba eliminar la
proyeccion de las condiciones absorbentes. Ahora formo el delta_u
despues de la proyeccion y converge muy bien. 

%===Tue Sep 12 13:25:13 ART 2000 mstorti@node1.beowulf.gtm
%
Voy a hacer un test para el bug de los `constraint'.

%===Tue Sep 12 18:46:51 ART 2000 mstorti@node1.beowulf.gtm
%
BUG: No estaba declarado estatico el regex en get_string(). 
Debia ser "static regex_t regex[2];" en getprop.cpp linea 109. 
Se colgaba cuando se hacia mas de un `get_string'. 

%===Tue Sep 12 19:29:54 ART 2000 mstorti@node1.beowulf.gtm
%
Hecho el test para el constraint bug. Esta hecho en un subdirectorio
de /test (esto es nuevo). 

%===Fri Sep 15 19:17:46 ART 2000 mstorti@node1.beowulf.gtm
%
Instale ANN (Approximate Nearest Neighbor) version 0.2 para LES. 
Hice un programa de verificacion (nneighbor/tryme.cpp) para verificar
los tiempos. Lo comparo con fuerza bruta (comparar con todos). 
%
ANN: total cpu_time: 2.010000, per point: 2.01e-09 , nnod: 10000, npoints: 100000
Fuerza bruta: total cpu_time: 5.080000, per point: 5.08e-07 , nnod:
100, npoints: 100000
%
O sea un factor 250 por lo menos. 

%===Mon Sep 18 08:27:14 ART 2000 mstorti@localhost.localdomain
%
Agrego al elemset un `double * elemprops_add' e `int * elemiprops_add'
para poner `propiedades adicionales', esto es, que son usadas
internamente por el `application writer'. Cuantos dobles o enteros son
necesarios es manejado por el campo `additional_props' y
`additional_iprops'. 

%===Tue Sep 19 11:24:23 ART 2000 mstorti@node1.beowulf.gtm
%
Estadistica de tiempos para back_step: (corriendo Beto). 
10,400 elementos, 2D con nsi_tet_les_fm2. Tarda 20 seg/subiter de
Newton. Es decir 60seg por iter del lazo exterior. Compitiendo con
RUperto. Sin ek procesador 5. Es decir que 
%
* considerando una potencia instalada de 5.65 procs. (tomando como base
el PIII  500Mhz). 
* 50% de uso efectivo (el otro 50% para Ruperto). 
%
El costo es de 5.4 [sec*proc/subiter/1000 elem]
%
Quiere decir que 100,000 elementos con una potencia instalada de 30
(14 PIII 700Mhz + los 6.65 PIII 500Mhz actuales) tendriamos un tiempo
de 18 sec/subiter. 

%===Tue Sep 19 17:05:32 ART 2000 mstorti@node1.beowulf.gtm
%
Sin correr Ruperto da 30 seg/iter lazo exterior como previsto. De
todas formas esta raro que la carga da un poco desbalanceada, en el
sentido de que con `$ bw uptime' da cargas fluctuando entre 60% y 90%
entre los diferentes procesadore. 

%===Thu Sep 21 10:54:13 ART 2000 mstorti@node1.beowulf.gtm
%
Debuggeando el elemento de pared LES `wall' encontre que los tiempos
de ensamblaje de la matriz se disparaban (como cuando calculas mal el
perfil). El perfil estaba bien calculado pero por eror estaba llamando
a `assemble' con `jobinfo="comp_wall_stres"' pero con un argumento
"A_tet,OUT_MATRIX" lo cual hacia que ensamblara la matriz. 

%===Thu Sep 21 11:16:08 ART 2000 mstorti@node1.beowulf.gtm
%
Para que de un warning cuando no alocaste bien la matriz (esto es,
cuando no calculaste bien el perfil) hay que poner:
%
  ierr =  MatSetOption(A_tet,MAT_NEW_NONZERO_ALLOCATION_ERR);

%===Sun Sep 24 18:24:36 ART 2000 mstorti@localhost.localdomain
%
Parece estar andando la implementacion de LES, por lo menos en cuanto
a la condicion de pared vista como condicion de contorno mixta. Si la
funcion de pared es lineal, es decir $\tau_w \propto u$ entonces da
convergencia en una iteracion. Si es no lineal, entonces da
convergencia cuadratica. 

%===Fri Oct  6 14:11:36 ART 2000 mstorti@node1.beowulf.gtm
%
Hago que FastMat2::print retorne void. Agrego una funcion
FastMat2::printd() que imprime las dimensiones de la matriz. 

%===Fri Oct  6 19:25:43 ART 2000 mstorti@node1.beowulf.gtm
%
Found serious bugs in 'FastMat2::reshape()'. It was missing a
`set_indx = Indx(ndims,0);' in order to reset the index restrict
vector, and a
%
   IndexFilter pp;
   for (int jd=0; jd<ndims; jd++) {
     ...
     dims[jd] = pp;
     ...
   }
%
in order to reset the filters for each dimension. 
%
Comment: this stuff with the 'store' = the fixed store, etc.. is
somewhat dangerous!! It should be rewritten properly!!

%===Mon Oct  9 17:17:50 ART 2000 mstorti@node1.beowulf.gtm
%
Agrego una bandera `shock_capturing_threshold' para regular cuando se
hace shock_capturing o no. 

%===Mon Oct  9 19:45:40 ART 2000 mstorti@node1.beowulf.gtm
%
Habia un bug en el macro `SGETOPTDEF_ND' (fem.h). No seteaba la
variable al valor default. 

%===Sun Oct 22 19:02:42 ART 2000 mstorti@localhost.localdomain
%
Ahora get_int retorna 1 si la entrada no tiene "miembro derecho" es
decir `print_internal_loop_conv' es equivalente a
`print_internal_loop_conv 1'. 

%===Mon Oct 23 19:11:47 ART 2000 mstorti@node1.beowulf.gtm
%
Abandono AFS y retomo LES. Da un error extranho cuando NP>1. Ahora me
doy cuenta de que parece que no puedo hacer lo de "procesar todos los
elementos" en cada procesador porque algunos no van a tener los datos
apropiados. 

%===Tue Oct 24 08:36:15 ART 2000 mstorti@node1.beowulf.gtm
%
Volviendo a hacer los calculos para "comp_shear_vel" por separado y
despues haciendo un "communicate_shear_vel" anda bien. Daba un SIGSEGV
pero era porque habia un FastMat2::deactivate_cache() cuando el cache
no estaba activado. Eso habria que arreglarlo. 

%===Tue Oct 24 11:07:15 ART 2000 mstorti@node1.beowulf.gtm
%
Ahora chequeo 'if (use_cache ...' antes de borrar el cache en
'void_cache()'. 

%===Wed Oct 25 11:57:59 ART 2000 mstorti@node1.beowulf.gtm
%
El les.epl tiene un problema que da NaN. Ahora esta andando, puede que
sea el compilar con Optimizador. 

%===Wed Oct 25 19:44:41 ART 2000 mstorti@node1.beowulf.gtm
%
Parece que se confirma que es el optimizador. 

%===Wed Oct 25 19:48:40 ART 2000 mstorti@node1.beowulf.gtm
%
No, falsa alarma. Con el debugger tambien da NaN. 

%===Wed Oct 25 19:53:02 ART 2000 mstorti@node1.beowulf.gtm
%
El mismo caso con LES=1 da NaN con LES=0 no. 

%===Fri Oct 27 13:25:21 ART 2000 mstorti@node1.beowulf.gtm
%
Da NaN cuando la velocidad en la pared es 0. Hay una division por 0 en
la formula. Habria que corregirlo (regularizarlo). 
%
Escribi un jobinfo "comp_res". La idea es no evaluar el jacobiano
todas las veces sino solo algunas. Pero parece no andar bien. 

%===Sat Nov  4 10:18:06 ART 2000 mstorti@localhost.localdomain
%
Tratando de mejorar la eficiencia. Observo que el tiempo de "comp_mat"
es aprox. la mitad del de comp_mat_res, cuando deberia ser mucho
menor!!!. Verifico que esto ocurre tanto con debugger como sin. 

%===Sat Nov 11 10:19:10 ART 2000 mstorti@localhost.localdomain
%

%
Agregue una version "linear" a FastMat2::prod(). En los experimentos
se gana bastante, un factor 2 o mas. Por ejemplo da 200-220 Mflops con
la version lineal y menos de 100 con la version no-lineal. Esto se
puede deber a que la version no-lineal hace mas operaciones pero
ademas requiere mas informacion ya que requiere un direccionamiento
mas. Estos datos en un P-III 733Mhz. 
%
Puesto en Navier-Stokes no se gana nada. 
%
Lo que si que la version FastMat2 eficiente se gana como un 20% sobre
le evaluacion del residuo. (11.32 sec/Ke contra 9.2 sec/Ke,
nsi_tet_les_fm2, P-II 350Mhz(spider)). 
%
Ahora voy a probar que pasa con el "cache" de matrices. 

%===Sat Nov 11 10:46:32 ART 2000 mstorti@localhost.localdomain
%
Con el cache de matrices se pasa de 9.2 sec/Ke a 6.7 sec/Ke!! Eso
parece estar bastante bien!! 

%===Sat Nov 11 10:49:18 ART 2000 mstorti@localhost.localdomain
%
Con el cache_grad_div_u tampoco parece haber diferencia entre la
version linear y no-lineal. Ahora vamos a verificar si la version mas
eficiente de todas `linear + cache_grad_div_u' da bien. (i.e., si
coincide con versiones anteriores.)

%===Sat Nov 11 17:44:00 ART 2000 mstorti@localhost.localdomain
%
Los tests dan bien. `oscaplate2b' da SIGSEGV pero despues no es usado
en los tests, asi que no lo cuento como test.
%
Sin embargo corriendo `test/les.epl' no da lo mismo que con versiones
anteriores. Voy a investigar esto. 

%===Sun Nov 12 10:04:03 ART 2000 mstorti@localhost.localdomain
%
Lo que pasa es que se junta bastante incompatibilidad con la nueva
estructura de datos, ya que ahora las opciones de elemento se pueden
poner en las opciones generales. 

%===Sun Nov 12 10:12:43 ART 2000 mstorti@localhost.localdomain
%
Tomando el caso test/sqcav5 parece dar lo mismo que la version 1.69g. 

%===Sun Nov 12 10:22:40 ART 2000 mstorti@localhost.localdomain
%
Vamos a ver si el oscplate2b deja de andar cuando se compila con -O2. 

%===Sun Nov 12 10:58:47 ART 2000 mstorti@localhost.localdomain
%
Efectivamente, con -O2 da un SIGSEGV. 

%===Sun Nov 12 18:27:22 ART 2000 mstorti@localhost.localdomain
%
Found a bug in tempfun.cpp in spline_function and piecewise_function.
Habia un error en la llamada a la sutina Fortran. Hay que reemplazar 
% spline_(&npoints,sd->time_vals,ampl_vals,sd->b,sd->c,sd->d);
por
% spline_(&npoints,sd->time_vals,sd->ampl_vals,sd->b,sd->c,sd->d);

%===Mon Nov 13 18:24:32 ART 2000 mstorti@node1.beowulf.gtm
%
En casa parecia que la version actual coincidia bien con una anterior
a empezar a investigar la eficiencia (creo que probe con la 1.69g),
pero ahora aca en geronimo no me coincide con la 2.16g. Voy a
verificar esto. 

%===Mon Nov 13 19:05:20 ART 2000 mstorti@node1.beowulf.gtm
%
Coincide bien pero si se usa la version vieja de `fmat2ep.cpp'. Quiere
decir que hay algun drama con la version `lineal'. 

%===Mon Nov 13 19:08:53 ART 2000 mstorti@node1.beowulf.gtm
%
Con "force non-linear" anda bien. 

%===Mon Nov 13 19:27:31 ART 2000 mstorti@node1.beowulf.gtm
%
Desactivando el "cache de fm2" anda mal igual. 

%===Mon Nov 13 19:31:52 ART 2000 mstorti@node1.beowulf.gtm
%
Decididamente lo que anda mal es el "linear addressing" no los caches.

%===Mon Nov 13 20:15:18 ART 2000 mstorti@node1.beowulf.gtm
%
Encontre el error! Cuando la dimension contraida era 1 (por ejemplo
cuando prod() se usa para hacer un kron) entonces el inca e incb
quedaban en 0 y el lazo no se hacia. Ahora dejo que si la dimension
contraida es menor o igual que 1 entonces se usa la version
no-lineal. 

%===Mon Nov 13 20:20:22 ART 2000 mstorti@node1.beowulf.gtm
%
Ahora anda bien version lineal y tambien si activo
EFFICIENT_FM2_VERSION en nsitetlesfm2.cpp.

%===Mon Nov 13 20:23:58 ART 2000 mstorti@node1.beowulf.gtm
%
Con `cache_grad_div_u' tambien da bien. 

%===Mon Nov 13 20:27:24 ART 2000 mstorti@node1.beowulf.gtm
%
Tambien les.epl da bien ahora. Vamos a medir la performance ahora. 

%===Tue Nov 14 09:21:50 ART 2000 mstorti@node1.beowulf.gtm
%
Con `cache_grad_div_u', linear addressing en FM2 y
EFFICIENT_FM2_VERSION da un 33% mas rapido, es decir 4.1sec/Ke contra
6.2sec/Ke. 
%
Ahora no me vuelve a coincidir con la version 2.16!! A revisar...

%===Tue Nov 14 10:12:47 ART 2000 mstorti@node1.beowulf.gtm
%
les.epl no coincide pero `back_step_3d' si. Asumo que esta bien y
genero una nueva version estable. 

%===Tue Nov 14 11:05:58 ART 2000 mstorti@node1.beowulf.gtm
%
Generado un test para el bug en la opcion linear de
FastMat2::prod(). Ademas hay una doble proteccion ya que ahora
inicializo inca,incb a 1.

%===Tue Dec  5 17:40:10 ART 2000 mstorti@node1.beowulf.gtm
%
BUG: en ffswfm2.cpp (funcion de flujo para shallow water). La matrix
flux_mass estaba inicializada como `static FastMat2 flux_mass(u)' lo
cual no tiene demasiado sentido y originaba que el almacenamiento de
flux_mass era el mismo que el de u. Podria ser una idea para generar
mascaras? 

%===Tue Dec  5 18:58:12 ART 2000 mstorti@node1.beowulf.gtm
%
back_step_3d: Aparentemente ahora el programa corre, despues de haber
booteado el nodo6. Guarda que quedo la version upgradeada o sea que
ahora esta corriendo una version mas nueva. Los residuos parecen ser
un poco diferentes. 
%
BUG: en ffswfm2.cpp ponia `start_chunk=1' despues de la
inicializacion. Mientras que deberia ponerlo a 0 para que no vuelva a
entrar. 
%
advdif: empiezo a escribir un `ffadvfm2.cpp' con flujos para adveccion
difusion lineal. 

%===Sat Dec  9 19:06:02 ARST 2000 mstorti@localhost.localdomain
%
Anda el problema de difusion lineal. Ahora voy a probar con algo de
adveccion.

%===Tue Dec 12 19:26:29 ARST 2000 mstorti@localhost.localdomain
%
Dejo lo de la difusion lineal por ahora. Voy a trabajar en hacer el
jacobiano analitico. 

%===Sat Dec 16 19:45:21 ARST 2000 mstorti@localhost.localdomain
%
Anda bien el jacobiano de la parte advectiva pura. Ahora voy a ver el
del bcconv (bccadvfm2.cpp). 

%===Sat Dec 16 20:05:02 ARST 2000 mstorti@localhost.localdomain
%
BUG en `advective': en `bccadvfm2.cpp' hay que pesar con `wpgdet' no
con WPG?? 
%
NOOOOO!! En los bccadvfm2, el vector `normal' incluye el jacobiano de
la trnasformacion (de superficie en superficie) asi que solo hace
falta multiplicar por el peso de gauss (WPG). 
%
Ahora voy a hacer la version no debilitada.

%===Sat Dec 16 20:50:09 ARST 2000 mstorti@localhost.localdomain
%
OJO! Hay que probar si anda con Jacobianos no simetricos...

%===Sat Dec 16 21:28:50 ARST 2000 mstorti@localhost.localdomain
%
Anda bien la version no-debilitada. 

%===Sun Dec 17 09:27:31 ARST 2000 mstorti@localhost.localdomain
%
Probar con la ec. de Burgers.
Probar con alpha\neq 1

%===Sun Dec 17 20:13:25 ARST 2000 mstorti@localhost.localdomain
%
Voy a llevar la ec. de Burgers a 2D como f = 0.5*u0*phi^2 donde u0 es
un vector constante que se entra.

%===Sat Dec 23 03:52:00 ARST 2000 mstorti@localhost.localdomain
%
Parece andar bien la ec. de Burgers 2D.

%===Sat Dec 23 03:59:14 ARST 2000 mstorti@localhost.localdomain
%
Vamos a verificar ahora que ande bien $\alpha \neq 1$

%===Sat Dec 23 10:55:38 ARST 2000 mstorti@localhost.localdomain
%
Estaba de mas el ALPHA en una contribucion al residuo:
! 	veccontr.axpy(tmp8,wpgdet*ALPHA);
cambiar por
! 	veccontr.axpy(tmp8,wpgdet);
Ahora converge bien la ecuacion del calor (u=0), es decir converge en
una sola iteracion pero el problema global no converge. 

%===Sat Dec 23 11:33:22 ARST 2000 mstorti@localhost.localdomain
%
Esta todo bien, solo era que el Crank-Nicholson puede no converger
para llegar a una solucion estacionaria si se usa un paso de tiempo
muy grande. Con paso de tiempo 0.8 anda todo bien. 
%
Ahora voy a probar que la convergencia de Newton-Raphson es
cuadratica.

%===Sat Dec 23 12:34:32 ARST 2000 mstorti@localhost.localdomain
%
No parece dar convergencia cuadratica.

%===Sat Dec 23 18:55:05 ARST 2000 mstorti@localhost.localdomain
%
Si bien todavia hay un drama con el tema de que se debrian pasar
argumentos que son vectores+tiempos, de todas formas por lo menos para
hacer que el Crank-Nicholson de convergencia cuadratica hay que pasar
como tiempo en todas las evalucaiones del residuo el tiempo
$t^*=t+\alpha\Dt$. Ahora si da convergencia cuadratica. 

%===Sat Dec 23 18:56:54 ARST 2000 mstorti@localhost.localdomain
%
vamos a probar si todas estas cosas dan bien con Burgers ahora.

%===Mon Dec 25 18:04:03 ARST 2000 mstorti@localhost.localdomain
%
Pude construir la documentacion para `glib' y entonces ahora pude
poner funciones de hash y de comparacion apropiadas para texthash. 

%===Thu Dec 28 13:08:39 ARST 2000 mstorti@minerva
%
Porting to gcc 2.96-54. There were several syntax errors. 
%
* `export' is a keyword
* Templates for functions can't have default values
* Pointers to functions can't be declared as `const'
* No te deja poner `template<class T> class T random_pop(set<...'
     tiene que ser `template<class T> T random_pop(set<' 
* Para declarar que el puntero es constante:  int get_line(char * const& line );
* Ahora PETSc PCSetType() no anda asi nomas ya que el segundo
       arugumento es char * y si le pasamos un string.c_str() el valor
       de retorno es const char *. Lo tuve qu copiar a una string de C
       clasico. 
* En la compilacion de mpi tuve que cambiar lo siguiente en
mpe/slog_api/src/slog_irec_write.c, linea 1171

/* Antes */
/*          dest_node_id      = va_arg( ap, SLOG_nodeID_t ); */ 
/*          dest_cpu_id       = va_arg( ap, SLOG_cpuID_t ); */
/*          dest_thread_id    = va_arg( ap, SLOG_threadID_t ); */
/* despues */
        dest_node_id      = va_arg( ap, int );
        dest_cpu_id       = va_arg( ap, int );
        dest_thread_id    = va_arg( ap, int );

El mensaje de error lo sugeria.

%===Fri Dec 29 11:46:08 ARST 2000 mstorti@minerva
%
En ElementIterator::begin() tuve que avanzar el iterator hasta el
primer elemento valido. Sino, te daba un error ya que incluia al
primer elemento qu podia no estar en el chunk. Ahora hay que modificar
el iterator para los otros modos de iteracion (jacobianos numericos). 

%===Fri Dec 29 11:59:30 ARST 2000 mstorti@minerva
%
Esta complicado convertir todo elemset con iterators ya que
`compute_this_elem' es usado muchas veces. 

%===Fri Dec 29 12:20:38 ARST 2000 mstorti@minerva
%
Lo que voy a hacer es que en el elemento se llama al iterator y
'is_valid()' llama en realidad a `compute_this_elem' de manera que se
asegura una cierta compatibilidad. 

%===Fri Dec 29 13:21:33 ARST 2000 mstorti@minerva
%
Guarda, sigue habiendo un quilombo con el ElementIterator con ghost
elems. 

%===Fri Dec 29 19:40:56 ARST 2000 mstorti@localhost.localdomain
%
No puedo hacer que recicle imagenes el latex2html. 

%===Sun Dec 31 10:05:58 ARST 2000 mstorti@spider
%
Retomo sine.epl. Adveccion difusion en un medio semiinfinito periodico
en la direccion `y'. La condicion en $x=0$ es $\phi = \cos (2\pi x)
\sin(\omega t)$
%
Da resultados mas o menos buenos, pero curiosamente no da convergencia
en una iteracion del problema no-lineal con LU. 
%
La convergencia inicial esta bien (baja el residuo a 1e-14 en una
iteracion) pero poco a poco se empieza a deteriorar. 

%===Sun Dec 31 10:22:24 ARST 2000 mstorti@spider
%
No parece estar relacionado con las condiciones variables en el
tiempo. Poniendo un escalon en el tiempo tampoco converge bien. 

%===Sun Dec 31 12:56:05 ARST 2000 mstorti@spider
%
El problema no esta en el elemento bcconv. Esta directamente en el
'advdife' y aparece para u\neq 0. 
%
con tau_fax=0 da mal tambien. 

%===Sun Dec 31 13:02:53 ARST 2000 mstorti@spider
%
Parece que era la famosa historia de como contraer el A_jac. 

%===Mon Jan  1 21:33:37 ARST 2001 mstorti@spider
%
Ahora da la convergencia todo bien. Crank-Nicholson converge
cuadraticamente y implicito puro converge lineal. Voy a hacer un caso
test. 

%===Thu Jan  4 17:49:26 ARST 2001 mstorti@minerva
%
TAG: beta-1.3

%===Fri Jan  5 20:28:18 ARST 2001 mstorti@minerva
%
TAG: beta-1.4
TAG: beta-1.5
TAG: beta-1.6

%===Sat Jan  6 11:11:17 ARST 2001 mstorti@spider
%
Haciendo upgrade a RH7.0 en spider.
%
Compilando MPI, PETSC, ANN, meschach: sin problema. 
%
Compilando Newmat: parche par newmat1.cpp

========cut here
diff -c /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp.orig /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp
*** /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp.orig	Sun Sep  7 04:38:35 1997
--- /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp	Sat Jan  6 11:18:05 2001
***************
*** 87,95 ****
     case Valid+Band+Lower:                   return "LwBnd";
     default:
        if (!(attribute & Valid))             return "UnSp ";
!       if (attribute & LUDeco)
!          return (attribute & Band) ?     "BndLU" : "Crout";
!                                             return "?????";
     }
  }
  
--- 87,102 ----
     case Valid+Band+Lower:                   return "LwBnd";
     default:
        if (!(attribute & Valid))             return "UnSp ";
! //        if (attribute & LUDeco)
! //           return (
!       if (attribute & LUDeco) {
! 	if (attribute & Band) {
! 	  return "BndLU";
! 	} else {
! 	  return "Crout";
! 	}
!       }
!       return "?????";
     }
  }
========cut here


%===Sat Jan  6 11:24:35 ARST 2001 mstorti@spider
%
Patch para meschach: matrix.h

========cut here
diff -c /home/mstorti/SOFT/meschach-1.2/matrix.h.orig /home/mstorti/SOFT/meschach-1.2/matrix.h
*** /home/mstorti/SOFT/meschach-1.2/matrix.h.orig	Mon Nov 11 06:20:57 1996
--- /home/mstorti/SOFT/meschach-1.2/matrix.h	Sat Jan  6 11:23:51 2001
***************
*** 178,184 ****
  /* free (de-allocate) (band) matrices, vectors, permutations and 
     integer vectors */
  extern  int iv_free(IVEC *);
! extern	m_free(MAT *),v_free(VEC *),px_free(PERM *);
  extern   int bd_free(BAND *);
  
  #endif
--- 178,184 ----
  /* free (de-allocate) (band) matrices, vectors, permutations and 
     integer vectors */
  extern  int iv_free(IVEC *);
! extern	int m_free(MAT *),v_free(VEC *),px_free(PERM *);
  extern   int bd_free(BAND *);
  
  #endif
========cut here

%===Sat Jan  6 18:58:10 ARST 2001 mstorti@spider
%
Tratando de hacer que el conseguir las coordenadas y H del elemento
sea mas `OOP'. Creamos dos funciones 
%
  void element_node_data(const ElementIterator &element,
			 double *xloc,double *Hloc);
%  
  void element_connect(const ElementIterator &element,
		       int *connect);
%
que recuperan las conectividades y valores nodales de un dado
elemento. 

%===Sun Jan  7 13:21:21 ARST 2001 mstorti@spider
%
Para que ande bien el `make depend' los SRCS en los makefile deben ser
definidos antes del `include ....Makefile.base'. 
%
Escribo 
  const double *
  element_vector_values(const ElementIterator &element,
			arg_data &ad) const;

%===Tue Jan  9 21:48:25 ARST 2001 mstorti@spider
%
Hago que la funcion de flujo en advdif sea ahora una `function object'
es decir una clase con `operator()' sobrecargado. La estructura es que
`AdvDif' tiene un puntero a una clase pure virtual `AdvDifFF' y
sobrecargando `operator()' de esta clase logras cambiar la funcion de
flujo. Ya anda para la clase `advdif_advecfm2' ahora hay que hacerlo
para `bcconv_adv_advecfm2'. 

%===Wed Jan 10 08:38:25 ARST 2001 mstorti@minerva
%
El comando para crear el patch es 
%
   $ diff -cNr DIROLD DIRNEW
%
y para patchear te pones en el directorio a patchear y
%
   $ patch -E -p1 < ../petscfem.patch

%===Wed Jan 10 12:48:37 ARST 2001 mstorti@minerva
%
Ahora voy a correr burgers. 

%===Wed Jan 10 13:55:21 ARST 2001 mstorti@minerva
%
Anda burgers. Ahora voy a ver si puedo transformar a NS. 

%===Sat Jan 13 21:45:30 ARST 2001 mstorti@spider
%
Para correr las viejas versiones de PETSc-FEM. Usar el compilador
egcs. Reemplazar en petsc/bmake/linux/base_variables `g++' por
$(GNUCC). Despues en Makefile.base se define `GNUCC = egcs++'. Des
esta forma compila pero despues hay errores de linkedicion. 
%

%===Sun Jan 14 15:13:06 ARST 2001 mstorti@spider
%
Puedo compilar versiones viejas con egcs!! 
%
Instrucciones:
%
* Cambiar el compilador C++ en petsc/bmake/linux/base_variables de g++
  a egcs++ (hay que tener los paquetes compat-egcs y compat-egcs++
  instalados). 
* Agregar '-static' a las banderas de linkedicion
* Agregar '-L/usr/lib/gcc-lib/i386-redhat-linux/2.96/' a las librerias
%
Para poder seguir compilando con gcc tambien, conviene agregar
las siguientes variables en Makefile.base 
%-------- para compilar con egcs++
GNUCXX = egcs++
OCXX_COPTFLAGS_USR = -static
CXX_SYS_LIB_USR = -L/usr/lib/gcc-lib/i386-redhat-linux/2.96/ 
%--------pra compilar con g++
GNUCXX = g++
OCXX_COPTFLAGS_USR = 
CXX_SYS_LIB_USR = 
%-------------
%
y modificar un poco el base_variables con el siguiente parche:
%-----------ESTE PARCHE ES CON RESPECTO AL BASE_VARIABLES ORIGINAL
%---------- CUT HERE
cd ~/PETSC/petsc-2.0.24/bmake/linux/
diff -c /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables.bck /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables
*** /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables.bck	Wed Mar 31 15:38:47 1999
--- /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables	Sun Jan 14 14:51:45 2001
***************
*** 1,4 ****
! # $Id: notes.txt,v 1.350 2003/10/17 22:52:36 mstorti Exp $ 
  #
  #     See the file bmake/base_variables.defs for a complete explanation of all these fields
  #
--- 1,5 ----
! # -*- mode: makefile -*-
! # $Id: notes.txt,v 1.350 2003/10/17 22:52:36 mstorti Exp $ 
  #
  #     See the file bmake/base_variables.defs for a complete explanation of all these fields
  #
***************
*** 7,41 ****
  OMAKE            = make  --no-print-directory
  RANLIB           = ranlib
  SHELL            = /bin/sh
! SH_LD            = gcc
  # ######################### C and Fortran compiler ########################
  #
! C_CC             = gcc -fPIC
  C_FC             = g77 -Wno-globals
! C_CLINKER        = gcc ${COPTFLAGS} -rdynamic -Wl,-rpath,${LDIR}:${DYLIBPATH}
  C_FLINKER        = g77 ${FOPTFLAGS} -rdynamic -Wl,-rpath,${LDIR}:${DYLIBPATH}
  C_CCV            = ${C_CC} --version
  C_SYS_LIB        = -ldl -lc -lg2c -lm
  # ---------------------------- BOPT=g options ----------------------------
! G_COPTFLAGS      = -g 
  G_FOPTFLAGS      = -g
  # ----------------------------- BOPT=O options -----------------------------
  O_COPTFLAGS      = -O -Wall -Wshadow  -fomit-frame-pointer
  O_FOPTFLAGS      = -O
  # ########################## C++ compiler ##################################
  #
! CXX_CC           = g++ -fPIC
  CXX_FC           = g77 -Wno-globals
! CXX_CLINKER      = g++ ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
! CXX_FLINKER      = g++ ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
  CXX_CCV          = ${CXX_CC} --version
! CXX_SYS_LIB      = -ldl -lc -lg2c -lm
  # ------------------------- BOPT=g_c++ options ------------------------------
! GCXX_COPTFLAGS   = -g 
  GCXX_FOPTFLAGS   = -g
  # ------------------------- BOPT=O_c++ options ------------------------------
! OCXX_COPTFLAGS   = -O 
! OCXX_FOPTFLAGS   = -O
  # -------------------------- BOPT=g_complex options ------------------------
  GCOMP_COPTFLAGS  = -g
  GCOMP_FOPTFLAGS  = -g
--- 8,43 ----
  OMAKE            = make  --no-print-directory
  RANLIB           = ranlib
  SHELL            = /bin/sh
! SH_LD            = egcs
  # ######################### C and Fortran compiler ########################
  #
! C_CC             = egcs -fPIC
  C_FC             = g77 -Wno-globals
! C_CLINKER        = egcs ${COPTFLAGS} -rdynamic -Wl,-rpath,${LDIR}:${DYLIBPATH}
  C_FLINKER        = g77 ${FOPTFLAGS} -rdynamic -Wl,-rpath,${LDIR}:${DYLIBPATH}
  C_CCV            = ${C_CC} --version
  C_SYS_LIB        = -ldl -lc -lg2c -lm
+ #C_SYS_LIB        = -ldl -lc -lf2c -lm
  # ---------------------------- BOPT=g options ----------------------------
! G_COPTFLAGS      = -g
  G_FOPTFLAGS      = -g
  # ----------------------------- BOPT=O options -----------------------------
  O_COPTFLAGS      = -O -Wall -Wshadow  -fomit-frame-pointer
  O_FOPTFLAGS      = -O
  # ########################## C++ compiler ##################################
  #
! CXX_CC           = $(GNUCXX) -fPIC
  CXX_FC           = g77 -Wno-globals
! CXX_CLINKER      = $(GNUCXX) ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
! CXX_FLINKER      = $(GNUCXX) ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
  CXX_CCV          = ${CXX_CC} --version
! CXX_SYS_LIB      = $(CXX_SYS_LIB_USR) -ldl -lc -lg2c -lm
  # ------------------------- BOPT=g_c++ options ------------------------------
! GCXX_COPTFLAGS   = -gstabs+
  GCXX_FOPTFLAGS   = -g
  # ------------------------- BOPT=O_c++ options ------------------------------
! OCXX_COPTFLAGS   = -O2 $(OCXX_COPTFLAGS_USR)
! OCXX_FOPTFLAGS   = -O2
  # -------------------------- BOPT=g_complex options ------------------------
  GCOMP_COPTFLAGS  = -g
  GCOMP_FOPTFLAGS  = -g

Diff finished at Sun Jan 14 15:14:01
%---------- CUT HERE
%
%---------- ESTE ES CON RESPECTO AL ULTIMO (YA CONFIGURADO)
%---------- CUT HERE
diff -c /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables.bck2 /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables
*** /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables.bck2	Fri Dec  8 15:19:17 2000
--- /home/mstorti/PETSC/petsc-2.0.24/bmake/linux/base_variables	Sun Jan 14 14:51:45 2001
***************
*** 1,3 ****
--- 1,4 ----
+ # -*- mode: makefile -*-
  # $Id: notes.txt,v 1.350 2003/10/17 22:52:36 mstorti Exp $ 
  #
  #     See the file bmake/base_variables.defs for a complete explanation of all these fields
***************
*** 25,42 ****
  O_FOPTFLAGS      = -O
  # ########################## C++ compiler ##################################
  #
! CXX_CC           = g++ -fPIC
  CXX_FC           = g77 -Wno-globals
! CXX_CLINKER      = g++ ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
! CXX_FLINKER      = g++ ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
  CXX_CCV          = ${CXX_CC} --version
! CXX_SYS_LIB      = -ldl -lc -lg2c -lm
! #CXX_SYS_LIB      = -ldl -lc -lf2c -lm
  # ------------------------- BOPT=g_c++ options ------------------------------
  GCXX_COPTFLAGS   = -gstabs+
  GCXX_FOPTFLAGS   = -g
  # ------------------------- BOPT=O_c++ options ------------------------------
! OCXX_COPTFLAGS   = -O2 
  OCXX_FOPTFLAGS   = -O2
  # -------------------------- BOPT=g_complex options ------------------------
  GCOMP_COPTFLAGS  = -g
--- 26,42 ----
  O_FOPTFLAGS      = -O
  # ########################## C++ compiler ##################################
  #
! CXX_CC           = $(GNUCXX) -fPIC
  CXX_FC           = g77 -Wno-globals
! CXX_CLINKER      = $(GNUCXX) ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
! CXX_FLINKER      = $(GNUCXX) ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
  CXX_CCV          = ${CXX_CC} --version
! CXX_SYS_LIB      = $(CXX_SYS_LIB_USR) -ldl -lc -lg2c -lm
  # ------------------------- BOPT=g_c++ options ------------------------------
  GCXX_COPTFLAGS   = -gstabs+
  GCXX_FOPTFLAGS   = -g
  # ------------------------- BOPT=O_c++ options ------------------------------
! OCXX_COPTFLAGS   = -O2 $(OCXX_COPTFLAGS_USR)
  OCXX_FOPTFLAGS   = -O2
  # -------------------------- BOPT=g_complex options ------------------------
  GCOMP_COPTFLAGS  = -g

Diff finished at Sun Jan 14 15:22:21
%---------- CUT HERE
%

%===Sun Jan 14 18:15:34 ARST 2001 mstorti@spider
%
Todo esto tiene el problema que despues no corre el programa. Entonces
la solucion es compilar con egcs++ pero linkeditar con g++. Para esto
%
* Tocar base_variables de manera de volver a que el linkeditor sea
   g++:
%
CXX_CLINKER      = g++ ${COPTFLAGS} -Wl,-rpath,${LDIR}:${DYLIBPATH}
%
* En tiempo de corrida no encuentra libpetscles.so, entonces hay que
agregar los directorios:
%
10	/home/mstorti/PETSC/petsc-2.0.24/lib/libO_c++/linux/
11	/home/mstorti/PETSC/petsc-2.0.24/lib/libg_c++/linux/
12	/home/mstorti/PETSC/petsc-2.0.24/lib/libg/linux/
%
a /etc/ld.so.conf y correr `ldconfig -v'.

%===Sun Jan 14 18:59:54 ARST 2001 mstorti@spider
%
Corro el sqcav5 en spider con las versiones beta-1.12 y 2.8s. Dan
igual a precision de la maquina a Re=400. 

%===Mon Jan 15 12:33:56 ARST 2001 mstorti@minerva
%
Correiendo en geronimo da diferente la version beta-1.13 con la
2.16g. En la salida dan
%
------------------ VERSION beta-1.13
Time step: 1, time: 0.005
Newton subiter 0, norm_res  =  8.687e-07, update Jac. 1
Newton subiter 1, norm_res  =  1.967e-07, update Jac. 1
============= delta_u =  1.636e+00
iter: 0, saving on rec 0, file outvector0.out
Writing vector to file "outvector0.out"
Time step: 2, time: 0.01
Newton subiter 0, norm_res  =  7.485e-07, update Jac. 1
Newton subiter 1, norm_res  =  1.915e-07, update Jac. 1
============= delta_u =  1.648e+00
------------------ VERSION 2.16g
Time step: 1, time: 0.005
|| R || =  2.703e-06
|| R || =  1.321e-07
============= delta_u =  2.002e+00
iter: 0, saving on rec 0, file outvector0.sal
Writing vector to file "outvector0.sal"
Time step: 2, time: 0.01
|| R || =  1.963e-06
|| R || =  7.997e-08
============= delta_u =  1.708e+00
------------------
%
y la diferencia entre los vectores de estado en Octave da
%
octave> un=aload("back_step_3d.state.new");
octave> un=aload("back_step_3d.state.new");
octave> merr(uo-un)
ans = 0.055116
octave> 
%
Ahora voy a probar con sqcav5 a ver si sigue dando igual (como daba en
casa).

%===Mon Jan 15 12:46:09 ARST 2001 mstorti@minerva
%
Para el sqcav siguen dando iguales. 

%===Mon Jan 15 12:51:28 ARST 2001 mstorti@minerva
%
Con Jacobi tambien sqcav da igual. 
Con 2 procesadores tambien da lo mismo. 
Con weak_form 1 tambien da lo mismo.
Con C_smag 0 da lo mismo.
Con LES 0 da lo mismo.
Con temporal_stability_factor 0. da lo mismo.
Con Re=40000 da lo mismo
Con inicializacion da lo mismo.
Con nnwt=2 da lo mismo.
Con maxits=10 da lo mismo.
Copiando el archivo de datos de back_step_3d y dejando todas las
    opciones de ese caso da lo mismo.
Si el paso de tiempo es muy chico hay que poner
   `temporal_stability_factor 0' para que de lo mismo. 

%===Tue Jan 16 13:21:59 ARST 2001 mstorti@minerva
%
Para que de igual hay que poner `temporal_stability_factor 1'!!

%===Tue Jan 16 19:10:12 ARST 2001 mstorti@minerva
%
Voy a poner en NS la posibilidad de tener estados "filtrados". 

%===Wed Jan 17 18:13:20 ARST 2001 mstorti@minerva
%
Escribi una clase simple de filtros. Anda, pero ahora voy a poner el
mixer que puede tener varias entradas. 

%===Fri Jan 19 10:45:13 ARST 2001 mstorti@minerva
%
Viendo que pasa con el cache_grad_div_u: La version beta--1-13
(brancheada a cache-gdu) con esto activado no converge. El elemento
nsi_tet_les_fm2 da muy parecido al nsi_tet (si no activamos el
cache_grad_div_u). 

%===Fri Jan 19 10:50:43 ARST 2001 mstorti@minerva
%
Hay dos posibilidades: que no guarde bien el operador grad_div_u en el
cache, o que no este bien el calculando grad_div_u y despues agregando
la contribucion al residuo y a la matrix al final. 

%===Fri Jan 19 11:18:42 ARST 2001 mstorti@minerva
%
Agregando una linea de forma que calcule cada vez el grad_div_u parece
que anda bien (da una pequena diferencia, a mi me parece que deberia
dar lo mismo a precision de la maquina). 

%===Fri Jan 19 12:15:11 ARST 2001 mstorti@minerva
%
Ya esta!! El problema es con los chunks... Habria que usar la posicion
del elemento dentro del procesador, pero no dentro del chunk. 

%===Fri Jan 19 13:03:44 ARST 2001 mstorti@minerva
%
Ahora a local_store_address(int global_elem) hay que pasarle el numero
global del elemento. 
%
Encontre un error en `elemset.cpp': tomaba `report_consumed_time' de
la thash global, no de la del elemento. 
%
Cambie el chunk_size default a 1000. 

%===Fri Jan 19 23:29:22 ARST 2001 mstorti@spider
%
Mergeadas las versiones `cache-gdu-fixed' y 'beta-1.15' en la version
`beta-1.16'. 
%
Ahora el problema es que dan SIGSEGV las corridas `plano...' y
contraint_bug, o sea en todas las que el programa es adv. 

%===Sat Jan 20 07:32:20 ARST 2001 mstorti@spider
%
PARA MERGEAR BRANCHES:
%
* Para crear branches: $ cvs rtag -b -r OLDTAG NEWBRANCH petscfem
* Cada vez que se salva en ese branch: $ cvs rtag -b -r NEWBRANCH NEWBRANCH2 petscfem
* Para mergear un branch con un directorio de trabajo $ cvs up -kk -j BRANCH <directory> 
* Si hay problemas de conflictos correr en Emacs: M-x vc-resolve-conflicts
* Hacer comit del working directory. Si da problemas con 'sticky tags' 
        correr $ cvs up -A .
* Hacer un nuevo tag

%===Sat Jan 20 09:10:57 ARST 2001 mstorti@spider
%
Resuelto el problema!! Era que Newmat no compila bien en spider con
g++. Lo compile con egcs++ y a partir de ahi anduvo bien. Sera el
parche ese que tuve que hacer en newmat1.cpp??? 
%
========cut here
diff -c /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp.orig /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp
*** /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp.orig	Sun Sep  7 04:38:35 1997
--- /home/mstorti/SOFT/NEWMAT/src/newmat1.cpp	Sat Jan  6 11:18:05 2001
***************
*** 87,95 ****
     case Valid+Band+Lower:                   return "LwBnd";
     default:
        if (!(attribute & Valid))             return "UnSp ";
!       if (attribute & LUDeco)
!          return (attribute & Band) ?     "BndLU" : "Crout";
!                                             return "?????";
     }
  }
  
--- 87,102 ----
     case Valid+Band+Lower:                   return "LwBnd";
     default:
        if (!(attribute & Valid))             return "UnSp ";
! //        if (attribute & LUDeco)
! //           return (
!       if (attribute & LUDeco) {
! 	if (attribute & Band) {
! 	  return "BndLU";
! 	} else {
! 	  return "Crout";
! 	}
!       }
!       return "?????";
     }
  }
========cut here

%===Sat Jan 20 23:47:06 ARST 2001 mstorti@spider
%
Volviendo a trabajar con filtros. Logro hacerlos andar muy basicamente
en ns.cpp. 

%===Mon Jan 22 18:02:32 ARST 2001 mstorti@minerva
%
Hay una incongruencia en la forma en que se tratan los filtros. Solos
se filtra el estado y si hay un nodo fijado a un valor dependiente del
tiempo entonces se pone el valor inmediato del nodos lo cual es
inconsistente.

%===Tue Jan 23 09:20:29 ARST 2001 mstorti@minerva
%
Andan los filtros. Falta hacer lo siguiente
* Que filtros impriman delta_u
* Documentar filtro. OK
* salvar version filtro
* Configurar mejor en ns.cpp. Que pueda leer nombre de archivo para
     guardar. 

%===Tue Jan 23 10:29:42 ARST 2001 mstorti@minerva
%
Voy a agregar las cosas de Beto (shallow water turbulento). Hago el
siguiente merge

version al 5 Jan 12:00  ---->  petscfem.beto
                    +------->  petscfem.actual

%===Wed Jan 24 19:14:08 ARST 2001 mstorti@minerva
%
Echo el merge con turbulento (version beta-1.23). Pero advdif no
linkedita en minerva!! No se que era, el sistema estaba inestable,
sali de X y a partir de ahi empezo a andar todo. Tampoco compilaba
algunos de los .cpp (daba internal error, SIGSEGV etc...) .

%===Sun Feb  4 22:47:13 ARST 2001 mstorti@spider
%
Agregue documentacion de opciones ('odoc.pl') a las opciones de
shallow water, en ffswfm2t.cpp. 

%===Wed Feb  7 12:04:50 ARST 2001 mstorti@minerva
%
En shallow water vamos a agregar un "bottom_slope" (equivalente al
"G_body" en NS) que es la friccion con el fondo evitando de tener que
definir la forma del fondo. 

%===Fri Feb  9 10:40:01 ARST 2001 mstorti@minerva
%
Se obtiene un perfil de velocidades con Ly=10, N=100, Dt=2, wall_coef=
v_wall/v_(y\to\infty)=0.9. La idea es ahora refinar la malla hacia las
paredes e ir bajando la velocidad en la pared a 0.

%===Fri Feb  9 10:57:26 ARST 2001 mstorti@minerva
%
Refinando por 3 cerca de la pared con los mismos parametros da bien. 
Bajando el factor de pared a 0.5 deja de converger. 

%===Fri Feb  9 11:01:53 ARST 2001 mstorti@minerva
%
No lo haces andar ni con factor de relajacion 0.1 (pero guarda que hay
que aumentar los nnwt). 

%===Fri Feb  9 11:11:51 ARST 2001 mstorti@minerva
%
Aumentando nnwt a 10 tampoco anda. Aumentano el Dt tampoco
mejora. Sera por inestabilidad temporal? Usando un paso de tiempo
suf. chico deberia andar pero tambien se puede armar quilombo con las
oscilaciones tipo "termino de reaccion" [(SU+C)PG.]

%===Fri Feb  9 11:45:34 ARST 2001 mstorti@minerva
%
Vamos a ver si se puede avanzar algo por continuacion. 

%===Fri Feb  9 11:56:33 ARST 2001 mstorti@minerva
%
Con continuacion se puede pasar de 0.9 -> 0.8 -> 0.7 -> 0.6
tranquilo. Ya no se puede pasar de 0.6 a 0.5.

%===Sat Feb 10 11:46:31 ARST 2001 mstorti@spider
%
Anda bien matriz lumped en ecuacion del calor.

%===Sun Feb 11 09:12:48 ARST 2001 mstorti@spider
%
Andan bien las variables logaritmicas para la ec. del calor.

%===Sun Feb 11 21:29:03 ARST 2001 mstorti@spider
%
Andan log-vars en shallow water y en general.
%
Ahora voy a documentarlo. 

%===Thu Mar  1 13:54:19 ARST 2001 mstorti@spider
%
In TextHashTable: Print a warning if some option is not used and a doc
when an option was got the first time.

%===Thu Mar  1 16:23:54 ARST 2001 mstorti@spider
%
Escribo TextHashTable::read, eventualmente va a reemplazar a
read_hash_table(). 

%===Fri Mar  2 20:00:34 ARST 2001 mstorti@spider
%
Agregado el conteo de veces que es accedida una opcion en
TextHashTable's. 

%===Sat Mar  3 05:05:03 ARST 2001 mstorti@spider
%
Doc++ no da segmentation fault cuando pongo
%
//---:---<*>---:---<*>---:---<*>---:---<*>---:---<*>---:---<*>---: 
/** Makes a temporary copy of a string.
    @author M. Storti
    @param cstr (input) the string to be copied
    @return a pointer to the copied string
*/ 
char * local_copy(const char * cstr);
%
Pero si lo da si cambio el parametro 'cstr' a 's'. Curioso no?

%===Sat Mar  3 07:17:29 ARST 2001 mstorti@spider
%
Agregue tests para variables logaritmicas en ecuacion del calor y sw turb.

%===Sat Mar  3 12:46:28 ARST 2001 mstorti@spider
%
Agregado elemset ns+termico (nsi_tet_les_ther / Beto). 

%===Sat Mar 10 11:30:50 ART 2001 mstorti@spider
%
Creado un parser (con bison) para leer la linea 'props'.

%===Fri Mar 16 19:07:03 ART 2001 mstorti@spider
%
Andan (no lo chequee demasiado) las propiedades. get_prop etc... Ahora
voy a hacer mejor las funciones de flujo... 

%===Sat Mar 17 19:39:15 ART 2001 mstorti@spider
%
Agrego una mascara de diag a FM2. Funciona poniendo A.d(j1,j2) y
quiere decir que a partir de entonces el indice j2 copia al j1. O sea
que si A es de 3x5x5 entonces si hacemos A.d(3,2) entonces cuando
accedemos a A(1,2) estamos accediendo a A(1,2,2).
%
Esto se hace seteando la variable set_indx[] de la dimension j2 a
-j1. Las modificaciones a fastmat2 fueron pequenhas. Corri el test
testfm2 y anda bien, ahora voy a correr todos los otros tests. 

%===Sat Mar 17 19:54:02 ART 2001 mstorti@spider
%
Hace unos dias descubrimos un error en 'ns' que la opcion
'shock_capturing_factor' no estaba puesta por default a 0. 

%===Fri Mar 23 09:58:42 ART 2001 mstorti@minerva
%
Agrego jacobianos difusivos, convectivos y reactivos en forma OOP. 
Ahora voy a corregir el bug descubierto por Beto en 'mydetsur'. 

%===Fri Mar 30 17:01:47 ART 2001 mstorti@spider
%
Hay un bug en elnuevo advdif. Cuando corro el test 'full_full_jacs_t'
con mas elementos en la direccion y no anda. 
%
Descubri un error que no leia correctamente el tau_fac, es decir no
habia hecho el EGETOPTDEF. 

%===Fri Mar 30 17:24:48 ART 2001 mstorti@spider
%
Otro bug: 'ret_options' no estaba pasado por referencia en start_chunk
de manera que no era modificado desde la funcion de flujo. 
%
otro BUG: el 'ndim' de la funcion de flujo estaba definido con
EGETOPTDEF y no con EGETOPTDEF_ND por lo cual no quedaba definido y
quedaba como cero. 

%===Mon Apr  2 17:23:25 ART 2001 mstorti@spider
%
Voy a agregar una entalpia generalizada para advdif. 

%===Fri Apr  6 14:20:05 ART 2001 mstorti@minerva
%
Para compilar con optimizacion maxima
%
> # ------------------------- BOPT=O_c++ options ------------------------------
> OCXX_COPTFLAGS   = -O2 -funroll-loops
> OCXX_FOPTFLAGS   = -O2 -funroll-loops
%
en base_variables.

%===Fri Apr  6 15:49:16 ART 2001 mstorti@minerva
%
Habia un bug en 'newff.m' el archivo que controla los tests en
'test/newff'. La expresion
     uana = (beta \ (1-exp(-beta*nstep*Dt)))*(CP\S);
hay que cambiarla por
     uana = (beta \ (eye(ndof)-expm(-beta*nstep*Dt)))*(CP\S);

%===Sat Apr  7 19:11:54 ART 2001 mstorti@spider
%
Agregadas las funciones de entalpia. Ahora falta el termino SUPG. 

%===Tue May  1 09:40:20 ART 2001 mstorti@spider
%
Estuve trabajando en modificar completamente la clase 'nodedata',
'elemset', etc... eso fue hasta la version beta-1.69 ahora vuelvo a la
beta-1.67. 

%===Tue May  1 10:01:27 ART 2001 mstorti@spider
%
En 'onedstr' hay que reemplazar por
  if h(2)>max(h([1 3]))

%===Tue May  1 19:50:49 ART 2001 mstorti@spider
%
Added `report_option_access' to ns module. 
Added a `steady' option. If set to one, then it is equivalent 
to have Dt=inf, but Dt is used to report time instants, etc... 

%===Tue May  1 19:52:40 ART 2001 mstorti@spider
%
Esta version fue vuelta atras a la beta-1.67, dejando de lado por
ahora todos los 'avances' en cuanto a nueva interfase. Campos de
entrada salida, etc... 
%
Corre bien todos los test 'oscplate_all'. 

%===Thu May  3 18:33:27 ART 2001 mstorti@minerva
%
Para modificar el copyright hay que tocar:
doc/license.txt, src/version.cppin y doc/latex2html.init

%===Thu May  3 19:05:51 ART 2001 mstorti@minerva
%
El problema al linkeditar con /linux: ... se debe a que si en
Makefile.base llegas a poner 'BOPT = O_c++   ' entonces los trailing
spaces te joden!!!

%===Fri May  4 19:11:45 ART 2001 mstorti@node1.beowulf.gtm
%
STL vector bug
==============
%
No compila en geronimo (o sea RH 5.2.)!! Para mi es un error en las
STL del compilador. Hay problemas con la implementacion de vectores y
da un error en el assembler. Los errores aparecen solo en NS en
ns.cpp, wall.cpp, walldata.cpp  y nsi_tet.cpp. Por ahora la solucion
es pasar 'hmin' como 'USER_DATA' en donde este el compilador viejo. Y
hacer una serie de trucos para que no use ciertos vectores STL en esos
archivos. Espero que esto cambie cuando upgrademos el compilador en
geronimo... 

%===Sat May  5 07:46:26 ART 2001 mstorti@spider
%
Cambio los VOID_IT por '.clear()'. En realidad lo que hago es
redefinir el 'VOID_IT'... Todo anda bien lo que quiere decir que se
puede ir reemplazando los VOID_IT por los clear y finalmente eliminar
el VOID_IT. 

%===Fri May 11 20:35:37 ART 2001 mstorti@spider
%
Voy a corregir el bug: bug100. Parece que es cuando un elemset no
tiene elementos pero tiene ghost-elements. 

%===Sat May 12 15:06:17 ART 2001 mstorti@spider
%
Resuelto el problema, ahora aloca un chunk_size que es al menos el
numero de elementos locales + numero de elementos ghost locales.

%===Sat May 12 15:07:19 ART 2001 mstorti@spider
%
Cambio las definiciones de PETSCFEM_ERROR y PETSCFEM_ASSERT. Ahora
petscfem_error emite el error con PetscPrintf() y PETSCFEM_ERROR llama
a petscfem_error con , ademas el numero de linea y archivo. Lo mismo
para PETSCFEM_ASSERT y petscfem_assert. 

%===Sat May 12 18:26:40 ART 2001 mstorti@spider
%
El bug100 se producia cuando en 'elemset.cpp' al definir el chunk_size
para alocar las cantidades locales. Cuando `nelem_here' era 0 pero habia
`ghost_elements', entonces el 'chunk_size=0' y se ve que se armaba lio
porque no alocaba memoria y despues daba un SIGSEGV. 

%===Sat May 12 18:28:55 ART 2001 mstorti@spider
%
Le hice unos retoques esteticos a `myexpect.pl'. 

%===Mon May 21 12:29:26 GMT+3 2001 mstorti@minerva
%
Merged kepsilon from beto.

%===Mon May 21 13:15:59 GMT+3 2001 mstorti@minerva
%
Comienzo a trabajar en k-epsilon. 

%===Thu May 24 22:57:04 ART 2001 mstorti@spider
%
Paso `genload' a `advdif' (Despues habria que hacerlo en forma
generica). Parece que empieza a andar. 
%
Falta:
* agregar diferentes source term y que no haya double layer. 
* elementos 1D. 
* entalpias (termino temporal)
Hacer los siguientes tests: 
* muchos genload

%===Wed May 30 13:56:43 ART 2001 mstorti@minerva
%
En la documentacion no se debe usar el environment 'align' o cualquier
otro que genere mas de un numero de ecuacion por 'display' porque sino
sale mal el numero de ecs. 

%===Fri Jun  1 11:20:07 ART 2001 mstorti@minerva
%
Debuggeando k-e: Un flujo homogeneo se hace inestable. La velocidad
lateral se hace inestable... Con la version original de Beto,
tambien. O sea que no es la opcion 'steady' que agregue. 
%
Habra una llave para desactivar la turbulencia?

%===Fri Jun  1 11:36:09 ART 2001 mstorti@minerva
%
Faltaba definir la viscosidad en el archivo de datos... 
Tambien habia que fijar un nodo de presion paa que no se dispare.
%
No se porque cuando se impone velocidad, p, k y e  a la entrada
entonces la matriz da singular si la presion es impuesta en el mismo
nodo. Hay que hacerlo en otro nodo. Se arregla imponiendo la presion
en un nodo a la salida. Tal vez se arregle cuando usemos weak_form. 
%
A esta altura produce bien el perfil de k y epsilon para un flujo
homogeneo. Voy a empezar a ver la restriccion no-lineal. 

%===Fri Jun 15 13:08:46 ART 2001 mstorti@minerva
%
k-e con ley de pared: Ahora con los jacobianos agregados por Beto
se banca pasos de tiempo muchos mas grande. Puedo correr con Dt=30. 
%
Incluso se banca Dt=30 desde el principio!! Pero los resultados dan
para la mierda... 
%
Lo mismo pasa para Dt=10...
%
Aumentando nnwt de 3 a 5 tampoco anda mejor. 
%
Recien con Dt=3 parece andar bien...
%
Para ese Dt, con 80 iteraciones ya da buenos resultados. 

%===Fri Jun 15 19:05:29 ART 2001 mstorti@spider
%
Para canales (Poiseuille y Couette) da bien. Para pipes (axisimetrico)
Poiseuille da bien. 

%===Sun Jun 17 12:49:15 ART 2001 mstorti@spider
%
Debuggeando k-e para el caso del canal. Encuentro que hay un problema
con los jacobianos. Cuando se desactiva la llave 'turbulence_coef=0'
entonces se deberia deactivar la turbulencia con lo cual el problema
deberia ser casi lineal. En el caso de flujo paralelo deberia ser
lineal. Pero esto no pasa, da convergencias tipo 1e-3, -5, -10 -15. Y
cuando miro los jacobianos numericos y analiticos con
'verify_jacobian_with_numerical_one' me da que el jacobiano numerico
tiene muchos elementos diagonales un factor 3 los analiticos!!! 
%
Parece que hay un factor 'Dt' entre el jacobiano analitico y el
numerico, pero eso se debe a una cuestion de convencion. 
%
El caso canal Poisuille, periodico da perfectamente lineal. 

%===Thu Jun 21 10:48:50 ART 2001 mstorti@minerva
%
Corriendo el caso 2D. 
%
Se produce una inestabilidad en k y epsilon a la salida, cerca de la
pared. Supongo que es la restriccion, voy a poner que el ultimo valor
de la pared mire al anterior en k y en epsilon.
% 
Da un problema con la resolucion de la ecuacion de transporte en K y
en epsilon. 

%===Sat Jun 23 09:31:20 ART 2001 mstorti@spider
%
Habia un error con los jacobianos de los terminos de produccion al
desactivar las banderas de turbulencia (`turbulence_coef' y
`turb_prod_coef'). 
%
Ahora da bien para Re bajos y altos (hasta 1000) en laminar, con
turbulencia desactivada. Converge bien en estacionario y con Dt=1. 
Pero da un ruido en el refinamiento de malla, por ejemplo en el perfil
de velocidad en el centro del canal o en el caudal. Voy a investigar
esto. 

%===Sat Jun 23 12:07:54 ART 2001 mstorti@spider
%
Decididamente la violacion de continuidad se debe al termino PSPG
cuando hay curvatura en la presion o cuando hay malla
variable. Recordar que el termino de estabilizacion es del tipo a
d/dx(h^2/nu dp/dx) o sea que o bien se activa si h=cte y p,xx =
0. Esto quiere decir que en el canal o ducto, con h=cte no aparece,
pero de todas formas eso tiene patas cortas, por ejemplo si la seccion
no fuera constante con lo que habria un 'p,xx \neq 0'. 
%
Podemos pensar que u + h^2/nu p,x = cte, entonces la perdida de masa
sera mas o menos 'Du = D(h^2) p,x'. En el canal tenemos que 'p,x = nu
* umax / L^2' donde L es el ancho del canal y entonces ' Du/umax =
h^2/L^2'. O sea que si usamos elementos muy alargados se nota mas el
efecto. 
%
Probe con 'pspg_factor=0.1' y da una perdida de masa muy pequena pero
da la presion muy oscilatoria. Con 'pspg_factor=0.3' da una perdida de
masa del 0.3% y el campo de presion es razonable. 
%
Tambien agregue un termino 'pspg_advection_factor' para controlar la
incidencia de la advecccion en la estabilizacion PSPG, pero
(obviamente) a Re bajos no incide. 
%
Incluso con 'pspg_factor=1' (pero 'pspg_advection_factor=0') dan
oscilaciones en la esquina entrada/pared.
%
Tambien dan oscilaciones con 'pspg_advection_factor=1' (hay que tener
en cuenta que la cantidad de estabilizacion baja. 
%
Con 'pspg_factor=2', 'pspg_advection_factor=1' da todavia oscilatoria la
presion pero la continuidad da bastante bien (baje la longitud 'x' de la
malla de 20 a 10, con lo cual 'h' bajo a la mitad y el termino de
estabilizacion 1/4.

%===Sun Jun 24 20:41:09 ART 2001 mstorti@spider
%
Ahora da bien el problema con 'turbulence_coef=1' y
'turb_prod_coef=0', pero cuando activo toda la turbulencia, es decir
'turb_prod_coef=1' entonces ahi se va al diablo en la salida. 'k' toma
valores negativos. Tambien 'k' tiene oscilaciones donde hay
refinamientos, asi que sospecho que debe haber alguna problema. 
%
El ruido tambien se produce en la velocidad.  Voy a desactivar
'turb_prod_coef=0' para ver si es una realimentacion a partir de K y E
o si la ecuacion de momento sola produce un salto. 
%
Con 'turb_prod_coef=0' tambien da un ruido en la velocidad en la
primera iteracion de Newton. Parece estar relacionado con el elemento
de pared (el eallke). Voy a correr varias iteraciones de Newton a ver
si es el jacobiano o el residuo o los dos. 
%
El pico tambien se produce en la solucion convergida. 
%
Puede ser que haya que pesar la condicion de contorno (es decir el
wallke) con la funcion perturbada? Creo que no... 
%
Con Re=0.1 tambien da un poco oscilatorio asi que no tiene que ver con
la adveccion. 
%
Con 'steady=1' tambien da un poco oscilatorio asi que no parece tener
que ver con el paso de tiempo. 

%===Mon Jun 25 20:17:57 ART 2001 mstorti@spider
%
Ahora con la version lumped anda mucho mejor. Pero de todas en formas
en steady revienta (bueno eso seria demasiado pedir). Vamos a probar
con un paso de tiempo menor. 

%===Mon Jun 25 21:35:40 ART 2001 mstorti@spider
%
Revienta igual. El lumped no parece dar mejor. Voy a probar
desactivando el 'turb_prod_coef=0'. 

%===Sun Jul  1 17:39:27 ART 2001 mstorti@spider
%
Lo pude hacer converger, para Re=1e4, 
    $turbulence_coef=1;         # mask turbulence
    $turb_prod_coef=0;          # mask turbulence production terms
lumped=0; converge bien, si bien da `eps' negativo en dos puntos sobre
la pared a la entrada. 
%
Haciendo que 'uwall' crezca desde cierto valor no unitario mejora
bastante. Ahora vamos a probar si se puede reducir la longitud de
empalme. 
%
Con longitud de empalme 1 (`$uwall_match_len=1') tambien
converge. Ahora voy a probar prendiendo '$turbulence_coef=1'. 
%
Tambien da con `$uwall_match_len=1' prendido. Voy a probar a ver si se
puede bajar todavia mas la longitud de empalme. 
%
Con longitud de empalme por debajo de 0.5 da valores de E negativos a
la salida. Como si hubiera un problema con la condicion de contorno. 

%===Mon Jul  2 09:13:22 ART 2001 mstorti@spider
%
Con funciones 'cutoff' (1e-3 para k y 1e-4 para eps) converge con
longitud de empalme .3 y Re=1e4 y da perfiles suaves, etc... Cuando lo
bajo a longitud de empalme .1 y Re=4e5 se va a la mierda enseguida. 
%
Con LE=.3 y Re=4e5 tambien diverge.
%
Con Re=1e5 diverge. 
%
Con Re=5e4 converge inicializando de la solucion a Re=1e4. Los cutoff
values todos en `kap_ctff_val 1e-4', `eps_ctff_val 1e-4'.

%===Fri Jul  6 12:31:51 ART 2001 mstorti@minerva
%
Pruebo a correr con turbulencia y con jacobi a ver como afecta la
convergencia. 
%
En el problema 1D (con peri) la convergencia es muy buena. 

%===Sun Jul  8 11:22:23 ART 2001 mstorti@spider
%
Estoy jugando con los parametros 
%
lagrange_diagonal_factor (abrev. ldf)
lagrange_residual_factor (lrf)
lagrange_scale_factor (lsf)
%
Si `ldf=0' entonces el `lu' no anda y el iterativo no converge. 
Si `ldf>0' y 'lrf=1' entonces introducimos un error de consistencia. 
Si `ldf>0' y 'lrf=0' entonces hacemos Newton, converge bien GMRES pero
no converge el Newton. 

%===Sun Jul  8 19:43:25 ART 2001 mstorti@spider
%
Incluso con "lu", no converge para nada si usamos "lrf=0". No tengo en
claro porque. 
%
21:25:56 ART: Todo se arreglo al eliminar el elemento de restriccion
primero. Ya que todas las variables del primer nodo en la pared estan
fijas. De todas formas deberia converger!!!

%===Mon Jul  9 12:18:49 ART 2001 mstorti@spider
%
Ahora converge razonablemente pero no puedo empezar directamente con
iterativo y despues de un cierto numero de iteraciones se clava. Voy a
probar si pasa lo mismo con "lu". 
%
12:22:04 ART: Con "lu" converge bien. Voy a probar a ver si es el
`lagrange_diagonal_factor'. 
%
13:06:59 ART: Pareceria ser que si usas `ldf>lsf' entonces diverge. Si
usas `ldf<<lsf' entonces converge rapido pero despues se estanca. Si
usas `ldf<lsf' (pero no `ldf<<lsf') entonces converge lenta pero
monotamente. 
%
14:10:48 ART: Reviso un poco lo anterior. Corro con `lsf=1e-3' y
`ldf=1e-2,1e-3,1e-4,1e-5,1e-6'. Diverge para `ldf=1-2' y converge
mejor cuanto mayor es el `ldf'. En el limite se estanca  para
`ldf=0'. 

%===Mon Jul  9 17:30:21 ART 2001 mstorti@spider
%
Puedo obtener una solucion completa con metodo iterativo. 
Con 
> Dt 1
> steady 0
> lagrange_scale_factor 1e-3
> lagrange_diagonal_factor 1e-3
> lagrange_residual_factor 0 
> maxits 100
> nnwt 4
%
y empezando 10 iteraciones 
> newton_relaxation_factor 0.1
y a partir de ahi siguiendo con 
> newton_relaxation_factor 1
%
converge a 1e-15 en unas 1400 iteraciones. Al final tiene una tasa de
100 iteraciones por orden de magnitud. (100 iter quiere decir 25 Dt  *
nnwt=4). 
%
17:50:31 ART: Corro el problema completo (100 nodos en direcicon y,
Ly=60). No converge con los parametros anteriores. Pruebo a bajar el
`ldf' a 1e-4.

%===Mon Jul 16 10:36:09 ART 2001 mstorti@spider
%
Escribiendo el IISD (Interface Iterative, Subdomain Direct)
solver. Para un procesador esta dando lo mismo, curiosamente no esta
accediendo a las opciones de iteracion varias veces (tantas como
factorizaciones de la matriz) sino una sola vez. 
%
No!!! Es al reves, ahora lo lee varias veces, cada vez que va a
resolver. (No se si esto puede traer problemas despues...)
%
10:52:19 ART: Ahora voy a verificar que ande bien con mas de un
procesador y con metodos iterativos. 
%
Aparentemente anda bien, pero las versiones viejas no andan bien con
mas de un procesador. 

%===Wed Jul 18 10:44:00 ART 2001 mstorti@spider
%
No andaba bien, porque aparentemente no es cierto que si llamas a
`KSPSetMonitor' con PETSC_NULL entonces desactiva el monitoreo sino
que se cuelga. 

%===Wed Jul 18 18:41:27 ART 2001 mstorti@spider
%
Decubri un error en `read_vector': si no hay suficientes valores en el
archivo entonces ahora manda un mensaje de error, antes probablemente
quedaba basura. 
%
18:55:24 ART: Voy a debuggear porque a partir de la segunda iteracion
no converge tan rapidamente como debiera??? 
%
18:57:52 ART: Ahora anda bien. 
%
19:03:01 ART: Voy a empezar a debuggear con varios procesadores. 
%
20:30:57: El SLES hay que volverlo a crear cada vez, sino no factoriza
la matriz asociada. 

%===Thu Jul 19 23:28:02 ART 2001 mstorti@spider
Da un SIGSEGV cuando en `nsitetlesfem2' cuando se usa
cache. Desactivando el cache no hay problema (despues da error al
cargar con `set_value'). 
%
00:11:29: Desactivando el cache solo en `comp_mat' da SIGSEGV igual. 

%===Fri Jul 20 07:19:57 ART 2001 mstorti@spider
%
Desactivando el cache a partir de un cierto punto con
`deactivate_cache()' no da SIGSEGV. 
%
08:28:43: Activando cache y desactivando en ciertas partes de
`nsitetlesfem2' puedo hacer que de SIGSEGV o no pero no avanzo
nada. Voy a desactivar el cache y avanzar con el tema del IISD en si. 
%
11:29:40: Se bloquea (tipo bloqueo por MPI) en `assembly_begin' de las
matrices LI, IL o II . Con el debugger se ve que esto occurre al hacer
un MPI_Allreduce. . Voy a probar si se debe al tema de no llamar a
`assemble' cuando el numero de elmentos procesados es nulo. 
%
11:49:26: Probe con lo de sacar el `if (iele_here > -1) {' pero no
anduvo. Voy a probar a hacer solamente el `FINAL_ASSEMBLY'. 
%
14:45:33: Voy a ver si tampoco anda sacando el `set_value' para los
bloques no `LL'. Si saco los `set_value' para todos los bloques menos
el `LL' entonces se bloquea en el MPI_Allreduce de
`local_has_finished'. 
%
Habia un lio con el error que mandaba el `IISDMat::create' que al
tirar un error generaba un deadlock. Para arreglarlo, resolvemos el
problema del bloque `LL' parcialmente. Cambiamos la numeracion de
nodos en `readmesh' de tal forma que ahora los nodos que estan
conectados a mas de un procesador se asignan al procesador de
numeracion mas alta. Despues, en las matrices  `IISDMat' como
consideramos a los dof's como interface aquellos que estan conectados
con un dof en un procesador de numeracion menor, entonces esto hace
que los dof's marcados como locales reciben contribuciones solo de
elementos locales. 
%
Pero esto tiene un problema: si el elemset no es considerado FAT
entonces podria ser que depues genere elementos `locales' en otro
procesador. De todas formas vamos a tener que hacer algo despues para
mandar las contribuciones locales a otros procesadores.  

%===Mon Jul 23 13:28:17 ART 2001 mstorti@spider
%
IISD is working!! Verified that works well for solution of several
systems (several iterations and time steps).
%
%
13:28:53: Voy a chequear que no pierda memoria. 
%
13:58:40: Efectivamente pierde memoria. Voy a verificar...
%
Ya encontre el error, ahora no pierde memoria. Probe con una caso mas
grande. Ahora habria que verificar que no refactoriza la matriz. 
%
Converge razonablemente, hay que probarlo en el cluster. 

%===Mon Jul 23 19:46:25 ART 2001 mstorti@spider
%
Anda bien la implementacion del producto de la matriz traspuesta para
IISD de manera que permite usar metodos como CGS, BiCG, etc... 

%===Wed Jul 25 12:45:16 ART 2001 mstorti@minerva
%
Haciendo upgrade a RH 7.1: las librerias 'glib' van a parar a
`/usr/include/glib-1.2; y a `/usr/lib/glib/include'. Este ultimo lo
pongo en una variable CPPFLAGS definida en el `Makefile.defs'. 

%===Wed Jul 25 15:21:15 ART 2001 mstorti@minerva
%
Al compilarlo en minerva cn RH 7.1 no tuvo problemas con IISD en mas
de un procesador. Ahora voy a probar con algo mas grosso en el
cluster. 

%===Wed Jul 25 21:10:18 ART 2001 mstorti@spider
%
Con la nueva version del compilador (RH 7.1, anda bien con IISD, en 2 y 3
procesadores (gcc version 2.96 20000731 (Red Hat Linux 7.1 2.96-81)
virtuales. 

%===Sat Jul 28 11:09:27 ART 2001 mstorti@spider
%
Corregido un error en cuanto a que no se retornaba error en algunas 
rutinas de `IISDMat' y `PETScMat'. 
%
%
11:12:00: Con RH 7.1 en casa vuelven a aparecer los problemas en
`advdif' para NP>1 en la dealocacion de memoria de los vectores
STL. Voy a probar a hacer que ciertas variables locales sean
estaticas, asi cuando las borra no hay drama. 
%
%
11:15:49: declarando 
%
static vector<int> nnz[2][2][2],flag0,flag,*d_nnz,*o_nnz;
%
no anda tampoco. 

%===Sat Jul 28 16:20:03 ART 2001 mstorti@spider
%
Encontre el error. Al crear los `d_nnz' `o_nnz' para IISDMat entonces
estaba mal el calculo de `row' y escribia en partes no habilitadas de
`d_nnz' o `o_nnz'. 
%
Ahora voy a escribir una clase 'DistMap' (distributed map) o
`distributed container'. 

%===Tue Aug  7 20:46:00 ART 2001 mstorti@spider
%
Escrita la clase DistMap y funciona bien. Habia un bug, escribi un
test para ello.
%
Cree un nuevo particionamiento llamado `random' que asigna un
procesador aleatorio para cada elemento. Esto sirve para debuggear
cosas. Por ejemplo asi se detecta mas facil el bug mencionado
anteriormente. 

%===Wed Aug  8 14:17:17 ART 2001 mstorti@minerva
%
Hace falta incluir un precondicionamiento para el IISD, aunque sea los
elementos diagonales de la matriz de FEM (no la de SCHUR). 

%===Thu Aug  9 16:35:11 ART 2001 mstorti@spider
%
El precondicionamiento diagonal fue implementado y anda muy bien!! 
%
En geronimo da SIGSEGV al correr compilado con optimizador. En spider
no, anda con optimizador para 3 procesos. 

%===Thu Aug 16 16:24:41 ART 2001 mstorti@node1.beowulf.gtm
%
Evaluacion de performance con IISD. Corremos el auto (malla
auto_11.epl, 11873 nodos, 22600 Navier Stokes 2D, a Re=3e6). 
%
Con IISD: 
=========
Con NP=10 procesadores usa 
%
94.72user 6.16system 2:03.61elapsed 81%CPU (0avgtext+0avgdata 0maxresident)
%
para hacer 20 pasos de tiempo con nnwt=1 (o sea 20 resoluciones del
sistema lineal). En promedio consume unos 30 y pico de iteraciones
para cada resolucion).  La tolerancia interna (del lazo de GMRES SOBRE
LA INTERFACE) puesto a 1e-3.
%
Con GMRES sobre el sistema global:
=================================
%
Para la misma precision (guarda que el operador es otro!!, en IISD es
la matriz complemento de Schur, aqui es la global). En promedio no
converge a 1e-3 en 200 iteraciones. Como los vectores son grandes se
ralentiza bastante a medida que avanza en GMRES y tarda:
%
396.97user 22.72system 8:52.55elapsed 78%CPU (0avgtext+0avgdata 0maxresident)k
%
Es de esperar que la relacion entre IISD y GMRES global sea mas
favorable a IISD si bajamos la tolerancia ya que se ralentiza menos
(el grueso del trabajo todavia esta en la factorizacion y
retrosustitucion) y la tasa de convergencia es mejor.
%
Con LU
======
%
Por supuesto solo se puede correr en un solo procesador. Se hace una
sola iteracion de GMRES. Tarda
%
1097.67user 5.44system 18:34.64elapsed 98%CPU (0avgtext+0avgdata 0maxresident)k
%
Lo cual es un factor 10 sobre el IISD. Ademas esta casi en el limite
de la memoria (son 10000 nodos, 40000 dof's, ancho de banda medio 400,
con lo cual son unos 16e6 elementos, 120 Mb aprox.). 

%===Thu Aug 16 17:18:28 ART 2001 mstorti@node1.beowulf.gtm
%
Reporte escrito por Jorge D'Elia
================================
%
Gauss solution on the cluster:
%
system size                  -> n
RAM for matrix A             -> r = 8*n^2/1e6
processor number             -> z
RAM of matrix A on each node -> d = r/z
%
------------------------------------------------------------------ 
A on one node:
  n = 3000 number of unknowns
  z =    1 processor
  r =   72 MBytes for whole matrix A
  d =   72 MBytes of A on each node
%
  mpirun -machinefile machi.dat -np 1 gauss3.exe      
%
  times for array with leading dimension          3000
      factor     solve      total     mflops       unit      ratio
  6.760E+02  0.000E+00  6.760E+02  2.665E+01  7.504E-02  1.207E+04
  end of test 
%
------------------------------------------------------------------ 
B on three nodes
  n = 3000 number of unknowns
  z =    3 processors
  r =   72 MBytes for whole matrix A
  d =   24 MBytes of A on each node
%
  mpirun -machinefile machi.dat -np 3 gauss3.exe
%
  times for array with leading dimension          3000
      factor     solve      total     mflops       unit      ratio
  3.200E+02  1.400E+01  3.340E+02  5.395E+01  3.707E-02  5.964E+03
  end of test 
%
------------------------------------------------------------------ 
C speed_up_{np = 3} = 5.395E+01 / 2.665E+01 = 2.0244
%
------------------------------------------------------------------ 
D corrida con el maximo tamanio del sistema posible en el
  actual cluster:
%
  n = 6500 number of unknowns
  z =    3    processors
  r =  338    MBytes for whole matrix A
  d =  112.67 MBytes of A on each node
%
  pghpf -Mautopar -Mmpi -o gauss3.exe gauss3.hpf
%
  mpirun -machinefile machi.dat -np 3 gauss3.exe
%
  times for array with leading dimension          6500
      factor     solve      total     mflops       unit      ratio
  3.152E+03  3.800E+01  3.190E+03  5.742E+01  3.483E-02  5.696E+04
  end of test 
%
------------------------------------------------------------------ 

%===Thu Aug 16 18:14:56 ART 2001 mstorti@node1.beowulf.gtm
%
Reporte de eficiencia para IISD:
=================================
Se corre el canal a Re=1e4 con 
%
iisd: IISD + preco jacobi 10 procesadores
lu: LU en 1 proc.
gmresg: GMRES + Jacobi global en 10 procs.
%
Para N=16, Ny=60, Ly=1, (malla `medium') da:
%
iisd: (Porque da tan alto el system?). Convergencia siempre a 1e-3 
10.71user 8.58system 0:29.69elapsed 64%CPU (0avgtext+0avgdata 0maxresident)k
lu:
28.98user 0.67system 0:31.59elapsed 93%CPU (0avgtext+0avgdata 0maxresident)k
gmresg: (La convergencia es muy mala, no ceonverge en ningun caso)
13.67user 16.74system 1:07.31elapsed 45%CPU (0avgtext+0avgdata 0maxresident)k
%
Malla 'very_large' de N=32 por Ny=120, Ly=10 ((N=3993 nodos)
============================================
%
lu: 
191.73user 2.76system 3:17.78elapsed 98%CPU (0avgtext+0avgdata 0maxresident)k
iisd: (En promedio usa al principio entre 120 y 150 iteraciones y
               despues va bajando, porque salta por atol)
65.65user 11.38system 1:52.05elapsed 68%CPU (0avgtext+0avgdata 0maxresident)k
gmresg:
53.17user 23.63system 3:58.88elapsed 32%CPU (0avgtext+0avgdata 0maxresident)k
%
Malla de 64 x 250 (N=16315 nodos):
==================
gmresg: la convergencia es pesima!! En 200 iteraciones no baja un 20%
           el residuo!!
174.78user 20.53system 5:24.05elapsed 60%CPU (0avgtext+0avgdata 0maxresident)k
iisd: tarda entre 140 y 190 iteraciones al principio. Al final baja
       bastante porque salta por atol. 
388.86user 16.09system 9:01.87elapsed 74%CPU (0avgtext+0avgdata 0maxresident)k
lu:
1465.77user 11.82system 25:00.16elapsed 98%CPU (0avgtext+0avgdata 0maxresident)k
%
Malla de 128 x 500 (N=64629 nodos):
==================
%
gmresg: no converge ni a canhonazos
lu: no entra en memoria
iisd: converge bien, 4 ordenes de magnitud en approx 200 iter. Usa 66
Mb de memoria. Usa unos 4' por iteracion, pero se podria bajar
bastante ya que podriamos usar una tolerancia de 1e-2 con lo cual
bajaria el tiempo a la mitad. La tasa de convergencia es de 60
iter/orden. Los 20 pasos de tiempo tardarian unos 80'. Con respecto al
problema de 64x250 vemos que el tiempo por iteracion aumento un factor
9, con lo cual el costo re resolucion va como N^1.6, donde N es el
numero de nodos. N=64629 nodos en la malla de 128x500. Ahora vamos a
tratar de probar con un problema 3D. Creo que andaba por ahi el LES. 

%===Sat Aug 18 17:11:14 ART 2001 mstorti@spider
%
Sinopsis de uso de PFMat:
%
  PFMat *A;
  A = PFMat_dispatch(solver.c_str());
  argl.arg_add(A,PROFILE|PFMAT); 
  // call to `assemble' to define profile and setup matrices
  for (....) { // loop over Dt or whatever that reuses the matrix profile
    A->zero_entries(); // This must be present
    argl.arg_add(A,OUT_MATRIX|PFMAT);
    // call to assemble to set values
    A->view(matlab);
    A->build_sles(GLOBAL_OPTIONS);
    for (...) {  // Solve several times with the same matrix
       ierr = A->solve(res,dx); CHKERRA(ierr); 
    }
    ierr = A->destroy_sles(); CHKERRA(ierr); 
  }
  delete A;

%===Sun Aug 19 13:23:51 ART 2001 mstorti@spider
%
`test/distmap/tryme4.cpp': Studying how to make the solution of local
systems with LU decomposition more efficient. Tratando de usar
`KSPPREONLY', `PCLUSetFill()' y `PCLUSetUseInPlace(pc)'. 
%
con:
    ierr = KSPSetType(ksp,KSPGMRES); CHKERRA(ierr); 
    ierr = KSPSetTolerances(ksp,0,0,1e10,1); CHKERRA(ierr); 
marca 25740 Kb de memoria usada (con top). 
%
con: 
    ierr = KSPSetType(ksp,KSPPREONLY); CHKERRA(ierr); 
marca 21688 Kb de memoria usada.

%===Sun Aug 19 17:23:11 ART 2001 mstorti@spider
%
otra prueba: Con N=100000 y M=100 la matriz factorizada requiere unos
16Mb. Con `pc_lu_fill=1.7' puedo correr con `ulimit -v 30000' mientras
que sin setear `pc_lu_fill' necesito ir a mas de `ulimit -v 35000'. 
%
19:31:15: Separo `iisdmat.h' de `pfmat.h'. 

%===Mon Aug 20 10:17:09 ART 2001 mstorti@spider
%
Voy a empezar a escribir el precondicionamiento para IISD. 
Branchear a partir de beta-2.02 si se quiere evitar el lio generado
por esto. 
%
11:39:52: Fuerzo a que los perfiles sean simetricos, haciendo que en
`compute_prof' se inserten `j,k' y `k,j'. 

%===Tue Aug 21 22:08:28 ART 2001 mstorti@spider
%
Escribiendo la clase DistMap (distributed Container). Habia un error
que ponia hacia pack tambien para mandarse a si mismo (myrank ->
myrank). Entonces hay que poner:
%
      if (k!=myrank) pack(*iter,send_buff_pos[k]);
 
%===Tue Aug 21 22:25:56 ART 2001 mstorti@spider
%
Works well with changed names from `distmap2.xxx' to `distcont.xxx'

%===Sat Aug 25 12:08:44 ART 2001 mstorti@spider
%
Adding Finite Difference Jacobian support to PFMat.
%
En principio no lo pude hacer andar asi que me voy a una version
estable anterior para avanzar con el tema del sloshing.  Hay que
implementar la clase PFMat como una maquina de estados (el esbozo esta
en `doc/OBJ/pfmat.fig'. 
%
Vuelvo a la version `beta-1.82'. 

%===Fri Sep  7 15:12:01 ART 2001 mstorti@minerva
%
Me daban mal los jacobianos, habia un bug en la version beta-1.82 para
el calculo de jacobianos. Se pasaba mal el `glob_param' eso se
arreglaria con `dynamic_cast' etc... 

%===Fri Sep  7 16:04:47 ART 2001 mstorti@minerva
%
Con jacobiano numerico anda bien si en las contribuciones a la
ecuacion de momento `resmom' agrego solo el termino du/dt. Voy
agregando otros terminos y veo. Si agrego el convectivo sigue andando
bien. 
%
Se jode cuando agrego el termino de viscosidad $\Delta!u$. 
%
La viscosidad hay que ponerla muy pequenha porque si la pones a 0 te
da una singularidad en el Peclet, etc... 

%===Sat Sep  8 09:57:15 ART 2001 mstorti@spider
%
Comentando todo da Ainf=ADt=0. Ahora voy a probar a prender solo la
matriz de masa. 
%
10:13:32: Poniendo solo la masa da perfecto. Ainf es 0. ADt da igual
para todos los campos y la suma de los elementos da 29.800 que
dividido por 3 (por campos) y por el Dt que es 0.01 da 9.9335e-02 que
es el volumen del sector de cilindro 0<=z<=1, r<=1, 0\le\theta\le
.2. cuyo volumen es h*\Delta\theta*R^2/2=.1, pero el volumen del area
discretizada (por el error entre la cuerda y el arco de
circunferencia) es h*\cos(\Delta\theta)*\sin(\Delta\theta)*R^2 que da
exactamente eso. Ahora prendemos el termino de viscosidad y apagamos
el temporal. 
%
10:28:13: Apagando el termino temporal y prendiendo viscosidad
efectivamente dan diferentes ADt de Ainf (deberian dar iguales). 
%
10:31:00: Los dos dan suma 0, esta bien. 
%
10:32:42: Guarda!! que me parece que estaba mirando mal. 
%
10:40:33: Efectivamente, ahora pongo todo y da lo que tiene que
dar. Todo el error se debio al error en el calculo de los jacobianos
numericos. 
%
11:26:19: Con upwind (`tau_fac=1') tambien anda todo bien. 

=======
%===Fri Sep  7 15:12:01 ART 2001 mstorti@minerva
%
Me daban mal los jacobianos, habia un bug en la version beta-1.82 para
el calculo de jacobianos. Se pasaba mal el `glob_param' eso se
arreglaria con `dynamic_cast' etc... 

%===Fri Sep  7 16:04:47 ART 2001 mstorti@minerva
%
Con jacobiano numerico anda bien si en las contribuciones a la
ecuacion de momento `resmom' agrego solo el termino du/dt. Voy
agregando otros terminos y veo. Si agrego el convectivo sigue andando
bien. 
%
Se jode cuando agrego el termino de viscosidad $\Delta!u$. 
%
La viscosidad hay que ponerla muy pequenha porque si la pones a 0 te
da una singularidad en el Peclet, etc... 

%===Mon Sep 10 12:10:44 ART 2001 mstorti@minerva
%
Voy a probar a tomar la matriz sin imponer p=0 en un nodo y ver sobre
esa matriz que pasa al imponer p=0 en un nodo o el otro. 
%
15:41:52: Poniendo la velocidad impuesta a la salida (en r=Rint) da
bien. Es decir el perfil de velocidades da variando inversamente
proporcional al radio (por continuidad). 
%
16:31:20: El problema con velocidad circunferencial da perfecto. Con
velocidad proporcional al radio y proporcional a la inversa del radio
(las dos soluciones fundamentales). 

%===Tue Sep 11 08:29:58 ART 2001 mstorti@minerva
%
El problema para el modo 1 con velocidad uniforme segun x no anda
bien. Pasa que no puedo usar complejos con el paquete sparse. Entonces
escribo el rotconden de forma que ahora trabaja todo en full. Si eso
anda despues pasare las rutinas del paquete SPARSE a complejos. 
%
10:10:30: Verifico que rotando las matrices A22 da A11 y A12 da A21. 
%
12:14:58: Anda bien el caso velocidad uniforme segun x. 

%===Thu Sep 13 10:55:51 ART 2001 mstorti@minerva
%
Ayer andaban bien los modos de oscilacion, me concentre en el programa
de visualizacion y ahora no puedo recuperar bien que me calcule modos
de oscilacion a frecuencias relativamente altas. 
%
13:20:45: Ahora empieza a andar bien (todavia no se bien porque no
andaba). Voy a probar a refinar la malla y reducir el diametro
interno. 
=======
%===Sun Sep  9 10:19:42 ART 2001 mstorti@spider
%
El `rotconden' anda ma o menos bien. Para la matriz de masa preserva
la masa total. 
%
17:08:22: Flujo vertical: da bien. Flujo radial, no da bien. Incluso
dejando que el valor a la salida lo saque solo, es decir dejando
u_normal a la salida libre. 
%
18:50:02: El flujo radial da "mas o menos" bien. La velocidad aumenta
hacia el eje, pero no todo lo que deberia y la presion tiene una
oscilacion fuerte. Me imagino que es por la imposicion de la presion. 

=======
%===Mon Sep 10 12:10:44 ART 2001 mstorti@minerva
%
14:20:50: Lo que parecia ser una onda de alta frequency era un error
de no correr el `rock2' despues de volver a correr un problema
incrementando el numero de nodos. 

%===Tue Sep 18 09:07:15 ART 2001 mstorti@minerva
%
En la region anular parecio que daba buenas ondas, ahora yendo hasta
el origen tiene sus dramas, sobe todo para el modo 1. Voy a volver a
probar con el modo 0. 
%
10:14:15: Estaba mal el calculo del `h_pspg': estaba con la formula 3D
y va la 2D. 
%
10:15:15: Estamos tratando de ver el modo 0. 
%
10:15:57: El `h_pspg' parece ser un problema. Como hacemos un calculo
de fourier en la direccion azimutal, entonces no hay que estabilizar 

%===Mon Sep 24 10:07:45 ART 2001 mstorti@minerva
%
Start writing a Sparse::Mat class based on SuperLU. Wrote already all
algebraic part, remains to add the `solve()' member based on
SuperLU. There is a problem with the fact that `SuperLU' is based on
columnwise stored representation. 

%===Sat Sep 29 09:41:20 ART 2001 mstorti@spider
%
Sparse::Mat is working as a finite state machine. Now I will include
the new Sparse::Mat (with SuperLu) as a sequential solver for IISD. 

%===Thu Oct  4 13:12:19 ART 2001 mstorti@minerva
%
Voy a agregar una restriccion como nonlr, pero ahora que pueda
involucrar a varios nodos. 
%
16:08:49: Detecte un error en el test `test/lupart' target
`test_iisd'. 

%===Sun Oct  7 18:11:01 ARST 2001 mstorti@spider
%
Corregido el jacobiano para weak_form=1 en `nsitetlesfm2.cpp'. 

%===Thu Nov  1 10:59:01 ART 2001 mstorti@minerva
%
El calculo del jacobiano numerico era super-lento con rockns.bin
(jacobiano en frecuencias). Resulta que el `MatAssemblyEnd()' hace un
laburo de eliminar aquellos elementos que no fueron seteados. Esto
hace que si en la primera iteracion no fueron seteados algunos
elementos entonces despues al compactarlo no te queda espacio para
lugares que eventualmente se pueden llenar y te da error si, por
ejemplo, pones `MatSetOption(A, MAT_NEW_NONZERO_ALLOCATION_ERR)'. Las
posibles soluciones son:
%
* Arreglar PETSc para que no compacte. 
* Que en `upload_vector' no mire el valor en si, sino una mascara. Aca
    habria que tocar solo PETSc-FEM
%
Ahora quedo medio emparchado porque en `upload_vector' le puse que
cargue todos los elementos, sin mirar si son nulos o no, pero si en
algun programa se uso la opcion de perfiles entonces tambien tendria
que haber devuelto perfil lleno. 

%===Mon Nov 12 14:06:19 ART 2001 mstorti@minerva
%
Agregue un nuevo particionador que permite correr con muchos mas
elementos. La idea es que de los $\Nelem$ elementos en la malla se
eligen al azahar $\Nelem/f$ donde $f$ es un numero digamos del orden
de 10 y se hace el grafo de conectividad haciendo `coalescer' los
elementos hacia estos puntos de precipitacion. Esto permite obtener
particionamientos razonables con mucho menos recursos. 
%
Tambien se agrego una opcion \verb+iisd_subpart+ de tal manera que en
realidad se particiona en $m$=\verb+size*iisd_subpart+
subdominios. Los subdominios $S_0$ a $S_{m-1}$ viven en el procesador
$P_0$, los  subdominios $S_m$ a $S_{2m-1}$ en el $P_1$, de manera que
el $S_j$ pertenece al procesador $P_{[j/m]}$ ($[x]$ denota la parte
entera de $x$). En realidad esto se hace permutando los dominios en
forma aleatoria de manera que en lo posible los dominios que viven en
un msmo procesador no esten en contacto. De esta forma en vez de tener
un subdominio grande tenemos pequenhos dominios disconexos. De esta
forma para $m=1$ tenemos el IISD clasico y para $m$ grande tendriamos
que todos los nodos serian interfase y el metodo seria puramente
iterativo. 
%
Resultados: Corriendo el `auto_32.epl' (183001 elementos, 40655 nodos)
y poniendo $m=20$ con 11 procesadores es decir que en realidad Metis
esta particionando en 200 subdominios (cada uno con del orden de 830
elementos) tenemos unos 40secs. por paso de tiempo (1 iteracion de
Newton por Dt). Usado $m=5$ tenemos 1min 50secs para calculo de
residuo/matrices  y factorizacion y 10'' para iterar GMRES (sin
debugger). Para $m=20$ tenemos 
%
Tenemos (sin debugger) (secs.)
m		res/mat/fact		GMRES
5		110			10
20		50			10
40		50			10
%
Con debugger los tiempos bajan a 34+6=40 secs. 

%===Wed Nov 14 23:47:06 ARST 2001 mstorti@spider
%
Estoy tratando de agregar opciones a las matrices Sparse::Mat, pero me
encuentro con el problema de poner el `TextHashTable' en el
Sparse::Mat o en el PFMat. Esta a medio hacer.

%===Tue Nov 20 12:54:06 ART 2001 mstorti@minerva
%
Con el IISD y `iisd_subpart=20' puedo correr con 60,000 nodos la cavidad
cubica. Tarda unos 2'38'' por iteracion. 

%===Sun Nov 25 19:45:32 ARST 2001 mstorti@spider
%
Voy a trabajar en el subparticionamiento dentro de cada subdominio. 

%===Fri Nov 30 10:20:32 ARST 2001 mstorti@spider
%
Subparticionamiento OK. 
Pasando en limpio los `adaptors' y escribiendo un nuevo adaptor donde
solo se calculan contribuciones a nivel del punto de Gauss. 

%===Mon Dec  3 13:57:59 ART 2001 mstorti@minerva
%
Volvemos a reflotar el tema del `cache_grad_div_u'. Activado da (en
una malla de 1600 elementos (sqcav con 40x40)) con debugger 
%
########## CON CACHE_GRAD_DIV_U = 0, 2D
[proc 0]   1.65   1.03125
[proc 0]   1.74   1.0875
[proc 0]   1.73   1.08125
[proc 0]   1.72   1.075
[proc 0]   1.69   1.05625
%
total: 16.54user 0.47system 0:20.04elapsed 84%CPU
%
########## CON CACHE_GRAD_DIV_U = 1, EN 2D
[proc] - total[sec] - rate[sec/Kelement]  
[proc 0]   1.47   0.91875
[proc 0]   1.48   0.925
[proc 0]   1.51   0.94375
[proc 0]   1.45   0.90625
%
total: 15.71user 0.45system 0:27.11elapsed 59%CPU
%
EN 3D -------------------------------
%
Sin debugger. En el cluster. 
%
%%% 
########## CON CACHE_GRAD_DIV_U = 0, 3D
Performance report for elemset "nsi_tet_les_fm2" task "comp_mat_res"
[proc] - total[sec] - rate[sec/Kelement]
[proc 0]   31.09   0.829509
[proc 1]   52.15   1.44813
[proc 2]   56.21   1.59082
[proc 3]   64.54   1.7229
[proc 4]   61.47   1.6542
[proc 5]   62.98   1.68041
[proc 6]   63.31   1.75282
[proc 7]   56.86   2.09229
[proc 8]   60.37   2.18558
[proc 9]   59.11   2.13656
[proc 10]   60.51   2.22602
%
########## CON CACHE_GRAD_DIV_U = 1, 2D
[proc] - total[sec] - rate[sec/Kelement]
[proc 0]   22.81   0.608591
[proc 1]   43.36   1.20404
[proc 2]   47.4   1.34148
[proc 3]   48.49   1.29445
[proc 4]   51.99   1.39909
[proc 5]   48.33   1.28952
[proc 6]   53.3   1.47568
[proc 7]   65.02   2.39255
[proc 8]   64.57   2.33763
[proc 9]   62.01   2.24138
[proc 10]   64.44   2.3706
%
Al activar el `cache_grad_div_u' los tiempos disminuyen para los
procesadores 1-7 en promedio en un 20%. Mientras que para los
procesadores 8-11 aumenta de un 5% a un 15%. 

%===Wed Dec  5 09:39:45 ART 2001 mstorti@minerva
%
Escribimos con Beto una modificacion para usar el programa para
deformar mallas y anduvo. Creo que se toco solo el main `ns.cpp' y la
nueva version la voy a mantener como un `branch' que sale de la
`petscfem-beta-2.52' y empieza como la `mesh-move-branch'. La version
anterior la vuelvo para atras. 

%===Wed Dec  5 13:39:32 ART 2001 mstorti@minerva
%
Con la nueva version da `pure virtual method called' en tiempo de
ejecucion, pero despues no afecta a los tests. 

%===Thu Dec  6 22:23:20 ARST 2001 mstorti@spider
%
No hay caso. No puedo encontrar el error. Aparentemente esta
relacionado con la logica de la maquina de estado, pero por otra parte
no hay problema con la version PETSc (`iisd_petsc') pero si con la
SuperLU. Abandono, porque de todas formas la version SuperLU no la
estamos usando. Ahora voy a tratar de debuggear porque da error cuando
se reusa la factorizacion de `iisd_petsc', por ejemplo cuando usamos
`update_jacobian'. 

%===Fri Dec  7 21:39:41 ARST 2001 mstorti@spider
%
Estoy agregando la maquina de estados, pero de a poco. Primero estoy
descomponiendo la funcion `solve()' anterior, como `factor_and_solve()' y
`solve_only()'. Esto ya esta compilando. 

%===Fri Jan  4 08:20:01 ARST 2002 mstorti@spider
%
La nueva version de `PFMat' (por ahora `IISDMat' y `PETScMat')
funciona aparentemente bien, pero pierde memoria. Aparentemente no
esta relacionado con StoreGrpah (lgraph). Todas estas pruebas hechas
con `pfmat.cpp'. 
%
08:31:23: Multiples soluciones (nsl>>1) no produce perdida de memoria.
%
08:33:24: Multiples definiciones de la matriz (nmat>>1) si produce
perdida de memoria. 
%
09:04:55: Solo se produce perdida de memoria con `IISDMat' no con
`PETScMat'. 
%
09:09:58: Con `iisd_subpart>1' es cuando pierde memoria. 
%
12:22:38: Con `SuperLU' como `LocalSolver' `parecia' al principio que
perdia menos, pero ahora no estoy muy seguro. 
%
21:56:04: Sacando la parte de resolucion no pierde memoria.
%
21:59:15: La perdida de memoria esta en el solve.

%===Sat Jan  5 07:34:25 ARST 2002 mstorti@spider
%
No es en el `A_LL_other->scatter()'.
%
07:46:03: Pierde memoria con `nsl'=1 y 2.
%
07:54:38: La perdida esta en `factor_and_solve'.
%
08:07:29: La perdida es en `local_solve'.
%
08:17:12: La perdida esta en el
`SLESSolve(sles_ll,y_loc_seq,x_loc_seq,&its_);' en `local_solve'. 
%
17:16:33: En realidad no perdia con `SuperLU', sino que leia mal los
datos y en realidad siempre usaba PETSc. Ahora voy a tratar de evitar
las perdidas que reporta `LeakTracer'.

%===Thu Jan 10 20:23:44 ARST 2002 mstorti@spider
%
El solver `IISDMat(PETSc)' da error en PETSc al hacer el
SLESDestroy(sles)
%
20:30:01: Comentando todo 'local_solve_SLU' no da error.

%===Fri Jan 11 19:19:06 ARST 2002 mstorti@spider
%
Cuando el local solver `A_LL_SLU' es `PETSc' (no `SuperLU') de nuevo
anda bien. 
%
21:02:29: Abandono... Parece ser un quilobo relacionado puramente con
SuperLU. 

%===Mon Dec 10 08:51:01 ART 2001 mstorti@minerva
%
`update_jacobians' anda en el cluster para la cavidad (el ejemplo
`test/sqcav/') pero no para la cavidad cubica. Los tiempos obtenidos
en la cavidad cuadrada con 40000 elementos (200x200) es de 0.79 secs
para el assemble de residuo (`update_jacobians=0') y 3.5 secs para
assemble de residuo y matriz (`update_jacobians=1'). O sea una
relacion de 4.4. Sin embargo en este caso la resolucion es mas cara
(7 a 8 secs promedio). 
%
08:56:36: La convergencia se deteriora pero llega a bajar el residuo
desde 1e-3 hasta 3e-8 en unos 30 pasos de tiempo. 
Las dudas son: si la convergencia se termina deteriorando por
problemas de precision o por usar el IISD. 

%===Wed Dec 12 14:52:11 ART 2001 mstorti@minerva
%
Uno de los tests de `lupart' no anda y se trata del IISDMat con solver
PETSc cuando no hay `sles_ll'. 

%===Mon Jan 14 06:16:44 ARST 2002 mstorti@spider
%
Start working in elemsets for surface and sub-surface hidrology. First
starting with adding a transmisivity and storativity ptoportional to
$\phi-\eta$. 

%===Sat Feb  2 12:16:31 ARST 2002 mstorti@spider
%
Volviendo al trabajo despues de las vacaciones. El elemento de
corriente `stream' anda bien, lo probe con una ley de flujo muy
sencilla y verifique que de lo mismo para una corriente lineal (segun
el eje `x') y una ciruclar (media circunferencia de radio `Lx/pi'). 

%===Sun Feb  3 10:12:08 ARST 2002 mstorti@spider
%
Despues de agregarle a `stream' la clase la `channel_shape' y
`friction_law' compila pero no No me linkea:
%
> stream.o: In function `FrictionLaw::FrictionLaw(NewElemset const *)':
> /home/mstorti/PETSC/petscfem/applications/advdif/stream.cpp:187: \
>         undefined reference to `FrictionLaw virtual table'
> collect2: ld returned 1 exit status
%
10:35:51: Probe a poner  `#pragma implementation "Chezy"' pero no
corrige. Probe a poner explicitamente constructores y destructores de
`FrictionLaw' y `Chezy' y tampoco anda. Probe a eliminar `Chezy' y
tampoco anda. 
%
10:40:28: Tambien probe a sacar el `static' de la funcion `factory' y
no anda. 
%
11:15:40: Era facil: faltaba implementar la funcion `flow' o ponerle
`=0'. 

%===Sat Feb  9 23:48:43 ARST 2002 mstorti@spider
%
El `dlopen()' de funciones de extension anda. Resulta que hay que
linkear como si fuera para armar un programa. 
%
11:52:10: ... Es decir, poniendo en la regla para '%.efn' en
`Makefile.base': 
>	${CXX_CLINKER} -g -shared -Wl -o $@ $< $(LDFLAGS)

%===Thu Feb 14 19:42:53 ARST 2002 mstorti@spider
%
Compilado con -O2 tiene unos cuantos problemas. Primero parece haber
problema con la clase `finite state machine' de `pfmat'. Eso se
arregla poniendo que compile con `-O0'. Despues en un test en `lupart'
se queda colgado en un `debug.trace' del main. 
%
19:51:10: Parece que se arregla tambien compilando `debug.cpp' con
-O0. 

%===Fri Feb 15 21:40:26 ARST 2002 mstorti@spider
%
Para resolver el problema creo una rama  `petscfem-optlevel-branch
(branch: 1.1.1.1.24)'. Compilo todo en `src' con -O0 menos `fastmat2'
y `fm2eperl'. 
%
07:30:53: Es bastante desconcertante. Probe a extraer todas las
versiones estables desde la 1.48 a saber las 2.48, 2.60, 2.61, 2.64,
2.76 y 2.82 y correr los tests y hasta la 2.61 da todo bien. Incluso
la 2.61 corre todos los tests OK. Entre la 2.61 y la 2.64 empieza
el drama del error en el PFMat::solve() y justamente es cuando se
agrego todo el tema de la Finite State machine compiler para PFMat. 
%
Probe con compilar todos los archivos que se habian tocado (debug.o,
distmat.o, dofmap.o, elemset.o, fem.o, graph.o, iisdcr.o, pfmatFSM.o,
sparse.o,  spsolve.o,  texthash.o,  util2.o no se porque no esta
pfmat.o) con -O0 y no hay caso. Probe con compilaar con -O0 todos los
archivos en `applications/ns' y tampoco. Probe con linkeditar con -O0
y tampoco, finalmente probe con compilar todos los que faltaban de
`src' y ahi anduvo (sospecho porque recien ahi compile pfmat.o con
-O0). Ahora voy  probar a volver a compilar solo `pfmat.o' con -O0. 
%
08:04:56: Pruebo a compilar ns de 2.64 con -O0 solo para `src/pfmat.o' y
`applications/ns/ns.o' y parece que corre los `oscplate'
%
08:43:49: Voy a ver que pasa con lo `test/aquifer' que no corre,
mientras que los tests `newff' y `burgers' si. 
%
10:53:38: Encontre los errores:
> Basically, there where two problems. `src/pfmat.o' needs to be
> compiled with -O2 and `applications/advdif/diff.o' had a bug related
> to the `glob_param' pointer. Also `test/pfmat/pfmat2.cpp' also needs
> to be compiled with -O0.
Corregidos estos errores corre todos los tests con -O2. 
%
10:59:35: Voy a cerrar la rama `optlevel', voy a agregar las
correcciones a la version `beta-2.84.pl1' y voy a seguir de ahi. 

%===Sat, 30 Mar 2002 09:06:12 -0300 mstorti@spider
%
Corrijo un error en los elementos de Navier-Stokes. El div_u_star
estaba calculado con el `ucols_new', por ejemplo 
%
nsikeps.cpp:717:	  div_u_star = double(tmp10.prod(dshapex,ucols_new,-1,-2,-2,-1));
nsilesther.cpp:526:	  div_u_star = double(tmp10.prod(dshapex,ucols_new,-1,-2,-2,-1));
%
Los pongo a todos en `ucols_star'. 

%===Sat, 13 Apr 2002 17:47:47 -0300 mstorti@spider
%
First fictitious node in `nsi_tet_keps_rot' state is now acceleration
not velocity. 
Fixed a bug in `nsikepsrot.cpp'. `acel_lin' was divided by Dt. Fixed.

%===Wed, 24 Apr 2002 19:36:43 -0300 mstorti@spider
%
Extraniamente el advdif no compila. Comparar con la version que tiene
Rodrigo. 

%===Sat,  4 May 2002 17:25:29 -0300 mstorti@spider
%
En el elemset `nsi_tet_keps_rot' al particionar los elementos
ficticios traen problemas porque hace que todos los elementos esten
conectados con todos. Voy a agregar una funcion que sea `real_nodes'
que retorne el numero de nodos `reales'.

%===Mon May  6 17:17:25 ART 2002 mstorti@node1.beowulf.gtm
%
Corregi un error que al particionar el grafo en `Graph::part()',
(file graph.cpp) cuando habia partes desconectadas del grafo resultaba
que no incrementaba el numero de vertices del grafo no conectada lo
cual llevaba eventualmente a un error. Ahora anda bien, pero
extraniamente en el caso del cilindro lleva a un numero de `ghost
elements' demasiado alto para uno de los procesadores lo cual
desbalancea totalmente. Por otra parte esto es medio extranio porque
normalmente si mal no recuerdo los ghost-elements era solo para cuando
se calculaba el perfil de la matriz. 

%===Mon,  6 May 2002 22:17:32 -0300 mstorti@spider
%
Estudio de performance: Tomando un chunk_size de 1000,
(nsi_tet_keps_rot) en un solo procesador el tiempo de evaluacion del
rediuo y matriz (tomado a nivel del elemento) es de 14.5 secs/ke. La
parte de ensamblaje (los MatAssembly...) aparentemente es muy rapida,
y aparentemente el upload/download tarda unos 3 secs. Juntando todos
los chunks (son 6144 elementos) eso da un total de 105 secs. Despues
el reporte final (para todo el asemble tomado en ns.cpp) es de
115secs. Ahora habria que ver como da todo esto en paralelo. 

%===Tue May  7 08:30:30 ART 2002 mstorti@node1.beowulf.gtm
%
Corriendo en node1, da 9.40secs/Ke (O_c++, nsi_tet_keps_rot), y
despues un promedio global de 10.7secs/Ke. Los assembly time
reportados dan muy bajos. Probablemente lo que se lleva tiempo son los
upload/download. En node12 (Pentium IV da 2.64secs/Ke raw y
3.74secs/Ke promedio), usando chunks de 4000. Total elapsed 26.17
%
Para 6144 elementos. 
PIV  1.4GHz (node12):  raw 3.10secs/Ke, averg  3.8secs/Ke, elapsed: 27.6secs
PIII 866Mhz (node1 ):  raw 9.40secs/Ke, averg 10.7secs/Ke, elapsed: 70.3secs
PIII 500Mhz (node8 ):  no puedo correr porque no entra en la memoria
%
Para 3456 nodos
PIV  1.4GHz (node12):  raw  2.7secs/Ke, averg   3.8secs/Ke, elapsed: 15.2secs
PIII 866Mhz (node1 ):  raw  9.2secs/Ke, averg  10.4secs/Ke, elapsed: 38.4secs
PIII 500Mhz (node8 ):  raw 10.2secs/Ke, averg  12.2secs/Ke, elapsed: 45.9secs
%
10:45:44: Corriendo en paralelo, con pesos 1.3/1/4,
chunk_size=4000. Los chunk_size son 1300,1000,4000. Tenemos una malla
de n=20/nz=10 (12000 eleementos). Los particionamientos en cada
procesador hacen que este bastante bien balanceado porque da tiempos
de entre 8 y 10.2 sec. Las tasas son mas o menos las reportadas
arriba. Los tiempos de upload van desde 1.6-1.8 para todos, menos
10.7secs para el P4. Los assembly time van de 34 a 44 secs. El elapsed
es de 58secs. O sea que del optimo de 40 secs (si corrieramos todo en
el P4) vamos a 58 secs por sincronizacion etc. Vamos a ver si la
perdida por sincronizacion se debe al gran desbalance. Vamos a correr
poniendo un pesos iguales para todos y ver el assembly, upload en ese
caso... 
%
11:04:32: Poniendo todos los pesos a 1 no mejora. Los tiempos de
calculo de residuo van de 3.3 sec para el P4 gasta 12.6 para los
P3-500. Los upload estan en el orden de los 3secs y los assembly estan
todos parejos en el orden de los 58secs. 
%
11:50:36: Tenemos entonces: evaluacion "raw": 8-10 secs, upload 2secs
(10 para el P4), scatter 9secs, assembly 1.5secs, assembly-end 32
secs. Total assembly: 41secs. 
%
13:43:59: En geronimo puedo correr una malla de 26/10. 
%
17:51:57: El problema esta con la matriz A_II. Si no hago el set
values de A_II entonces el tiempo baja de 320secs a 120secs. 

%===Sat, 11 May 2002 10:21:00 -0300 mstorti@spider
%
Pareceria haber demasiados nodos de interfase. Por ejemplo con una
malla 20/1 hay 1200 elementos, particionado en dos procs. da un total
de 11000 grados de libertad de los cuales en proc0 hay approx. 6000
dof, todos locales y en proc1 hay 5000 dofs, 2700 de interfase. Ahora
voy a probar a sacar las condiciones de contorno (para hacer el
analisis mas simple). Desactivo las cond. de contorno con un `#if'. 
%
10:49:35: Sacando las condiciones de contorno tambien particiona muy
mal los nodos. Ahora voy a probar a sacar los elemset `no-fat'. 
%
10:53:17: Con un solo `no-fat' tambien da mal. Ahora voy a probar a
dejar un solo dof por nodo. 

%===Sun, 12 May 2002 12:01:37 -0300 mstorti@spider
%
Creo que encontre el problema. No era un error en el programa sino con
el quilombo de elementos ficticios, `is_fat', nodos ficticios,
etc... Al marcar los elementos de supeficie como `is_fat=0', entonces
los nodos de superficie quedaban desconectados y entonces los asignaba
arbitrariamente a un procesador, por lo tanto quedaba todo un monton
de nodos marcados como interfase. 
%
18:51:14: Con una malla de 20/1 (1200 elementos) en un procesador da
todo bien. Porque los tiempos de calculo de elementos son 28secs para
`nsi_tet_keps_rot' y 28 secs para `bcconv_ns_fm2' y despues reporta un
tiempo de ensamblaje total de 58secs. Despues tarda unos 20secs en
resolver. Ahora voy a probar con una malla con mas `nz' y menos `n'. 
%
19:31:43: Con una malla 12/6 (2592 elementos) da 65'' para evaluar
`nsi_tet_keps_rot' y 15'' para evaluar `bcconv'. No puedo correr con
optimizacion (no se porque). Ahora voy a probar en paralelo. 
%
19:38:19: Corriendo con dos procesadores da 95secs total,
`bcconv_ns_fm2' 17secs y `nsi_tet_keps_rot' 73secs. Lo cual quiere
decir que el ensamblaje y otros elementos se comen solo 5secs, lo cual
esta muy bien. La resolucion se come 100secs. En un solo proc. parece
converger mejor, probablemente porque al mantener el `iisd_subpart' al
pasar a dos procs. se reduce el tamanio de los subdominios y aumenta
el numero de nodos de interfase. 
%
20:15:07: Con optimizacion da total=50, `nsi_tet_keps_rot=45',
`bcconv_ns_fm2=5'. Con lo cual esta muy bien. La resolucion tarda unos
130secs. La verdad es que el perfil de velocidad en el waterline es
bastante ruidoso. 

%===Mon May 13 10:52:11 ART 2002 mstorti@node1.beowulf.gtm
%
Puedo correr `cylinder.epl' con una malla de 30/15 (46939 nodos, 40500
hexas) en el cluster. Tarda 1'40'' evaluacion del residuo y 2'30''
resolucion (300 iter GMRES, baja 4 ord. magn. el residuo). 
%
13:37:43: Guarda que desactivando el bwm da 3'10'' toda la
iteracion. 70 secs. ensamblaje y 2' resolucion. Voy a probar con
tetras. 
%
15:17:41: Con tetras da 75'' el ensamblaje y 2'30'' la
resolucion. Curiosamente da mas que con hexas!!.

%===Fri May 24 14:54:35 ART 2002 mstorti@node1.beowulf.gtm
%
Se corre PETSc-FEM con una malla de 3000 elementos nsi_keps_rot
compilado con -O2-funroll-loops , version beta-2.94.pl18
%
Performance en los Pentium 4:
[proc] - total[sec] - rate[sec/Kelement] rate[Kel/sec]
node1:     30.17      10.05
node3:     12.56       4.18
node9:      9.43       3.14
node12:    10.61       3.53

%===Sun,  2 Jun 2002 20:34:59 -0300 mstorti@spider
%
Atencion: no se porque no esta salvando bien las versiones y las esta
poniendo como 2.94 cuando en realidad son 2.96 o mas. 

%===Sun, 16 Jun 2002 12:14:57 -0300 mstorti@spider
%
Encuentro en un error en `fstack'. Si se hace `close()' y despues
`delete fstack'. 
%
12:22:54: Cuando se hacian dos `close()' seguidos se estaba haciendo
un `fclose()' de un archivo ya cerrado o algo por el estilo. Ahora
cuando se hace el `close()' se hace solo si `file_at_top' no es NULL y
al cerrarlo se pone en NULL. 

%===Sat, 13 Jul 2002 19:20:37 -0300 mstorti@spider
%
Mirando eficiencia corro una malla de 50^3=125,000 elementos hexas con
qharm con `-O2 -funroll-loops' en spider (spider: dyntest 83Mflops
para N=41, 22Mflops asint). Tarda 60 secs en ensamblar (0.5sec/Kelem)
y 160 secs. en resolver el sistema a toleacia rtol=1e-6 con
iisd_subpart=60. Si todo escaleara bien deberia tardar unas 80 veces menos
en un cluster de 10 procesadores P4, o sea 1sec para ensamblar y 2
secs para resolver. Ahora voy a tratar de hacerlo con tetras. (Solo
deberia cambiar significativamente el tiempo de ensamblaje). 
%
21:08:06: Con tetras la malla de 50^3=625K elementos da: 36 secs de ensamblaje
(0.058secs/Ke), 100 secs de resolucion. 
%
22:10:51: Con tetras la malla de 60^3=1.08Melementos da: 62 secs de ensamblaje
(0.058secs/Ke), 276 secs de resolucion. El tiempo de resolucion parece
ser bastante mas que lineal. 
%
22:18:35: Vamos a probar hasta donde se puede llegar. 

%===Sun, 14 Jul 2002 13:16:13 -0300 mstorti@spider
%
Reflotando el fractional-step. Compila. 
%
20:11:04: En un momento (circa Nov 1 2001) yo anule el tema de 
cargar los perfiles segun la matriz devuelta sino que directamente
usaba una mascara (la misma para todos los elementos). 

%===Sat, 20 Jul 2002 23:46:42 -0300 mstorti@spider
%
Estoy tratando de ver porque consume tanta memoria el armado del
grapho y encima despues no lo libera. 

%===Mon Jul 22 08:42:16 ART 2002 mstorti@minerva
%
Escribi una nueva clase grafo basada en una clase de vector
dynamic dvector<>. Ahora parece que consume mucho menos memoria pero
todavia no se si la libera o no. Verifique que los resultados son los
mismos en la cavidad cubica con una malla de 4x4x4. Ahora voy a
verifica si por lo menos entra en el liberador de memoria
(lgraph->clear()). 
%
15:05:26: Con graphdv_dis pasa de la etapa de calculo de perfile con
una RSS de 220MB pero tarda como 2horas y media. Ahora voy a probar
con `link_graph'. 
%
16:33:00: Corriendo con 30^3 con link_graph va a los pedos, pero
iniciamente aloca unos 90 megas que despues no se si los libera y
asintoticamente ocupa unos 270 Mb, cuando para mi deberia ocupar unos
60 MB las matrices sin factorizar. 
%
17:28:39: No estaba limpiando los grafos en
`link_graph::clear()'. Ahora la malla de 30^3 consume unos 158MB. 

%===Mon, 22 Jul 2002 19:05:00 -0300 mstorti@spider
%
Con la clase `link_graph' se puede correr una malla de 50^3 con
512MB (creo). Ahora hay que hacer el scatter. 

%===Wed Jul 24 12:34:25 ART 2002 mstorti@minerva
%
La clase de grafo distribuida esta funcionando bien, pero hay un
problema en el cual se plantea una disyuntiva en cuanto a como
funcionan los iterators de la clase base en `DistCont<>' cuando se
hace un `erase()' de un dado elemento. Para `maps' y listas el
iterator es borrado y no hay que actualizarlo, mientras que para
vectores hay que avanzarlo cuando es borrado. Todavia no se como
hacers para que quede en forma automatica. 

%===Wed, 24 Jul 2002 21:11:35 -0300 mstorti@spider
%
Esta andando la nueva representacion de grafo
`LinkGraphWrapper'. Puedo correr en minerva el problema de 
>      Etapa   |     sub-etapa           |  termina(reloj)| diff(t) | Nro etapa | 
> 
>   -PREDICTOR-   residual computation... --  [t=262.638]   263.19   1 
>   -PREDICTOR-   solving linear system... -- [t=525.896]     4.80   3
>   -POISSON-     matrix computation... --    [t=530.748]    90.53   5
>   -POISSON-     residual computation... --  [t=621.297]    68.84   7
>   -POISSON-     solving linear system... -- [t=690.159]   264.23   9
>   -PROJECTION-  matrix computation... --    [t=954.451]   106.30  11
>   -PROJECTION-  residual computation... --  [t=1060.776]   79.03  13
>   -PROJECTION-  solving linear system... -- [t=1139.876]    5.83  15
> Primer paso: 886secs						  
> 								  
>   -PREDICTOR-  residual computation... --   [t=1148.656]   263.0   17
>   -PREDICTOR-  solving linear system... --  [t=1411.675]     4.9   19
>   -POISSON-    residual computation... --   [t=1416.786]    69.6   21
>   -POISSON-    solving linear system... --  [t=1486.415]    96.1   23
>   -PROJECTION- residual computation... --   [t=1582.726]    81.0   25
>   -PROJECTION- solving linear system... --  [t=1663.779]     5.0   27
> Segundo paso: 523secs						  
> 								  
>   -PREDICTOR-  residual computation... --   [t=1668.957]   263.4   29
>   -PREDICTOR-  solving linear system... --  [t=1932.435]     4.7   31
>   -POISSON-    residual computation... --   [t=1937.192]    69.3   33
>   -POISSON-    solving linear system... --  [t=2006.542]    95.0   35
>   -PROJECTION- residual computation... --   [t=2101.716]    79.6   37
>   -PROJECTION- solving linear system... --  [t=2181.356]     5.1   39
> Segundo paso: 517secs
> 
%
22:18:20: Conclusiones: 1/3 del tiempo en el primer paso y 1/2 de los
siguientes (160secs) se va en calculo del residuo y matriz. Lo que se
ahorra en las iteraciones n>1 es el ensamblaje de las matrices
(90+106\approx 200 secs) y la diferencia de no factorizar las matrices
unos 160 secs. Eso da unos 350secs de diferencia. 
%
22:23:45: Puedo correr la cavidad cubica con 50^3 con 20 subdominios
en minerva, llega a ocupar unos 370 MB. Tarda unos 660secs (11'). Voy
a probar a incrementar el iisd_subpart. Consume unos 377MB. 
%
Con iisd_subpart=10
%
-- residual computation... -- [22:11:13    184.128]       36.204
-- solving linear system... -- [22:11:49    220.332]     624.739
-- residual computation... -- [22:22:14    845.071]       37.247
-- solving linear system... -- [22:22:51    882.318]     634.037
-- residual computation... -- [22:33:25   1516.355]       36.790
%
%
22:50:10: laplace con iisd_subpart=100. Requiere 128MB
%
--  residual computation... -- [22:42:49    184.357]      36.6
--  solving linear system... -- [22:43:26    220.981]	  73.4
--  residual computation... -- [22:44:39    294.458]	  37.0
--  solving linear system... -- [22:45:16    331.475]     80.0
%
23:19:47: Puedo correr Laplace en una malla de 70^3 = 357911 nodos
(1715000 tetras) con 300 subdos en spider. Llega a ocupar unos 350MB y
260secs resol (factor/backsubs) y 102 sec eval. La malla tiene un
refinamiento de hratio=5. 
%
23:24:58: Creo que puedo llega a los 2millones de tetras. En la
iteracion 2 llega a las 100 iteraciones y consume 354MB. Con la malla
de 70^3 tarda 100sec evaluar y 160sec resolver. 
%
> -- Before residual computation... -- [23:13:19    509.403]
> -- Before solving linear system... -- [23:15:02    611.692]
> -- Before residual computation... -- [23:19:23    872.970]
> -- Before solving linear system... -- [23:21:07    977.667]
%
00:22:46: Puedo correr con 75^3 (2,109,375 elementos). Requiere maximo
unos 460MB, con krylov de 100 (iisd_subpart=300, tal vez se podria
aumentar mas). Los tiempos son 130secs eval, 450secs resol. 

%===Thu Jul 25 11:36:39 ART 2002 mstorti@node1.beowulf.gtm
%
Puedo correr malla de 50^3 (625000 tetras) con nsi_tet_les_fm2 en geronimo con 5
procs, usa del orden de 140MB por nodo. Tarda unos 105+30 secs por
iteracion. 
%
> --  residual computation... -- [11:27:10    236.669]     105.180
> --  solving linear system... -- [11:28:55    341.849]     30.195
> --  residual computation... -- [11:29:26    372.044]     108.410
> --  solving linear system... -- [11:31:14    480.454]     28.136
> --  residual computation... -- [11:31:42    508.590]     108.694

%===Fri Jul 26 13:30:30 ART 2002 mstorti@minerva
%
Anda mas o menos el fractional step con FastMat2 (diverge despues de
unos cuantos pasos de tiempo). Ahora voy a ver un poco el tema de la
eficiencia. 
%
13:53:04: Informe de performance FastMat2/Newmat. Diferentes sub-steps
del fractional step. Tasas en secs/Kelem
%
                                        FastMat2          Newmat
Predictor         (comp_res_mom):        0.37            2.31
Matriz Poisson    (comp_mat_poi):        0.15            0.79
Residuo Poisson   (comp_res_poi):        0.058           0.71
Matriz Projection (comp_mat_prj):        0.157           0.66
Residuo Projection(comp_res_prj):        0.162           0.80
%
Lo cual en general da una relacion casi 10 a 1 a favor de FastMat2. 
%
14:10:21: verificando los diferentes terminos del fractional step. La
matriz de predictor esta bien. Inicialmente el residuo es 0 en los dos
casos. 
%
14:14:14: El paso de poisson tambien coincide. 

%===Fri Jul 26 18:48:01 GMT+3 2002 mstorti@spider
%
Esta andando bien el procsel. 
%
20:44:21: puedo correr malla de 30^3 con fractional step en spider.
%
21:11:15: para la malla de 10^3 con laplace el
`use_full_diagonal_preco' mejora la convergencia en un factor 2 (hace
la mitad de iteraciones). 
%
21:23:13: Con 30^3 tambien baja a la mitad. Sin embargo el costo es
mayor, aparetemente porque itera demasiado en la interfase. 
%
23:38:17: Con una malla de 50^3 tarda unos 37secs eval y 80secs resol
con `use_interface_full_preco', mientras que tarda 120secs sin
`use_interface_full_preco'. 
%
23:45:37: Aumentando el iisd_subpart a 100 (estaba en 50) el tiempo de
resolucion baja a 70 con `use_interface_full_preco=0' y a 50secs con
`use_interface_full_preco=1'. Las iteraciones bajan a la mitad.  
%
00:05:49: Con NS tarda unos 70secs de resolucion con una malla de 26^3
y `iisd_subpart=100' y con `use_interface_full_preco=1'. Tarda entre
110 y 125 secs con `use_interface_full_preco=0'.

%===Sat, 27 Jul 2002 10:37:04 -0300 mstorti@spider
%
Voy a escribir el gpdata para el prismatic element. 
%
15:35:31: Ya esta hecho. Tal vez habria que verificar que no pierda
memoria, pero seria medio extranio, porque no se crea ningun objeto
adentro. (Se crean dos pero no con new). 
%
15:36:38: Hay un efecto extranio con el `use_interface_full_preco'. Si
no se resuelve bien el sistema lineal entonces la solucion no
converge. 
%
17:45:15: Parece que se debe a precondicionar con GMRES (o cualquier
otro metodo `no-estacionario'). Por eso cambio a `KSPRICHARDSON' pero
no converge entonces le agrego una factor de relajacion 0.5, que
despues voy a dejar que se pueda entrar como una opcion. 
%
21:18:07: Navier-Stokes en una malla de 10^3*2 (prismatic)
`nsi_tet_les_fm2' elements tarda 13secs para resolver con
`use_interface_full_preco=1' y 27secs con
`use_interface_full_preco=0'. La tolerancia es `rtol=1e-8' y se hacen
10iter en la interface con relajacion =0.5.

%===Sun, 28 Jul 2002 09:23:14 -0300 mstorti@spider
%
Corro el fractional step con la cavidad cubica 30^3*2 (prismatic) y
diverge lentamente, puede ser que sea por el paso de tiempo (Dt=0.1)
cuando hasta ahora siempre habia corrido con Dt=0.01. 
%
22:33:14: Added flag `reuse_proj_mat', if deactivated then we
recompute the projection matrix each time. 

%===Mon Jul 29 16:24:35 ART 2002 mstorti@minerva
%
Ahora TextHashTable `GLOBAL_OPTIONS', esta definido en `texthash.cpp'
y declarado en `texthash.h'. 

%===Thu,  8 Aug 2002 14:09:33 -0300 mstorti@minerva
%
Anda el `embedded_gatherer' + `visc_force_integrator'. Es menos
preciso que calcular las reacciones en los nodos. Se logra mejor
precision refinando hacia las paredes. 

%===Sat, 17 Aug 2002 12:40:14 -0300 mstorti@spider
%
Starts including an example of the nutating cylinder in the test
suite. However, I will try to make it work with a relatively small
mesh. I think at W_spin=9000rpm, W_nut=630rpm, nu=1e5 cSt, nut_angle =
15deg. The despin moment from the paper of Vaughn is 1.55 lbf ft = 2.1
N m. 
%
12:49:39: Con una malla de 3*6^2*20 da Mz_raw = -1.7877e-02 N m, lo
cual es mucho. 
%
12:51:45: Para nut_angle = 0, da Mz_raw = -1.53276e-02  N m, lo cual
por diferencia da (1.7877e-02 -1.53276e-02)*950*.738 = 1.7874 lbf ft,
que esta mucho mas cerca. Voy a probar a refinar mas a ver si se
acerca. 
%
13:31:15: Con una malla mas fina (3*10^2*20) da -8.823945e-03
(nut_angle=15) y -6.736648e-03 (nut_angle=0). Entonces la fuerza da 
(8.823945e-03-6.736648e-03)*950 = 1.9829, lo cual esta bien. 
%
19:10:34: Vamos a hacer el test con una malla de 3*4^2*10 con la cual
la fuerza da (5.3970598663e-02-5.2234708713e-02)*950*.738 = 1.2170 lbf
ft. 

%===Thu Aug 22 10:47:46 ART 2002 mstorti@node1.beowulf.gtm
%
Corridas en geronimo con 5 Pentium 4.
malla de 40^3*5 = 320000 tetras. NS, Re=200, Krylov=500
%
Eval. 54 secs.
Resol. 36secs [131 iters]
%
13:35:59: En un solo procesador (el 9, con 512MB RAM) no entra con 129
subdominios (nelem_subdo=500). Corro con N=30 (27,000 nodos, 135,000
elementos). 
%
Con 55 subdos, tarda en un proc. 53.8 sec eval, 31.5sec
resol. (rtol=1e-8, 90 iter). 
%
Con los 5 P4 da 24sec eval y 11.6sec resol. Hay un desbalance?  
%
14:11:43: Con pesos
%
node9     9.0842
node3 	  6.1246
node7 	  8.8746
node10	  7.9754
node12	  7.5813
%
da un tiempo de 19.8secs eval, 11.2sec resol. Lo cual da un speed-up
de 19.8/53.8=2.7 y eficiencia de 62%. 
%
Con un solo procesador el calculo de residuos da 0.38secs/Ke. Voy a
probar a aumentar el numero de nodos a ver si asi mejora la
eficiencia. 
%
14:31:00: Con N=40 da 46secs eval, 34 secs resol. 
%
La eficiencia teniendo en cuenta el desbalance es 
%
t(1[en node9])/t(5) = 53.8/19.8 = 2.71
%
y la eficiencia es 2.71/4.36 = 62%, donde 4.36 es la suma de los pesos
de los procesadores. 

%===Fri Aug 23 08:50:34 ART 2002 mstorti@node1.beowulf.gtm
%
Con 135,000 hexas (malla de 30^3), con solver PETSc da 17.6 secs eval,
118.8 secs resol. (421 Krylov). (Con 5 procs y los pesos
"optimizados"). 
%
09:03:04: En un solo proc, da 49.2 secs eval y 327.5 secs resol. Lo
cal da una eficiencia de 49.2/17.6/4.36= 64%, speedup de 49.2/17.6 =
2.8. (Tampoco es tan bueno) para la parte de eval. Para resol da 63%
eficiencia tambien. 
%
11:19:31: Para la malla de 30^3 el numero de elementos scattereados en
A_LL_other son del orden de 50,000 (me parece que es mucho!!) y la
mayor parte del tiempo se va en el scatter de la matriz (entre el
begin() y el end()). Voy a probar con una malla mas grande a ver si el
numero de elementos scattereados crece (si no escalea). 
%
11:44:31: para la malla de 50^3 son 138289 elementos scattereados. Lo
que mas tarda es el lazo sobre I-J en IISDMat::assembly_begin_a. Es
curioso porque tarda incluso si no hay que escatterear elementos. 
%
11:53:22: El numero de elementos scattereados es 48042 para la malla
de 30^3. Lo cual esta en la misma relacion que el refinamiento al
cuadrado: (50/30)^2 = 2.77, (138k/48k)=2.87
%
13:31:24: Error: el mayor tiempo se pierde en el
`A_LL_other->scatter();' 

%===Tue, 27 Aug 2002 10:39:40 -0300 mstorti@minerva
%
Como hacer la compilacion con `#pragma imlementation/interface':
%
> From: Peter Englmaier (ppe@nospam.org)
> Subject: Re: compiling templates
> Newsgroups: gnu.g++.help
> Date: 1998/11/07       View: Complete Thread (19 articles) | Original Format
> 
> Here is an example. Is that what you need?
> Peter.
> 
> >>>>Makefile: (for gnu make)
> CPPFLAGS=-g -frepo -fexternal-templates
> LDFLAGS=-lg++
> 
> user: user.o stack.o
>         g++ -o user $^
> 
> clean:
>         rm -f *.o *~ user
> 
> stack.o user.o: stack.h
> 
> >>>>>user.C:
> #include <iostream>
> #include "stack.h"
> 
> Stack<char> sc(2);
> 
> int main()
> {
>   Stack<int> si(2);
> 
>   try {
>     sc.push('a'); sc.push('b');
>     si.push(5);   si.push(3);
>     cout << "Read back from stack: " << sc.pop() <<
>     si.pop() << si.pop() << sc.pop() << endl;
> 
>     cout << "Read beyond " << sc.pop() << endl;
>   } catch (underflow_error) { cout << "Underflow !\n";
>   }
> 
> 
> }
> 
> >>>>>stack.C
> // create a stack class for type T
> #pragma implementation
> #include "stack.h"
> 
> template<class T> Stack<T>::Stack(int size)
> {
>   top = 0;
>   max_size = size;
>   v = new T[size];
> }
> 
> // destructor
> 
> template<class T> Stack<T>::~Stack()
> {
>   delete [] v;
> }
> 
> // push one element on top of stack
> 
> template<class T> void Stack<T>::push(T c)
> {
> 
>   if (top == max_size) throw overflow_error();
>   v[top++] = c;
> }
> 
> // pop one element from top of stack
> 
> template<class T> T Stack<T>::pop()
> {
>   if (top == 0) throw underflow_error();
>   return v[--top];
> }
> >>>>>>>stack.h:
> #include <stdexcept>
> // create a stack class for type T
> #pragma interface
> // a generic stack class
> 
> template<class T> class Stack {
>   T* v;          // stack array
>   int max_size;  // size of stack array
>   int top;       // points to first free element
>  public:
>   class underflow_error {  };
>   class overflow_error {  };
> 
>   Stack (int size); // constructor for Stack of size 'size'
>   ~Stack();         // destructor
> 
>   void push(T);
>   T pop();
> };
> 
> typedef Stack<int> Stackint;
> typedef Stack<char> Stackchr;

%===Thu Aug 29 10:16:49 ART 2002 mstorti@node1.beowulf.gtm
%
Corriendo el cylinder detecto una perdida de memoria. En 100 pasos
pasa de un pico de memoria de 130 a 230 y al poco revienta. Voy a
probar a correrlo con una version anterior. Vamos a probar con la
version 3.01 que es estable. 
%
11:01:35: Con la version 3.03 no pierde memoria. Puede ser que sea en
el mydetsur que fue modificado? 
%
12:10:54: Con la version 3.03.pl4 no pierde. 
%
12:33:42: No guarda que seguia corriendo con la 3.03 en vez de la
3.03.pl4 asi que no esta verificado que esta no pierda memoria. Ahora
ya compile la 3.03.pl7 asi que voy a probar con esa. 
%
13:00:19: beta-3.03.pl7 SI pierde memoria
%
13:00:19: beta-3.03.pl4 NO pierde memoria. 
Usar:  $ perl -ne 'print "$1\n" if m|node\d*\s*(\S*)/|;' < bwm.log > pp
%
13:59:54: beta-3.03.pl5 NO pierde memoria. 
%
14:39:07: beta-3.03.pl6 SI pierde memoria. 
%
15:53:33: Efectivamente verifico que la pl6 si pierde y la pl5 no
pierde. Basicamente la diferencia esta en el agregado del cache para
mydetsur. Las modificaciones estan en src y
applications/ns-advdif.. Voy a probar a volver applications/ns a su
version original (con mydetsur). 

%===Thu, 29 Aug 2002 21:02:25 -0300 mstorti@spider
%
En spider, con un problema no tan grande corriendo con dos procesos
parece tambien perder memoria con la beta-3.03.pl6. Ahora voy a
probar a usar en applications/ns la 3.03.pl5. 
%
21:23:57: El comando ahora es 
$ perl -ne "print \$1' if m|cache:\s*(\S*)\s|;" < bwm.log > pp
%
21:28:18: Pareceria que decididamente la .pl6 pierde mientras que la
.pl5 no. 
%
22:14:03: Coriendo el test `test/testfm2.cpp' pareceria no
perder. Pongo chequeos en `FastMat2::detsur()' para ver si no esta
recreando el cache, pero no detecto ninguna inconsistencia. 
%
22:18:25: Corriendo en un solo procesador tambien pierde. 
%
22:26:18: Parece que pierde cuando no se usan los caches. Parece que
alguno de los elemsets de superficie tiene desactivado el cache y
entonces ahi es donde se pierde la memoria. Esta mal la logica de los
`subcache' cuando no se usan los cache. 
%
22:36:31: Creo que esta resuelta la perdida de memoria. Ahora hay que
ver porque hay algun elemento de sueprficie que no esta cacheado. 

%===Fri, 30 Aug 2002 10:25:50 -0300 mstorti@minerva
%
Guarda que para el elemento nsikeps/hexa son nel=10, ndof=6, son 60
dof por elemento, lo cual hace que para un chunk_size de 1000 tenemos
un vector de retorno para matrices de chunk_size*ndoft*ndoft*8/1e6 =
28Mb. Mas adelante no va a ser necesario almacenar temporariamente las
matrices (se va a ir uploadando cada matriz a la vez) asi que este
problema no va a existir. Pero mientras tanto lo que se puede hacer es
no alocar para los nodos ficticios. En este caso se reduciria
muchisimo y mas todavia si no tenemos en cuenta los grados de libertad
fijos como k y epsilon. 

%===Fri Aug 30 17:00:45 ART 2002 mstorti@node1.beowulf.gtm
%
Con los dos nuevos procs. (node8 y node11, P4 1.7GHz, 512MB RAM) corro
la cav. cub. con una malla de 50^3*5 = 625000 tetras en los 3
procs. que tienen 512MB RAM. Usa unos 350MB por proc., tarda 103secs
eval y 104 secs resol. (120 Krylov, iisd_subpart=84). 
%
17:20:03: Corriendo con los 7 procs da 70 secs eval, 60 secs resol
(hay que balancear mejor). Los nodos usan un maximo de 180MB. El
iisd_subpart=36. 
%
17:55:04: Con fractional step se puede correr 50^3 con 200MB por proc,
tarda 130secs/Dt. 

%===Mon,  2 Sep 2002 10:01:50 -0300 mstorti@minerva
%
> %
> 22:33:19: Siempre con fracstep se puede correr con 60^3 (1,080,000
> tetras). Tarda 240sec/paso de tiempo. 
> %
> PREDICTOR-  residual computation.    =    75.08
> PREDICTOR-  solving linear system.   =     7.99
> POISSON-  residual computation.      =    11.38
> POISSON-  solving linear system.     =    64.49
> PROJECTION-  matrix computation.     =    39.05
> PROJECTION-  residual computation.   =    40.85
> PROJECTION-  solving linear system.  =     2.56 
> Total                                =   241.43
> 
> %===Sat Aug 31 09:23:27 ART 2002 mstorti@node1.beowulf.gtm
> %
> Con los 7 procs desbalanceados tarda 232secs en hacer una malla de
> 65^3*5=1437480 tetras. 
> PREDICTOR-  residual computation.   =   65.52
> PREDICTOR-  solving linear system.  =    7.43
> POISSON-  residual computation.     =   10.55
> POISSON-  solving linear system.    =   75.22
> PROJECTION-  matrix computation.    =   35.02
> PROJECTION-  residual computation.  =   36.65
> PROJECTION-  solving linear system. =    2.14
> Total                               =  232.57

%===Thu,  5 Sep 2002 17:26:35 -0300 mstorti@minerva
%
Actualizando para petsc-2.1.3
> 
> perl -pi.bak -e 's/sles\.h/petscsles.h/g'; `find . -name '*.h' | xargs grep -l 'sles\.h' `
> perl -pi.bak -e 's/sles\.h/petscsles.h/g;' `find . -name '*.h' | xargs grep -l 'sles\.h' `
> perl -pi.bak -e 's/vec\.h/petscvec.h/g;' `find . -name '*.h' | xargs grep -l 'sles\.h' `
> perl -pi.bak -e 's/sles\.h/petscsles.h/g;' `find . -name '*.h' | xargs grep -l 'sles\.h' `
> perl -pi.bak -e 's/sles\.h/petscsles.h/g;' `find . -name '*.cpp' | xargs grep -l 'sles\.h' `
> perl -pi.bak -e 's/vec\.h/petscvec.h/g;' `find . \( -name '*.cpp' -o -name '*.h' \) | xargs grep -l 'vec\.h' `
> perl -pi.bak -e 's/petscpetscsles/petscsles/g;' `find . \( -name '*.cpp' -o -name '*.h' \) | xargs grep -l 'petscpetscsles' `
> perl -pi.bak -e 's/mat\.h/petscmat.h/g;' `find . \( -name '*.cpp' -o -name '*.h' \) | xargs grep -l 'mat\.h' `
> perl -pi.bak -e 's/petscmat\.h/mat.h/g;' `find . \( -name '*.cpp' -o -name '*.h' \) | xargs grep -l 'petscmat\.h' `
> perl -pi.bak -e 's/<mat\.h/<petscmat.h/g;' `find . \( -name '*.cpp' -o -name '*.h' \) | xargs grep -l 'mat\.h' `
> perl -pi.bak -e 's/adpetscvec\.h/advec.h/g;' `find . \( -name '*.cpp' -o -name '*.h' \)`
> perl -pi.bak -e 's/(\W)Scalar(\W)/$1PetscScalar$2/g;' `find . \( -name '*.cpp' -o -name '*.h' \)`
> perl -pi.bak -e 's/(\W)Scalar(\W)/$1PetscScalar$2/g;' `find . \( -name '*.cpp' -o -name '*.h' \)`
> perl -pi.bak -e 's/(\W)Viewer(\W)/$1PetscViewer$2/g;' `find . \( -name '*.cpp' -o -name '*.h' \)`
> perl -pi.bak -e 's/VIEWER/PETSC_VIEWER/g;' `find . \( -name '*.cpp' -o -name '*.h' \)`
> history | grep perl | tail
>  perl -pi.bak -e 's/OptionsGetString/PetscOptionsGetString/g;' `find . \( -name '*.cpp' -o -name '*.h' \)`
> history | grep perl | tail
> perl -pi.bak -e 's/PETSC_VIEWER_FORMAT_ASCII_MATLAB/PETSC_VIEWER_ASCII_MATLAB/g;' `find . \( -name '*.cpp' -o -name '*.h' \)`
> 

%===Fri,  6 Sep 2002 11:51:59 -0300 mstorti@minerva
% From Makefile.defs
From version beta-3.04 on, PETSc-FEM must be compiled with PETSc 2.1.3. 
For compiling older versions (that compile with PETSc 2.0.24) you may want
probably to have several versions of PETSc compiled. You put all the versions
in a directory pointed by `PETSC_ROOT_DIR' and then versions of PETSc-FEM newer
than beta-3.04 will specify the version of PETSc needed through the variable
`PETSC_VERSION_USED_BY_PETSCFEM'
%
ifneq ($(PETSC_VERSION_USED_BY_PETSCFEM),)
PETSC_DIR=$(PETSC_ROOT_DIR)/petsc-$(PETSC_VERSION_USED_BY_PETSCFEM)
endif
%
12:52:03: Corriendo ROCKET/ident no da lo mismo la nueva version de
PETSc. La diferencia empieza siendo pequenha pero despues
aumenta. Pareceria estar relacionado con un mal condicionamiento de la
matriz. 

%===Fri,  6 Sep 2002 21:00:36 -0300 mstorti@spider
%
Corriendo con las versiones 3.04 (actual) y 3.03.pl15 (vieja) da
exactamente lo mismo (merr=0) si usamos LU. Ahora voy a probar con `iisd' en
un solo proc.  
%
21:15:09: La diferencia es grande, incluso para una pequenha matriz
usando iisd (iisd_subpart=2). (malla de 3*4^2*4). Ahora voy a tratar sin preco. 
%
21:21:42: Sin preco la diferencia es menor pero sique existiendo. 
%
21:26:38: Con matrices PETSc, tambien converge bastante mejor la
solucion con la version vieja y despues hay diferencia en la
solucion. 

%===Sat,  7 Sep 2002 12:26:46 -0300 mstorti@spider
%
Resuelto el problema. El metodo de ortogonalizacion, ksp->orthog
(puntero a funcion) por default el "iterative refined" (IR)
(`IROrthogonalization') en petsc 2.0.24, aunque en la documentacion
decia que era por default el
`UnmodifiedGramSchmidtOrthogonalization'. En la version 2.1.3 de PETSc
paso a ser efectivamente por default el
`UnmodifiedGramSchmidtOrthogonalization'. Aprentemente es mucho mas
estable el IR, asi que lo hemos puesto por defecto en
`PFPETScMat::build_sles()'. 

%===Sat,  7 Sep 2002 17:56:40 -0300 mstorti@spider
%
El `make clean' de PETSc borra los archivos `g_*' con lo cual borraba
el test/sqcav/g_body.ans.txt' por lo cual lo renomine a
`gbody.ans.txt'. 

%===Mon,  9 Sep 2002 21:23:10 -0300 mstorti@spider
%
`applications/ns.cpp' no compila correctamente con -O2, se queda
compilando por mas de 4 min. Asi que le pongo -O0. 

%===Thu, 12 Sep 2002 10:17:52 -0300 mstorti@minerva
%
Detectado un bug en IISDMat. El sles para resolver el problema de
interface era creado en el communicator PETSC_COMM_SELF mientras que
debia serlo en el `comm' general del operador (pfmat.comm). 

%===Thu Sep 12 11:51:34 ART 2002 mstorti@node1.beowulf.gtm
%
Con `nsi_tet_les_fm2' se puede correr con los nuevos nodos
(asignandoles pesos mas bajos a los que tienen 256MB para optimizar el
uso de la memoria) se puede correr 1,080,000 (5*60^3) elementos en la
cavidad cubica. Con el nodo11 hay un problema que da una carga
desmesurada asi que por ahor lo saco. 
Tenemos 110secs eval y 138 secs resol (rtol=1e-8,
maxits=500). Re=200). 
%
113secs eval, 120secs resol. 
%
13:32:03: Con 5*64^3=1.3Melem, consume 490MB~max (4 x peso=4. procs + 4
x peso=2. procs). 
%
-- After computing profile. -- [13:23:06    396.407]
-- After residual computation. -- [13:26:05    575.507]
-- After solving linear system. -- [13:28:36    726.902]
-- After residual computation. -- [13:31:47    917.808]
-- After solving linear system. -- [13:34:46   1096.700]
%
180 secs eval, 160secs resol. (150-170 iters). 

%===Thu Sep 12 20:12:09 ART 2002 mstorti@node1.beowulf.gtm
%
Con fractional step puedo correr con 1.8Melem. (rtol=1e-4)
%
> PREDICTOR-  residual computation.   =   77.97
> PREDICTOR-  solving linear system.  =    5.65
> POISSON-  residual computation.     =   12.03
> POISSON-  solving linear system.    =   26.66
> PROJECTION-  matrix computation.    =   40.90
> PROJECTION-  residual computation.  =   44.02
> PROJECTION-  solving linear system. =    0.97
> Total                               =  208.23 secs
%
%===Fri Sep 13 10:51:10 ART 2002 mstorti@node1.beowulf.gtm
%
Con 2.2Melem (sin node 11), 
%
Proctable=
node1         1.3   server
node2         4.2
node4         4.2
node9         4.
node8         4.2
node3         1.6
node7         1.6
node10        1.6
node12        1.6
%
> PREDICTOR-  residual computation.   =   106.2873
> PREDICTOR-  solving linear system.  =    13.5177
> POISSON-  residual computation.     =    15.1890
> POISSON-  solving linear system.    =   169.9040
> PROJECTION-  matrix computation.    =    66.0503
> PROJECTION-  residual computation.  =    69.7543
> PROJECTION-  solving linear system. =     2.7333
> Total: 443.44
%
% Agregando el nodo 11 con peso 2.2
%
> PREDICTOR-  residual computation.   =  174.12 (sigma  17.66)
> PREDICTOR-  solving linear system.  =   26.24 (sigma   1.05)
> POISSON-  residual computation.     =   27.48 (sigma   2.25)
> POISSON-  solving linear system.    =  229.47 (sigma  19.52)
> PROJECTION-  matrix computation.    =  125.78 (sigma  15.63)
> PROJECTION-  residual computation.  =  134.89 (sigma  14.67)
> PROJECTION-  solving linear system. =    5.14 (sigma   0.09)
> Total                               =  723.11
%
O sea que tarda el doble. 
%
11:05:32: Pruebo a intercambiar los nodos 11 y 8 (deberian ser
equivalentes). 
%
12:16:57: Aparentemente intercambiando los nodos el que sigue jodiendo
es el nuevo 11 (o sea que el problema en principio no esta con el
hardware). Le corro el nettest para ver si anda bien la placa,
etc... y parece andar todo bien. (Da velocidades del orden de
90Mbit/sec). 
%
12:26:02: Empiezo a correr con una malla de 60^3*5, para que no tarde
tanto. 
%
% Sin el nodo 11
%
> PREDICTOR-  residual computation.   =  27.20 (sigma    0.72)
> PREDICTOR-  solving linear system.  =   1.37 (sigma    0.00)
> POISSON-  residual computation.     =   4.07 (sigma    0.00)
> POISSON-  solving linear system.    =   8.94 (sigma    0.37)
> PROJECTION-  matrix computation.    =  14.26 (sigma    0.03)
> PROJECTION-  residual computation.  =  15.25 (sigma    0.01)
> PROJECTION-  solving linear system. =   0.53 (sigma    0.00)
> Total                               =  71.63
%
% Con el nodo 11
%
> PREDICTOR-  residual computation.   =   29.48 (sigma    0.89)
> PREDICTOR-  solving linear system.  =	   3.83 (sigma    0.05)
> POISSON-  residual computation.     =	   5.79 (sigma    0.12)
> POISSON-  solving linear system.    =	  46.75 (sigma    2.66)
> PROJECTION-  matrix computation.    =	  18.00 (sigma    0.26)
> PROJECTION-  residual computation.  =	  19.87 (sigma    1.29)
> PROJECTION-  solving linear system. =	   1.49 (sigma    0.01)
> Total                               =  125.20          
%
% Sin el nodo 8:
%
> PREDICTOR-  residual computation.   =  32.92 (sigma    0.00)
> PREDICTOR-  solving linear system.  =   5.16 (sigma    0.00)
> POISSON-  residual computation.     =   6.06 (sigma    0.00)
> POISSON-  solving linear system.    =  67.05 (sigma    0.00)
> PROJECTION-  matrix computation.    =  17.25 (sigma    0.00)
> PROJECTION-  residual computation.  =  19.93 (sigma    0.00)
> PROJECTION-  solving linear system. =   1.31 (sigma    0.00)
> Total                               = 149.69
%
% Con todos los nodos (el viejo 11 -> 8 ahora es el 5):
%
> PREDICTOR-  residual computation.   =  28.32 (sigma    0.11)
> PREDICTOR-  solving linear system.  =   4.04 (sigma    0.02)
> POISSON-  residual computation.     =   4.19 (sigma    0.01)
> POISSON-  solving linear system.    =  50.85 (sigma    4.20)
> PROJECTION-  matrix computation.    =  15.22 (sigma    0.05)
> PROJECTION-  residual computation.  =  16.49 (sigma    0.03)
> PROJECTION-  solving linear system. =   1.08 (sigma    0.00)
> Total                               = 120.17
%
%
17:14:45: Saco el nodo5 y vuelvo a los 72sec/iter. 
%
17:36:50: Le cambio la placa al nodo5 de forma de que ahora es el
nodo6. Los tiempos vuelven a ser buenos (aunque no hay ganancia). 
%
> PREDICTOR-  residual computation.   =  25.51 (sigma    0.44)
> PREDICTOR-  solving linear system.  =   1.57 (sigma    0.20)
> POISSON-  residual computation.     =   3.89 (sigma    0.02)
> POISSON-  solving linear system.    =  14.69 (sigma    0.00)
> PROJECTION-  matrix computation.    =  13.61 (sigma    0.04)
> PROJECTION-  residual computation.  =  14.74 (sigma    0.00)
> PROJECTION-  solving linear system. =   0.49 (sigma    0.00)
> Total                               =  74.50
%
%
17:48:04: Vuelvo a correr sin el nodo 5 para ver si en realidad no se
bajan los tiempos y obtengo:
%
> PREDICTOR-  residual computation.   =  28.57 (sigma    0.57)
> PREDICTOR-  solving linear system.  =   1.64 (sigma    0.14)
> POISSON-  residual computation.     =   4.11 (sigma    0.00)
> POISSON-  solving linear system.    =  11.86 (sigma    4.57)
> PROJECTION-  matrix computation.    =  15.28 (sigma    0.00)
> PROJECTION-  residual computation.  =  16.70 (sigma    0.22)
> PROJECTION-  solving linear system. =   0.26 (sigma    0.00)
> Total                               =  78.42

%===Sat Sep 14 10:36:35 ART 2002 mstorti@node1.beowulf.gtm
%
Resolviendo Poisson con CG en la malla de 2.2Melem tarda
%
> PREDICTOR-  residual computation.   =  80.22 (sigma    1.20)           
> PREDICTOR-  solving linear system.  =   6.02 (sigma    0.01)           
> POISSON-  residual computation.     =  12.42 (sigma    0.00)           
> POISSON-  solving linear system.    =  22.53 (sigma    0.04)           
> PROJECTION-  matrix computation.    =  41.31 (sigma    0.01)           
> PROJECTION-  residual computation.  =  43.91 (sigma    0.03)           
> PROJECTION-  solving linear system. =   1.04 (sigma    0.00)           
> Total                               = 207.45 (number of steps: 4)
%
Lo cual es bastante menos que los 400 y pico que teniamos antes. Ahora
voy a probar a bajar el rtol ya que como estamos usando CG no deberia
haber problema con la memoria. 
%
13:42:32: Con rtol=1e-8 revienta pero el problema es que deberia poder
poner la menor tolerancia solamente en el paso de poisson ya que en el
predictor y proyeccion ahi es gmres y entonces si que con sume
memoria. 

%===Mon Sep 16 11:00:59 ART 2002 mstorti@node1.beowulf.gtm
%
Con 2.37 Melem
> PREDICTOR-  residual computation.   =  84.67 (sigma    3.08)
> PREDICTOR-  solving linear system.  =   7.32 (sigma    0.06)
> POISSON-  residual computation.     =  13.12 (sigma    0.00)
> POISSON-  solving linear system.    =  54.98 (sigma   66.53)
> PROJECTION-  matrix computation.    =  44.50 (sigma    0.05)
> PROJECTION-  residual computation.  =  47.46 (sigma    0.07)
> PROJECTION-  solving linear system. =   1.32 (sigma    0.00)
> Total                               = 253.38 (number of steps: 6)
%
11:01:36: La reoslucion de Poisson tarda unos 50 secs (130 iters.), siempre pero
curiosamente en el paso 7 hace 250 iters y tarda 100 secs. 
%
11:32:51: Con la malla de 80^3*5 (2.56 Melem) revienta por memoria
en el segundo paso de tiempo. Pruebo a desbalancearlo todavia un poco
mas, es decir poner los que tienen 256MB a w=1.1. 
%
11:54:53: No hay caso, no llego a los 2.56 Mb. 
%
12:10:39: Ahora voy a probar a cuanto llego con el monolitico (TET). 
%
13:16:19: Con 60^3 anda al pelo, con 66^3 revienta por memoria. Voy a
probar con 66^3 pero rebalanceando un poco mejor los pesos. 
%
13:34:48: Poniendo 
%
node2         4.2
node4         4.2
node8         4.2
node6         4.2
node9         4.2
node3         1.7
node7         1.7
node10        1.7 
node12        1.7 
%
puedo correr 66^3*5  = 1.44 Melem.
 %
13:50:26: No, finalmente revienta en el paso 3. Pruebo con 64^3. 

%===Mon, 16 Sep 2002 12:58:29 -0300 mstorti@minerva
%
Corriendo la cavidad cuadrada con Fractional Step: Converge bien y da
cosas razonables pero a Re=1000 da diferencia con Ghia. 

%===Tue, 17 Sep 2002 08:22:04 -0300 mstorti@minerva
%
El fractional step da mas o menos bien a Re=1000. Con una malla de
40x40 y hratio=4 el monolitico da bastante bien comparado con Ghia
mientras que el fractional step no tanto. 

%===Mon, 30 Sep 2002 08:31:38 -0300 mstorti@spider
%
Aprovecho que estoy buscando una perdida de memoria para corrrer el
LeakTracer que me canta un monton mas (la gruesa todavia no la
encontre y el LeakTracer en realidad parece no encontrarla). Corregi
bastnte pero ahora se cuelga. al terminar (probablemente al destruir
objetos). Parece perder 5Mb en 30x3 iters (nnwt=3). Son unos 56kB por iter. 
%
08:40:11:  Con una malla bien chica pierde unos 2MB en 3x100
iters. 6.7kB por iter. 
%
11:07:59: Descubro que el programa principal lo tengo que linkeditar
con la opcion `-E' para el linker (es decir `-Wl,-E'), para que el
objeto cargado dinamicamente pueda ver hacia atras a los simbolos del
main. No se si esto tenga que ver con la perdida de memoria. Del
manual de `ld':
%
> `-E'
> `--export-dynamic'
>      When creating a dynamically linked executable, add all symbols to
>      the dynamic symbol table.  The dynamic symbol table is the set of
>      symbols which are visible from dynamic objects at run time.
> 
>      If you do not use this option, the dynamic symbol table will
>      normally contain only those symbols which are referenced by some
>      dynamic object mentioned in the link.
> 
>      If you use `dlopen' to load a dynamic object which needs to refer
>      back to the symbols defined by the program, rather than some other
>      dynamic object, then you will probably need to use this option when
>      linking the program itself.
%
11:13:26: Compruebo que pierde igual. 
%
11:37:39: Corre todos los ejemplos en `test/aquifer' (que tiene varios
casos con funciones dinamicas (.efn). 
%
11:49:00: Pierde tambien para cubcav. Voy a probar a recuperar
versiones viejas para ver cuando empezo a perder. 
%
13:34:08: Con la version estable 3.05 tambien pierde!!
%
14:09:22: Pruebo con el programa de prueba `pfmat.cpp' en `test/pfmat'
y tambien pierde. Asi que debe ser algo relacionado con `pfmat' en
PETSc (ya que el LeakTracer no lo detecta). Ahora voy a probar con la
3.03 que fue antes de hacer la migracion a PETSc 2.1.3. Efectivamente
con la 3.03 parece no perder. 
%
14:48:09: Con la 3.03.pl16 tampoco parece perder. Verifico nuevamente
que con la actual (beta-3.05.pl13) pierde. Ahora voy a probar con la
3.04. 
%
16:38:13: La `beta-3.04' tambien pierde. 
%
17:08:08: Corre la 3.04 con `-log_summary' y no detecta ninguna
perdida de memoria (no constante). Voy a probar a correr asi la
actual. 
%
21:48:46: Con la version actual (3.05.pl14) tengo con iisd, maxits=10:
%
Memory usage(kB): 31696 32148 32148 32148 32148 32148 32148 32932
32988 32996
%
Con solver 'petsc': 30088 30428 30432 30432 30432 30432 30432 30432
30432 30432. O sea que debe haber una perdida de memoria en el IISD. 

%===Tue,  1 Oct 2002 19:33:28 -0300 mstorti@spider
%
Ahora voy a probar con el IISD con y sin subpart. Con iisd_subpart=15
(elegido automaticamente) tenemos 31700 32152 32152 32152 32152 32152
32152 32936 32992 33000 33008 33016 33020, o sea que parece perder. 
%
Con IISD y un solo subdo tambien pierde. Con una malla de 6x6x6 en
cubcav pierde linealmente a razon de 8.3KB por iteracion. 
%
21:18:59: Si hago que no resuelva comentando la linea correspondiente
en `ns.cpp' entonces pierde a una tasa de 2.83KB/iter. 
%
21:33:08: Si aumento el tamanho de la malla a 12x12x12 entonces pierde
a una tasa de 2.93KB/iter lo cual indicaria que es una perdida
constante. Sera una especie de book-keeping del PETSc? Ahora voy a
probar con el iisd_subpart.
%
21:58:40: Con `iisd_subpart=4' pierde a razon de 13.4KB por
iter. Pareceria perder proporcional al numero de subdo's?
%
22:48:48: Con 20 subdo's parece perder tambien a 13.4KB/iter.
%
06:52:31: Con una malla de 30x30x30 tambien pierde lo mismo, a razon
de 1.33KB/iter. 

%===Wed Oct  2 12:58:10 ART 2002 mstorti@node1.beowulf.gtm
%
Con una malla de 40x40x40 corriendo en paralelo pierde a razon de
701KB/iter. (verificar mejor!!, sec corrieron pocas iteraciones). 
%
13:21:11: Corriendo con 2 procs en el cluster una malla de 20x20x20
pierde 14KB/iter. 
%
13:35:27: Con 11 procs tambien pierde a razon de 11/16KB iter. 
%
14:30:19: Con una malla de 50x50x50 y 11 procs da hasta 29 iters, un
crecimiento muy pequenho o casi nulo. 
%
14:48:08: Si miro los valores 'After solving...' entonces el minimo
crece como 20KB/iter y el maximo 13KB/iter. 
%
15:30:21: Con el cylinder da un crecimiento de 110K/iter con una malla
de 3*14^2*8. 

%===Sat,  5 Oct 2002 22:11:08 -0300 mstorti@spider
%
Advdif tambien parece perder.

%===Sun,  6 Oct 2002 10:03:32 -0300 mstorti@spider
%
Con NS, malla de 3*14^2*8, matriz PETSc, pierde relativamente poco. 
%
11:21:52: Adapto el `ns_id' para que reporte nodos ficticios. 
%
13:26:50: Anda el ns_id, pero de todas formas con una malla de
3*10^2*8, 6-8 sudo's por procesador y 3 procs, crece a razon de
10-12KB/iter. 
%
Command:
perl -ne 'print "$1 $2 $3\n" if /min (\d*), max (\d*), avrg (\d*)/;' < petscfem.output.tmp 
%
16:00:46: Pruebo a correr un problema chico con BOPT=g_c++ y -trdump,
-trmalloc y el espacio total usado por PETSc es siempre (para nstep=5,
50 y 200) malla de 3*2^2*1:
%
[0]Total space allocated 60332 bytes
%
con lo cual pareceria que no hay perdida de memoria en PETSc. 
%
16:07:13: Con la malla de 3*10^2*8, nstep=5, solver=iisd da
%
[0]Total space allocated 18746008 bytes
%
Ahora voy a correr nstep=50 a ver si crece o no. Posibilidad a
investigar: usar PetscTrDUmp() en cada paso de tiempo que
probablemente vacie los buffers, si es que esta logueando algo.  
%
16:25:54: Con 50 pasos de tiempo report exactamente lo mismo. 
%
19:47:53: Escribo un pequenho programa `leak.cpp', que crea una matriz
PETSc que corresponde a una maatriz 1D, con stencil [-.1 1 -.1],
periodica y la compilo con petsc-2.0.24 y petsc-2.1.3. Con la 2.0.24
pierde a razon de 0.15KB/iter, mientras que con la 2.1.3 pierde a
razon de 3.2KB/iter. 

%===Thu, 10 Oct 2002 21:34:16 -0300 mstorti@spider
%
La perdida de memoria se debia en realidad a un error en PETSc. Lo
cual fue arreglado en un parche released en Oct 4 2002. 

%===Sun, 13 Oct 2002 11:21:30 -0300 mstorti@spider
%
Anda bien el cambio  de escalas para longitud y tiempo, pero no para
densidad. Sospecho que puede ser que falte escalear las fuerzas de
aceleracion en `nsi_keps_rot'. 
%
11:26:22: Si, efectivamente ahora tambien es invariante ante un cambio
de densidad. 
%
12:49:17: Hmmmmmmmmm.... no se... Verificando de nuevo. 
%
17:43:31: El escaleo en densidad no lo puedo hacer andar. De todas
formas voy a usar siempre `rho_scale =rho' con lo cual el progrma
corre con `rho=1' y despues escalea fuerzas y momentos por `rho'. 

%===Wed, 23 Oct 2002 09:08:13 -0300 mstorti@node1.beowulf.gtm
%
Corriendo tests con cubcav. Con rtol=1e-8 y una malla de 5*50^3
tenemos iisd(auto=2000)=57.6secs y con petsc(gmresg)=323 secs y no
converge. (rtol8.fig)
%
Con rtol=1e-4, iisd=17.7secs, petsc=63.8secs. 

%===Thu, 24 Oct 2002 17:15:05 -0300 mstorti@node1.beowulf.gtm
%
Con 2194880 (2.2Melem): iisd: 252secs (rtol=1e-4). Petsc (gmresg) caga
la fruta en la iteracion 122 (habiendo convergido 2e-3). 
%
19:31:03: Corriendo con rtol=1e-2, gmresg hace 90ecs, mientras que con
IISD tenemos 120secs(fac) + 70secs(iter) = 190 secs/step. 

%===Fri, 25 Oct 2002 11:39:12 -0300 mstorti@node1.beowulf.gtm
%
Tardaba mucho en cargar la malla y hasta parecia que se colgaba la
corrida. Los sintomas eran los siguientes: uno de los nodos empezaba a
tener mucha carga de CPU y poca memoria, despues los otros empezaban a
bajar la carga hasta casi 0 y el otro seguia procesando. A veces
parecia que se colgaba. Empezando a poner sincronizaciones
(MPI_Barrier + mensaje) llegue a la conclusion de que en realidad es
que el nodo lento (ahora era el node8) se quedaba leyendo la
malla. Haciendo que el proc 0 lea la malla y se la mande a los otrod
via MPI tarda mucho menos. Por ejemplo para una malla de 5*76^3 con
MPI tarda unos 160 secs contra 470 (3 veces mas) leyendo todos via
NFS. 

%===Sun, 24 Nov 2002 06:39:01 -0300 mstorti@spider
%
LES: pareceria que si hago DNS entonces para RE altos los perfiles de
velocidad parecen un poco ruidosos. Implemento que se pueda usar LES
sin tener que calcular el factor de van Driest, lo cual agrega un poco
de viscosidad. Pero parece que lo laminariza. Ahora voy a correr con
C_smag=0.1.  

%===Wed, 11 Dec 2002 18:38:53 -0300 mstorti@spider
%
Retomo `mesh-move'. La version vieja (`mesh_move_branch') logra
deformaciones del orden del 90% en el problema del `step.epl'. Con la
nueva tuve dramas asi que estoy tratando de recuperar lo que
tenia. Primero inserto el viejo elemento como `mesh_move_old' y trato
de poder correrlo con el nuevo main. 

%===Sat, 14 Dec 2002 09:07:15 -0300 mstorti@spider
%
Estoy verificando adveccion lineal. Transporte estacionario en una
direccion cruzada con la malla con una malla de 10x10 parece estar mas
o menos bien. Ahora estoy advectando una gaussiana con velocidad (1,0)
desde una posicion inicial (0.5,0.5) en un recinto 0<x<5, 0<y<1 (la
malla es de 20x100). Con
una sigma inicial de 0.2 y una radio rcone=0.4. Lo corro con un
Courant the 0.7 y alpha=0.7. Da todo bastante bien, pero difunde
demasiado en la direccion x. La verificacion consiste en calcular el
centro de gravedad y momentos de segundo orden con respecto al centro
de la campana. Los momentos deberian permanecer constantes, y el
centro de la campana se deberia transportar u*T. Lo transporto 100
pasos de tiempo y el centro da muy bien (en x=3.5) pero el sz se
incrementa en un factor mas de 3. Voy a probar a bajar el alpha a
0.55. 
%
> octave> [xc,sx,sy]=bellpar(x,ini)
> xc = 0.50000  0.50000
> sx = 0.016759
> sy = 0.016759
> octave> [xc,sx,sy]=bellpar(x,phi)
> xc = 3.99989  0.50001
> sx = 0.066410
> sy = 0.017461
%
09:18:20: Con alpha=0.55 da bastante mejor, baja a sx=0.029 (no llega
a crecer un factor 2). 
%
xc =  3.99998  0.50000
sx = 0.029747
sy = 0.017461
%
Ahora voy a probar a bajar el Courant (digamos Co=0.3). 
%
09:32:38: Con una malla refinada a la mitad y Co=0.3 tenemos 
xc =  1.02500  0.50000
sx = 0.017272
sy = 0.016878
(sx-sx0)/(x-x0)/sx0 = 0.058306
%
Con la malla fina, pero voviendo a Co=0.3 tenemos
xc = 1.20000  0.50000
sx = 0.018139
sy = 0.016913
(sx-sx0)/(x-x0)/sx0 = 0.11763
%
Con lo cual la influencia del Co es grande. 
%
09:42:07: Si dejo la malla original (de 20x100) y bajo el paso de
tiempo a Co=0.3 entonces tampoco se gana mucho 
%
octave> phi=aload("save.state.tmp"); [xc,sx,sy]=bellpar(x,phi), (sx/0.016759-1)/(xc(1)-0.5)
xc =  2.24997  0.50000
sx = 0.020190
sy = 0.017112
ans = 0.11700
%
Con la malla fina y el Courant a 0.3 es de esperar bajar la disipacion
0.058306*3.5 = 0.20407, es decir que el ancho de la campana creceria
un 20%. 
%
Con alpha=0.5 baja muchisimo la disipacion, casi da lo mismo segun x
que segun y. Ahora voy a probar a volver a Co y h originales. 
%
xrate = 0.011944
yrate = 0.011922
%
11:03:05: Con la malla gruesa (20x100) da un poco de undershoot, pero
pasando a 30x150 ya no. Salvo.
%
11:40:14: Corro con adveccion segun la diagonal con una malla de
100x100. Da bastante bien:
%
> x0: 0.500000 0.500000, sx0 0.016759, sy0 0.016759
> x: 1.249957 1.249957, sx 0.016929, sy 0.016929
> xrate = 0.013562
> yrate = 0.013562
> octave> 0.775+0.5
> ans = 1.2750
%
Ahora voy a pasar al cono rotante. 

%===Sun, 15 Dec 2002 00:59:06 -0300 mstorti@spider
%
El cono rotante anda bastante bien. Con una malla de 50x50 da toda la
vuelta y el sx se incrementa un 23%. 
%
octave> proc
x0: 1.500000 0.500036, sx0 0.016799, sy0 0.016751
x: 1.503951 0.494056, sx 0.019052, sy 0.020769

%===Sun, 15 Dec 2002 11:20:13 -0300 mstorti@spider
%
Con una malla bastante mas fina (160x160, 838 Dt) el cono rotante produce 
una solucion muy buena (un cambio en el 2do momento del 10%). 5hs in
spider. 
%
x0: 1.500000 0.499996, sx0 0.016774, sy0 0.016774
x: 1.501796 0.499932, sx 0.018354, sy 0.018368
rate = (sx-sx0)/sx0/(2*pi) = 0.014988
%

%===Wed, 25 Dec 2002 21:52:35 -0300 mstorti@spider
%
Acoplamiento potencial/NS: Parece andar bien, con un factor de
relajacion 0.1 converge bien. Voy a probar a largarlo con omega=1,
pero de un punto mas convergido. 
%
22:37:03: Con un factor de relajacion de 1 es inestable.
%
23:10:37: Comparando v en el borde exterior (y=Ly) con la el empalme
potencial y con una solucion obtenida tomando un Ly bastante mas
grande (Ly=4, Ny=80). Se observa una muy buena coincidencia, asi que
los resultados son muy alentadores. 

%===Thu, 26 Dec 2002 13:17:31 -0300 mstorti@minerva
%
Compilando `dofmap2.cpp' con optimizacion revienta en los tests
`newff'. Lo pongo a compilar con -O0. 

%===Sun, 29 Dec 2002 17:16:35 -0300 mstorti@spider
%
El cylinder con la malla Rext=4 da un periodo de oscilacion de T =
0.94248sec a una Re_D = 100. El Strouhal seria: omega*D/U = 13.33. 

%===Sun,  5 Jan 2003 09:35:37 -0300 mstorti@pider
%
Anda el acoplamiento con el potencial. El cylinder con la malla de
Rext=4 da un Strouhal similar pero las velocidades son bastante
menores, tal vez se debe a que el acoplamiento con el potencial puede
estar un poco `laggeado' por la subrelajacion y el paso de tiempo de
lag propio del acoplamiento explicito. De todas formas lo que yo tome
son velocidades en y=Rext, x>0, las cuales en el caso de no acoplar
puede ser que se amplifiquen bastante mas el ruido por efectos del
entubamiento. Ademas hay que ver que en realidad el `acoplado' lo
habia corrido con omega=0.1, lo cual tal vez es un poco demasiado
filtrante. 
%
Por otra parte los perfiles de velocidad exterior son bastante mas
bajos cuando acoplo, lo cual esta bien ya que no se produce el
entubamiento. Las velocidades que se producen en x=5, y=4 oscilan
alrededor de 1.6 para el no acoplado y de 1.25 para el acoplado. El
acoplado parece estar francamente mejor en este sentido.
%
09:56:34: Lo que voy a hacer ahora es ver como cambia mejorando el
acoplamiento, por ejemplo poniendo menos subrelajacion (si se la
banca) o poniendo un filtro mejor. 
%
11:07:11: Con omega=0.5 y partiendo de la solucion no acoplada se va a
a la mierda. 
%
13:10:26: Con omega=0.25 y saliendo de la solucion sin coupling anda
bien. 
%
17:56:01: Con omega=0.5 saliendo de la solucion a 0.25 se hace
inestable. 
%
19:41:06: Habia un error en el calculo del dphi_dn. Se terminaba
poniendo el valor en otro `k'. 
%
21:15:47: Tomando la velocidad `v' (comp `y') en un punto a x=4.494,
y=0 y midiendo los puntos de cruce por 0 da un periodo T de 37 pasos
que a Dt=0.157, da un T=5.81, con lo cual el Strouhal = D/VT = 2/5.81
= 0.344 (demasiado alto). Deberia dar 0.17 (no habra un factor 2
jodiendo?).  Salvo los resultados de Octave en
`cylin.filter.octave.tmp'.  Ahora voy a probar a ver si tocando algun
parametro del filtro se obtienen resultados diferentes.
%
Parametros a tocar:
* Refinamiento - refinamiento hacia la pared
* Parametros de integracion temporal, paso de tiempo
       parametros de acoplamiento potencial/inviscido

%===Tue,  7 Jan 2003 23:27:00 -0300 mstorti@spider
%
Ya esta!! Era un error: al tomar los puntos donde `v(x0,t)'  cambia de
signo (para un dado `x0') , la diferencia entre esos puntos es _la
mitad_ del periodo. De ahi el factor 2. Incluso con `omega=0.5' da
> Strouhal 0.173946, max dev. rel .0.016894
Ahora voy a ver que pasa con omega=0, o sea que 

%===Wed,  8 Jan 2003 07:24:31 -0300 mstorti@spider
%
Sin acoplar con el potencial en 311 pasos la oscilacion sigue
creciendo y se hace muy intensa. (La verdad que no se si es demasiado
o si cuando ponemos el acoplamiento esta dando demasiado poco). Si lo
tomamos como una siunoside creciente en el tiempo y le calculamos el
Strouhal da 0.2 (lo cual estaria mal). 

%===Tue,  7 Jan 2003 13:43:28 -0300 mstorti@minerva
%
Trabajando sobre el `gasflow' (NS compresible). Corro el ejemplo de
Beto (flujo homogeneo con una perturbacion) parece andar bien. Voy a
resolver ahora una tobera. 

%===Wed,  8 Jan 2003 10:12:52 -0300 mstorti@minerva
%
Para mergear las cosas de Navier Stokes compresible y bubbly creo una
banch ns-comp-merge-root/ns-comp-merge-branch, y despues mergeo sobre 
beta-3.14.pl12. 

%===Wed,  8 Jan 2003 17:03:53 -0300 mstorti@spider
%
Estoy tratando de escribir los embedded gatherer para `line2quad' y
`tri2prism'. 
%
21:02:35: cylinder with viscous/inviscid coupling: Sin acoplamiento
(v.n = v_inf.n) produce Strouhal =  0.206 
%
19:06:43: Con el nuevo filtro anda bien. Lo voy a largar con Rext=2.
%
20:26:07: Con Rext=2 y el nuevo filtro da muy bien!! Da
Strouhal=0.17... (en los primeros ciclos, lo tengo que dejar iterar un
poco). 
%
23:13:23: Despues de 380 Dt, la cosa cambia un poco y el Strouhal pasa
a ser de 0.193. Lo cual esta mal pero de todas formas es bueno que
oscile y con un Strouhal no tan malo. 
%
03:40:23: Con condicion (v.n = v_inf.n) y Rext=2 se va a
St=0.41. Ahora voy a probar a R=1000. 
%
05:40:08: Con Re=1000 le baje Co=1 y nnwt=3 porque al principio
divergia pero despues vi que estaba con el filtro de O(Dt^2) y con
(omega=0.25,xi=2). Asi que le baje el xi=1 y ahi anda bien. Ahora voy
a probar a volver a Co=2 y nnwt=2. 

%===Thu,  9 Jan 2003 09:19:49 -0300 mstorti@minerva
%
Con Rext=2 y sin acoplamiento (v.n = v_inf.n) el sistema se estabiliza
(no hay vortex shedding) y el Strouhal (amortiguado!!) es del orden de
0.4.
%
10:55:20: Con Rext=2 se desestabiliza con el filtro (omega=0.9;
xif=0.1). Pruebo a aumentar nnwt=3 y Co=1, pero igual. 
%
10:56:29: Pruebo a cambiar el filtro (omega=0.5; xif=0.1). 
%
11:21:05: Habia un error, el filtro debia ser con xif=1, y no 0.1!!
%
11:40:40: De todas formas con (omega=0.9; xif=1) parece que era
inestable. Estoy probando con (omega=0.5; xif=1) y por ahora parece
OK. Me pregunto la precision de todo esto. 
%
11:48:24: Efectivamente con (omega=0.5; xif=1) parece estable. 

%===Sat, 11 Jan 2003 10:38:42 -0300 mstorti@spider
%
Voy a tratar de recuperar un branch de trabajo que habia hecho en
casa. 

%===Wed, 15 Jan 2003 23:03:34 -0300 mstorti@spider
%
gasflow: Corro un nozzle quasi-unidimensional (con condicion slip en
el contorno). M_in = 0.3 y la contraccion es de 0.7. Converge
lentamente, conserva bien caudal, entropia y entalpia total. Ahora voy
a probar con un Mach mas alto. 
%
23:19:39: Con M_in=0.5 y contraccion 2:1 deberia dar supersonico y la
verdad es que da muy ruidoso. Debe haber algun problema con la
estabilizacion. 

%===Fri, 17 Jan 2003 12:05:57 -0300 mstorti@minerva
%
Corro un cilindro rotando inicialmente con cierta velocidad y con
pared solida. El fluido deberia comprimirse hacia el exterior y poco a
poco se deberia ir frenando. A Re=100 se freno superrapido (en pocos
pasos de tiempo). Ahora voy a probar a subir el Re y bajar el paso de
tiempo. 
%
20:26:34: A Re=10000 y con Dt se frena tambien rapidamente (pero
menos). Digamos que a t=0.033 la maxima velocidad es 5e-4 (habiendo
partido de 200). Hay que ver que ademas la malla es muy gruesa. (4
elementos en la direccion radial). Esta muy lento. 
%
20:38:51: gasflow calcula 512 elementos, nnwt=5, 10 Dt en 36 minutos,
lo cual da una tasa de 36*60/10/5/512*1000 = 84 secs/Ke en spider, lo
cual no parece estar tan mal (son hexas). 
%
20:45:14: Calculado por PETSc-FEM: 71 secs/Ke, mientras que para
nsi_tet_les_fm2/tetra da 1.23secs/Ke. 
%
20:49:13: nsi_tet_les_fm2/hexa: 11 secs/Ke. 
%
21:01:13: gasflow: con tetra da 13sec/Ke, que de todas formas esta un
orden de magn. arriba del nsi_tet_les_fm2. Debe ser por la
ineficiencia en el calculo de las funciones comp_.... 
%
Ademas: No se gana demasiado entre tetra y hexa (36 secs para 512
tetra contra 33secs para 2560 tetras.)
%
21:51:36: Pruebo a bajar la viscosidad pero tambien bajar la velocidad
de rotacion (para mantener el Re). 
%
12:29:27: Habia un problema con las condiciones de contono en
Rin. Ahora voy a volver al compresible. 

%===Sat, 18 Jan 2003 19:14:53 -0300 mstorti@spider
%
En un problema 1D de rotacion con bastantes puntos (40 elementos en la
direccion radial) y Re alto se mantiene bastante el perfil. 
%
22:43:00: En el problema 1D suficientemente refinado parece
comportarse OK. 
%
* Da bien el perfil de presion por a fuerza centrifuga, 
se produce un menisco parabolico de presion de amplitud
rho*omega^2*r^2/2 (lo verificamos asumiendo rho casi constante). 
* La masa total se mantiene constante. Haciendo 
octave> sum(leftscal(diff(x).*xcent(x),2*pi.*xcent(rho)))
se otiene la masa total (aprox.) y muestra variaciones relativas del
orden de 3e-4. 
* La temperatura y la presion muestran un pico hacia la pared. Uno en
principio esperaria un calentamiento gradual.
* La energia mecanica total: \int_0^R0 \rho (i + 1/2 v^2) se mantiene
constante: tiene una variacion del 4% de la variacion de energia
mecanica y de energia interna (la energia mecanica se convierte en
interna por disipacion). 
%
Conclusion: habria que empezar con la geometria del vtube pero con
condiciones periodicas para poder hacer modelizaciones mejores. 

%===Sun, 19 Jan 2003 13:48:04 -0300 mstorti@spider
%
Corre ejemplo vtube con axisimetrico. Se forma un flujo secundario
interesante. Ahora voy a probar con una malla mas refinada. 
%
17:00:30: Corre bien con una malla mas refinada. Ahora voy a tratar de
correr con el problema total: entrada/salida y velocidad
circumferencial a la entrada. 
%
19:35:47: Las veolocidades se hacen muy grandes en la salida fria
(como si se cortocircuitara con la entrada). Voy a probar a dejar solo
la salida caliente. 
%
21:39:35: Cuando pongo presion impuesta en algun contorno se tiende a
ir a la mierda. Voy a probar sin entrada/salida. 

%===Mon, 20 Jan 2003 10:54:30 -0300 mstorti@minerva
%
Sin entrada y salida de material, rotando a velocidad 200 diverge. Se
pueden probar varias cosas: 
* una velocidad mas baja (menos compresible). 
* ir frenandolo mas despacio
* que inicialmente este en reposo e ir acelerando la entrada. 
Primero voy a probar usar la velocidad mas baja. 
%
11:00:58: Con L0=2*R0; y velocidad 20m/s, acanza unos cuantos pasos de
tiempo pero detecto que el lazo de newton (nnwt=2) no converge asi que
le pongo (nnwt=5) a ver si detecto algo. 
%
11:20:15: Con `u_circunf_in=20' y `nnwt=5' parece converger. Tambien por
un error le estaba poniendo mal el ancho de la entrada Dz_in. 
%
11:32:56: Con `u_circunf_in=200' diverge. 
%
12:46:32: Con `u_circunf_in=40' converge mas o menos. Pero la
velocidad en el anillo de entrada (pero con velocidad radial nula) no
se propaga hacia adentro. Ahora voy a probar de nuevo a poner un poco
de entrada y salida. 
%
12:51:24: Le agrego una componente radial de velocidad. Diverge. Voy a
probar a correrlo en incompresible. 
%
13:19:20: En incompresible con `axisymmetric "z"' da pivote nulo. Sin
el axisymmetric anda bien. En 10 Dt converge bien (u_rad_in=0). El
perfil de velocidad tampoco penetra nada. 
%
13:23:39: Poniendo velocidad a la entrada anda bien, por supuesto se
va todo al nodo con presion nula, por continuidad. Ahora voy a probar
a ponerle presion en algun lado. 
%
13:33:13: Poniendo presion=0 en las salidas anda bien. Ahora voy a
probar a subir el Re. 
%
20:55:52: Con Re=1e3 y u_rad=10, u_circ=40 parece mantenerse la
rotacion. Converge bien y parece llegar a un estacionario.
%
22:36:33: Con presion=-160 en la salida fria se forma bien la
sepracion de corrientes en incompresible. Ahora voy a probar a refinar
mas. 

%===Tue, 21 Jan 2003 11:08:28 -0300 mstorti@minerva
%
Voy a mergear las cosas de BEto del Bubbly que habian quedado en
advdif/NEW_VERSION. Basicamente esta el avance del bubbly por
multi-steps y lo de los jobinfo-fields. Mi idea es que lo de los
jobinfo-fields ya se podria mergear en la corriente principal y lo de
las multiples etapas en el main podria quedar en una main aparte. 
%
Las correcciones hay que agregar las al branch
`ns-comp-merge-branch'. La version `ns-comp-merge-old-version'
contiene lo mergeado 
%
11:27:03: El `advdif_bubbly.cpp' lo borro (creo que era viejo) y al
`advdif.cpp' que estaba en el NEW_VERSION pasa a ser el nuevo
`advdif_bubbly.cpp'. La idea es que va a haber dos main, pero lo demas
seria compartido. 

%===Tue, 21 Jan 2003 22:23:24 -0300 mstorti@spider
%
Empiezo a correr el `vtube' en compresible. Con `refine=2' y
`u_rad_in=20', y solo salida caliente converge bien y se forma un
flujo saliente correcto. 
%
22:26:21: A `nstep=85' lo corto para relanzarlo con salida fria. 
%
22:43:34: Le largo la salida fria con p_c = 1.013e5 pero revienta. Le
bajo el Dt de 1e4  a 5e-5 y le pongo una presion de salida fria mas
alta 1.2e5. 
%
22:57:55: Sigue divirgiendo. El flujo se va todo hacia la salida
fria, e incluso se invierte el sentido de la salida caliente. 
%
23:24:16: Con p_c=1.3e5 converge bien. Sale casi todo por la salida
fria, asi que en realidad se deberia usar una p_c todavia mayor. 
%
08:40:05: Con 1.33e5 y 1.3e5 tambien diverge. 

%===Wed, 22 Jan 2003 11:36:14 -0300 mstorti@node1.beowulf.gtm
%
Lo largo en el cluster bastante refinado (refine=12) tarda unos
12secs/iter o sea 1'/Dt (nnwt=5). Son 4608 elementow=quads. Con
Dt=5e-4 revento y ahora estoy usando Dt=5e-5 y anda bien. 
%
12:18:25: Anduvo bien la corrida con solo la salida caliente en el
cluster (gasflow.hot_only_run.tgz). Ahora voy a probar a poner la
salida fria con p_c = 1.315e5.
%
12:31:35: Por ahora parece que va bien. El corte parece dar demasiado
hacia la salida fria. Pero hay que ver por el factor 'r'. 
%
15:43:24: Termina bien la corrida con salida fria. Ahora hay que
empezar a bajar la temperatura de entrada y presion de salida. 

%===Thu, 23 Jan 2003 13:54:19 -0300 mstorti@node1.beowulf.gtm
%
Una cosa a observar con respecto a la corrida con salida fria es que
por la salida fria el gas sale mas caliente. Pero esto se puede deber
a que todavia no estan bien puestos los parametros geometricos, el Re
no es el correcto y el Pr esta incrementado en un factor 10. Salvo en 
`gasflow.hot_cold_outlet.tgz'. 

%===Fri, 24 Jan 2003 13:58:44 -0300 mstorti@node1.beowulf.gtm
%
Con las dimensiones reales se hace mas inestable. Para estabilizarlo
le pongo visco=1e-3 (en vez de 1e-5) y el Dt bajo bastante (5e-5 ->
1e-5). 

%===Sat, 25 Jan 2003 09:32:08 -0300 mstorti@spider
%
Anda el `flow_rate_integrator' en compresible. El balance entre la
entrada y la salida no da del todo bien, pero tambien hay que ver que
el numero de elementos que hay en una salida es muy bajo (3). Ahora
voy a agregar la salida fria y tratar de regular el corte
automaticamente. 
%
17:53:19: Anda informaticamente el acoplamiento caudal/presion via
hooks, pero ahora realizo que conviene hacerlo mas a pata, ya que por
ejemplo hay que tener en cuenta la axisimetria. 
%
20:17:30: Cuesta sintonizar el `flow_controller'. Ahora voy a tratar
de regular solo el corte (no el caudal para cada corriente). 

%===Tue, 28 Jan 2003 16:02:03 -0300 mstorti@node1.beowulf.gtm
%
Corro el ejemplo grande. Llega el estado estacionario con solo salida
caliente OK. Cuando le agrego la salida fria a p_c=1.1e5 el flujo en
la salida caliente se tiende a invertir asi que ahora voy a probar con
p_c=1.15e5. 

%===Wed, 29 Jan 2003 09:48:19 -0300 mstorti@node1.beowulf.gtm
%
Tambien se invierte voy a probar con 1.3e5.
%
11:32:05: Se invierte la salida fria. Pruebo con 1.22e5. 
%
13:39:36: Con 1.22e5 se invierte la fria. Guarda que tambien modifique
el paso de tiempo. (1e-5 -> 5e-5). 
%
14:11:28: Tambien diverge pero no es facil cual de las dos salidas da
mal. Asumo que es la fria (como pasaba con el otro Dt.). 
%
14:49:14: Tambien diverge. No veo que se invierta. Voy a probar a bajarle mas el Dt. 
%
15:00:01: Veo que diverge cerca de la salida fria sobre el eje. Me
pregunto si es por imponer condicion de pared solida sobre el eje. Una
posibilidad podria ser poner condcion slip ahi. Es lo que voy a tratar
ahora. 
%
15:45:54: Diverge porque en rho=Rin, z=L0 hay que imponer pared
solida. Sino se puede meter flujo por ahi. 
%
16:56:42: Poniendo condicion slip anda mucho mejor pero de todas
formas en un cierto momento con p_c=1.2e5 la velocidad se invierte en
la salida caliente. Asi que lo corto y lo vuelvo a arrancar desde el
step 140 con p_c=1.3e5. 
%
17:07:40: Se invierte la salida fria. Voy a probar con p_c=1.25e5. Se
invierte la salida caliente. Lo arranco a partir del estado 70 con
p_c=1.275e5. 

%===Thu, 30 Jan 2003 09:58:06 -0300 mstorti@node1.beowulf.gtm
%
09:58:01: En realidad veo que en el paso 70 ya estaba el flujo
invertido en la salida caliente. Revisando veo que la salida se
invierte entre el paso 10 y el 20 asi que toda la corrida no sirve. 
Voy a probar a relanzar desde la corrida anterior (RUN1) step=140 como
antes pero con la nueva `p_c' (1.275e5). 
%
%
11:09:49: Se invierte la caliente. Surge la idea de subir la
viscosidad un poco mas. Segun el paper de vortex tube en IJHMT la
correccion por turbulencia deberia andar en el orden de 1000 (yo estoy
usando 100). De todas formas quiero hacer una pequenha pruebita
primero de volver a correr con 1.3e5 (se deberia invertir la fria). 
%
11:39:06: Efectivamente se invierte la fria. Voy a probar a aumentarle
el `flow_coef' (1e-2 -> 1e3). Vuelvo a p_c=1.275e5. 
%
13:35:55: Con el `flow_coef' y esa p_c parece andar bastante
bien. Pero despues de un rato los dos flujos se van 0. 
%
14:18:53: Corrijo que directamente la correccion sea no en flujos
relativos sino con respecto al caudal que quiero que entre por cada
entrada. Despues eventualmente habria que agregarle una correccion por
si hay perdidas. Tambien hay que agregarle una correccion por si el
caudal se va a 0, pero por ahora no lo pongo. La ecuacion que estoy
usando es, mas o menos
%
\dot p_j = -C (q_j(t) - cut_j * q_inlet(j))
%
donde p_j, q_j son la presion y caudal de la entrada j, my_cut es la
fraccion que quiero que salga por la salida j, y q_inlet(t) es lo que
esta entrando en este momento por la entrada. 
%
Lo arranco a partir del estado RUN3/120 e inicializo las presiones
p_h, p_c a los valores que tenia en ese momento. 
p_h = 133599, p_c=131901. 
%
16:01:03: Parece regular bien, pero por un error en el mkvtube.m, no
estaba imponiendo la presion regulada en la salida fria. 
%
17:16:07: Ahora esta controlando bien. Tal vez le puede costar llegar
al estacionario, pero no diverge. 

%===Fri, 31 Jan 2003 09:50:59 -0300 mstorti@node1.beowulf.gtm
%
A los 240 pasos mas o menos la velocidad se inviert en la salida fria
r=Rin y enseguida diverge. Me pregunto si tiene que ver con imponer la
velocidad radial a cero. Voy a probar a sacar u_r=0 en sobre la salida
fria. Arranco a partir del step=180. 
%
11:09:41: Tambien diverge y mas rapido. Se invierten la salida
fria. Vuelvo a poner u_r=0 en la salida fria y bajo el Courant un
factor 4 y lo lanzo a partir de RUN8/210.
 %
11:55:59: Incluso con el D=2.5e-6 diverge la presion. Estoy pensando
en un poblema de estabilizacion. Le voy a aumentar el tau_fac. 
%
12:21:50: Con tau_fac=10 primero el ruido decrece fuertemente pero
despues se vuelve a amplificar y pareceria haber ruido tambien en la
entrada. Voy a probar con un tau_fac=3. 
%
12:37:18: Tambien al primero decrece y despues aumenta. Voy a probar
con tau_fac=0.3.
%
13:10:50: Empieza a diverger de entrada. Pruebo con tau_fac=3 y
Dt=2.5e-6, empieza convirgiendo y despues diverge. Mas lentamente,
pero eso se puede deber a que el paso de tiempo es mas chico. Pienso
en ver de agregarle un shock-capturing. Tambien hay que tener en
cuenta que el Mach en esa zona puede ser muy bajo. 
%
13:31:04: Voy a tratar de llevar los parametros a los parametros
reales y aumentar un poco el radio interior. 


%===Tue, 11 Feb 2003 10:53:35 -0300 mstorti@spider
Tested splittings for 
triangle
  dx_indices 1 2 3 triangles
cartesian2d: 
  dx_indices 1 2 4 3 quads
  dx_indices 1 2 3 3 4 1 triangles
cartesian3d:
  dx_indices 1 2 4 3 5 6 8 7 cubes
tetra:
  dx_indices 1 2 3 4 tetrahedra
prism  (not conforming though)
  dx_indices 1  2  3  4   5  4  6  2   2  6  3  4 tetrahedra

%===Sat, 22 Feb 2003 12:49:10 -0300 mstorti@spider
%
Retomo el tema del acoplamiento viscoso inviscido. Visualizo con DX el
campo de velocidades viscoso junto con el potencial y  detecto gran
diferencia entre ambos campos. Descrubro que esta mal inicializado el
campo. EL uini en los nodos ficticios tiene que tener n_x en la
componente 1 y lo demas no importa. Ademas habia un error por el cual
no se calculaba bien el potencial. sobre la sup. de
acoplamiento. Corro a Re=1000 y ahora se ve bastante bien. El Strouhal
da 0.196, creo que si lo dejo mas tiempo puede subir un poco mas (mas
cerca del 0.20 que tiene que ser). Resultados en plate.tgz
%
Hizo 426 pasos de tiempo y en un cierto momento se corto la luz, pero 
en realidad parece como si se hubiera empezado a diverger _antes_ de
que se cortara la luz. 

%===Mon, 24 Feb 2003 20:20:46 -0300 mstorti@spider
%
En el problema `wave' que es un sempliano con u=1+du*sin(ky),
periodico segun y, lo resuelvo bien imponiendo vorticidad nula a la
entrada. Esto lo hago tomando dos capas de elementos estructuradas y
usando una aproximacion de segundo orden para la vorticidad. Ahora lo
quiero aplicar al `cylin'. 

%===Wed, 26 Feb 2003 11:17:31 -0300 mstorti@spider
%
Para funciones de `dvector' con muchos indices voy a tener que volver
a la estrategia de usar dos nombres distintos (por ejemplo `reshape' y
`reshapev') ya que sino hay ambiguedad cuando hay un solo argumento
variable y es la constante 0, ya que en ese caso el compilador lo
puede interpretar como NULL. 

%===Fri, 28 Feb 2003 21:14:57 -0300 mstorti@spider
%
Con null_vort, malla de 50x50, tiempos de resolucion (secs. en spider)
%
 null_vort    ad_hoc (octave)
 ---------- ----------------
  7.7910      4.0040
  7.9930      6.0000
  6.6890      4.5790
  7.8640      5.5670
  8.9470      5.9020
  2.8660      1.6410
  8.1530      3.5890
  8.4230      5.7180
  2.7950      1.5670
  6.1100      3.2500
	      6.0420
  	      1.5460
%  
O sea que el tiempo de resolucion tambien aumenta en un buen factor. 
%
21:24:41: La pregunta ahora es si este incremento en los tiempos se
debe a que el `id_map' esta con lineas muy largas o se debe a la
ineficiencia intrinseca de los set<> y map<> que contiene. 
%
21:35:56: Las lineas en el `id_map' pasan de tener longitud 4 a
107. Con los stenciles ad-hoc el perfil sale diferente porque tiene
muchos elementos nulos que directamente no se entran. 
Por mas que mejoremos la eficiencia de los accesos, a 
%
23:18:42: Si en `null_vort' hago que solo incluya los coeficientes no
nulos, entonces la eficiencia pasa a ser igual que la implementacion
ad-hoc. Pero esto no va a funcionar bien cuando pasemos a problemas
reales. De todas formas voy a tratar de correr el `cylin.epl' para ver
si el acoplamiento anda bien. Despues me voy a preocupar por la
eficiencia. 
%
23:23:21: Habia un bug en `cylin.m' por el cual creo que el vector
normal no quedaba normalizado. 

%===Sat,  1 Mar 2003 10:42:11 -0300 mstorti@spider
%
Corro el clindro acoplado OK, pero el Strouhal no me da del todo bien,
me da St=0.226, con lo cual se va un poco mas arriba de lo esperado. 
Resultados en plate-01-Mar-2003.tgz
%
13:30:44: Porque aumento el Strouhal? Habria que revisar los
parametros con los que corria antes. A ver si no se modifico algo, por
ejemplo antes corria con nnwt=3. 
%
13:43:12: Corro dos periodos (unos 120 pasos) a partir del ultimo de
la corrida plate-01-Mar-2003.tgz. Lo salvo en plate-01-Mar-2003a.tgz. 

%===Wed,  5 Mar 2003 11:52:19 -0300 mstorti@minerva
%
Verifico que el programa no corre cuando uso la version vieja de SSL
(creo que es la 2.09). Con la nueva version de SSL no anda el
comunicarse a otra maquina (socket2.cpp). Si anda con la libreria de
sockets de libc (socket.cpp). 
%
13:55:39: El problema era que el nombre del servidor en SSL es
servername@hostname. O sea que habria que poner @minerva. 

%===Sun,  9 Mar 2003 15:11:42 -0300 mstorti@spider
%
Volviendo a null_vort. Parece estar andando bien, pero no me queda
claro si la vorticidad que ingresa por el contorno es suficientemente
baja o no. Corriendo la vieja version `beta-3.18.pl21' con Rext=2
resulta que la maxima vorticidad en la corriente libre es
<0.22. Tambien hay una cuestion de que cuando pongo multiplicadores de
Lagrange la convergencia es bastante mala. En esta version vieja la
convergencia es mas o menos (1 om/iter). Por ej:
%
Newton subiter 0, norm_res  =  5.041e-01, update Jac. 1
Newton subiter 1, norm_res  =  4.334e-02, update Jac. 1
Newton subiter 2, norm_res  =  3.958e-03, update Jac. 1
Newton subiter 0, norm_res  =  4.784e-01, update Jac. 1
Newton subiter 1, norm_res  =  4.305e-02, update Jac. 1
Newton subiter 2, norm_res  =  4.179e-03, update Jac. 1
Newton subiter 0, norm_res  =  4.798e-01, update Jac. 1
Newton subiter 1, norm_res  =  4.356e-02, update Jac. 1
Newton subiter 2, norm_res  =  4.298e-03, update Jac. 1
Newton subiter 0, norm_res  =  4.642e-01, update Jac. 1
Newton subiter 1, norm_res  =  4.358e-02, update Jac. 1
Newton subiter 2, norm_res  =  4.429e-03, update Jac. 1
Newton subiter 0, norm_res  =  4.588e-01, update Jac. 1
Newton subiter 1, norm_res  =  4.247e-02, update Jac. 1
%
Tambien surge la pregunta de si al aumentar el nnwt (al menos en la
version vieja) aumenta la vorticidad. Voy a volver a nnwt=1. 
%
15:38:46: Efectivamente, al pasar a nnwt=1 se observa un transitorio
con una alto valor de vorticidad y despues baja a valores menores
(O(0.25)). 
%
16:05:30: Con la version de Mult.Lagrange se llega a valores de
vorticidad de 0.4, en sectores muy concentrados, en la malla
circular. 
%
16:32:41: Con respecto a la convergencia, cuando se usa
Lag. Mult. empieza a ser bastante mala. 
%
Newton subiter 0, norm_res  =  4.451e+00, update Jac. 1
Newton subiter 1, norm_res  =  1.052e+00, update Jac. 1
Newton subiter 2, norm_res  =  7.003e-01, update Jac. 1
Newton subiter 0, norm_res  =  3.501e+00, update Jac. 1
Newton subiter 1, norm_res  =  4.733e-01, update Jac. 1
Newton subiter 2, norm_res  =  3.149e-01, update Jac. 1
Newton subiter 0, norm_res  =  2.637e+00, update Jac. 1
Newton subiter 1, norm_res  =  5.610e-01, update Jac. 1
Newton subiter 2, norm_res  =  3.737e-01, update Jac. 1
Newton subiter 0, norm_res  =  1.832e+00, update Jac. 1
Newton subiter 1, norm_res  =  1.475e-01, update Jac. 1
Newton subiter 2, norm_res  =  9.711e-02, update Jac. 1
%
Esto lo probe con lagrange_diagonal_factor=1e-4 y 1e-6 y con
lagrange_residual_factor=1. 
%
20:27:02: Habia un bug en `nsid.cpp' por el cual la matriz no estaba
escaleada por `ns_id_fac' con lo cual cuando poniamos `ns_id_fac=0' no
estabamos haciendo Newton. Eso puede haber hecho que el Newton no
convergiera bien ya que los nodos ficticios tenian un `ns_id' con
`ns_id_fac=0'. 
%
20:46:30: No mejora demasiado el Newton. En realidad lo que pasa es
que no converge bien el lazo internos. No llega a bajar 2omag en 200 iter. 
Una duda que tengo es con respecto a si en la forma que estoy poniendo
las restricciones lo estoy haciendo simetrico o no. El analisis se
complica por el factor -1 al poner el Jacobiano en el `lagmul.cpp'. 
%
21:06:05: Habia un error que estaba usando solver "petsc" con jacobi,
y convergia muy lentamente, ahora pase a IISD. 
%
21:13:03: Con el IISD converge muy bien y mas rapido. Ahora voy a ver
el lazo de Newton. Dio pivotes nulos con el IISD. Yo pienso que debe
ser aquellos nodos de frontera donde por una lado la estrella esta mal
condicionada y no se genera una condicion (ahora dudo de si esto es
asi (???)) y tambien en los extremos hay nodos que tienen nodos
ficticios pero no se les agrega condicion. 
%
21:21:47: El lazo de newton sigue convirgiendo mal. La vorticidad a la
entrada tiene un pico de 0.35 mas o menos cerca del punto de empalme
del circulo con las rectas. Ahora voy a ver que pasa con la malla
parabolica. 
%
23:24:12: Con la malla parabolica da cosas similares como con la
circular. Ahora voy a hacer un test de convergencia para el Lagrange
multiplier, usando `ns_id'. 

%===Mon, 10 Mar 2003 13:23:48 -0300 mstorti@minerva
%
El `ns_id' tenia puesto por default el `ns_id_lumped_cn1' a 1e-2. Voy
a probar si (habiendo corregido eso) como converge el cylinder con
viscous/inviscid. 
%
17:04:54: Escaleo la ecuacion de vorticidad por h^ndim (basado en la
heuristica de que las ecuaciones de FEM estan pesadas por el volumen
del elemento). No mejora. 
%
Pruebo a sacarle la condicion de vorticidad y converge mejor (pero no
demasiado bien, digamos 1om/iter) y la vorticidad es bastante mas alta
(0.8 en el contorno de entrada). 

%===Tue, 11 Mar 2003 17:51:45 -0300 mstorti@minerva
%
En el `wave.epl', con el elemento de traccion en la interfase
`inviscid_coupling', la vorticidad da un maximo de 0.6. 
%
17:55:40: Sin el `inviscid_coupling' da muy similar. 
%
13:10:17: Imponiendo u y v a la entrada el nivel de vorticidad se hace
muy, muy  bajo. Despues se forma una vorticidad muy alta a la salida y
finalmente parece diverger. 
%
13:36:15: Sacandole la condicion de contorno `v=0' a la salida anda al
pelo. EL nivel de vorticidad es muy bajo (O(1e-4)) y converge
enseguida con steady. 
%
14:17:42: Si no impongo la velocidad transversal entonces da una
vorticidad muy alta. 

%===Thu, 13 Mar 2003 18:24:29 -0300 mstorti@node1.beowulf.gtm
%
Corregido una serie de errores con LES en la parte de generar la
funcion de amortiguamiento de van Driest. No estaba bien construido el
`data_pts', ahora con la funcion initizalize quedo mucho mejor. Falta
a extenderlo a varios elemsets de pared. 
%
18:26:20: Verifique que da lo mismo con 1 y dos procesadores. 

%===Thu, 20 Mar 2003 18:41:33 -0300 mstorti@spider
%
spiller: (voy a cambiar todo a `spillway'). Empieza mas o menos bien,
pero despues de una cierta evolucion la superficie libre empieza a
flamear y cuando le incide un vortice que viene desde la capa limite
sobre la superficie del vertedero se desestabiliza completamente
achicandose cada vez mas hasta que el script de octave no puede
generar la malla. Pruebo  correr con `fs_relax=0' es decir a malla
fija y veo que tambien es inestable ya que probablemente se forman
regiones con velocidades entrantes lo cual debe ser una condicion de
contorno inestable. 
%
20:54:07: Corriendo con slip en la FS anda bien, se produce una capa
de vorticidad que se desprende desde la mitad del vertedero hasta
aguas abajo. En la frontera aguas abajo se produce un problema y
termina divirgiendo. Creo que puede haber problema con la
gravedad. Voy a probar a desactivar la gravedad. 
%
21:26:19: Desactivo la gravedad y va mejor. Le saco la condicion v_t =
0 a la salida y diverge. 
%
22:45:31: Habia un serio bug en los datos y es que el G_body se leia
de las opciones globales, no del elemset. 

%===Sat, 22 Mar 2003 12:43:56 -0300 mstorti@spider
%
Poniendo p = p_stedy en la FS se vuelve inestable aguas abajo. Mi idea
ahora es agregar un termino calmante tipo dv/dt = .... + c v (solo en
la componente vertical). 

%===Sun, 30 Mar 2003 22:59:27 -0300 mstorti@spider
%
Retomo `spillway'. Escribo un ejemplo `wave' donde se modela una
longitud de onda en una canal. Anda bien a Re=100, Fr=0.5. Ahora voy a
probar a Fr=2. 

%===Mon, 31 Mar 2003 09:36:49 -0300 mstorti@spider
%
Detecto que hay un error en el update de la FS. Estoy usando 
$!dx = (!v.!s) !s$ donde $!s$ es la direccion del spin, mientras que lo que
hay que exigir es $(!dx.!n) = Dt (!v.!n)$ de donde, si $!dx = d !s$,
entonces tenemos que $d = Dt (!v.!n)/(!s.!n)$ y por lo tanto 
$!dx = Dt (!v.!n)/(!s.!n) s$ 
%
18:52:41: Corro a Fr=0.1, Re=100, eta0=0.1 y se observa un periodo de
2.1sec lo cual esta +/- de acuerdo con el valor teorico de `
%
$ T = 2\pi/\sqrt{gk} = \sqrt{2\pi L/g} = 1.79 $
%
La velocidad mxima es de 0.47 con lo cual la velocidad de fase
estacionaria deberia ser: 
%
octave> kw=2*pi/5
kw = 1.25663706143592
octave> omega = sqrt(9.8*kw)
omega = 3.50927958448340
octave> c = omega/kw
c = 2.79259596281003
%
Con lo cual se deberian desprender dos ondas con velocidad 0.47 +/-
2.8. O sea que es casi una onda estacionaria. Se observa un
amortiguamiento razonable: en 6 periodos se amortigua la amplitud baja
un 30% mas o menos. Ahora voy a probar con un Froude mayor. 
%
19:24:47: Con Fr=2 no hay problemas pero es raro que se frena bastante
(la velocidad media da bastante menor que la estacionaria
calculada). La amplitud de la onda parece crecer.  No se observa un
movimiento notable de la onda. Voy a probar a correr con una
perturbacion menor, y despues con un fondo variable.
%
22:24:48: A Fr=2, Re=500 y con un bump de t=0.6 anda bien unos cuantos
pasos (190) y despues revienta. Parece que el nivel de agua va bajando
(por falta de conservatividad) y entonces finalmente la capa de agua
se hace muy delgada en el vertedero. 
%
23:02:55: Con t=0.4 tambinen termina divirgiendo, tambien por `secado'
aparentemente. 
%
10:30:54: Corro un caso con mas de 1000 pasos de tiempo. Termina dando
un SISEGV, pero todavia no se porque. El nivel de la FS se mantiene
mas o menos bien. spillway.02-Apr-2003.tgz (45MB gzipped). 
%
15:00:51: Detecte une error, que estaba haciendo mal el update de la
FS, ya que no estaba calculando bien las normales. Ahora el movimiento
es bastante fisico (las ondas van en el sentido correcto) pero la FS
se hace inestable y eventulamente colapsa. Voy a probar a bajar el
Froude. 
%
15:51:20: Con Fr=0.5 no hay problema, la perturbacion a al sup. libre
es muy pequenha. 
%
16:07:26: Para un Fr inicial de 0.7 y mayores se hace
inestable. Mientras que para 0.5 no. Voy a verificar esto bien de
nuevo. 
%
16:58:48: Con Fr=0.5 no se hace inestable y parece llegar a un regimen
periodico (o caotico) pero estable. 

%===Thu,  3 Apr 2003 14:17:40 -0300 mstorti@minerva
%
Con el filtro se hacen menos inestables las ondas cortas pero de todas
formas quedan ondas mas largas inestables que finalmente hacen
diverger la malla. Una posibilidad es aumentar el filtrado, poniendo
un filtro mas `implicito'. 
%
14:26:25: Tambien tiene sentido verificar la influencia del paso de
tiempo. 
%
14:29:17: Con Fr=0.8 y fs_smoothing_coef=0. deverge enseguida. La
longitud de onda de la perturbacion es bien alta (tipo checkerboard)
asi que el smoohthing deberia actuar. Voy a probar con
fs_smoothing_coef=0.1. 
%
14:55:29: Tambien diverge, lo cual es medio raro. Lo largue con
fs_smoothing_coef=0.5. 
%
16:15:50: fs_smoothing_coef=0.5 deberia ser el punto de mayor
disipacion. El factor de amplificacion es 0 para k=pi. Sin embargo se
inestabiliza por una frecuencia muy cercana a pi (tal vez sea
pi). Entonces le pruebo a aplicarle el filtro varias veces (3). Con un
solo filtrado suena en step=77. Vamos a ver si puede avanzar mas con 3
filtrados. 

%===Fri,  4 Apr 2003 07:14:49 -0300 mstorti@spider
%
Con 3 fitrados se mantiene estable. Si bajo Dt=0.02 entonces puedo
correr con fs_smoothing_coef = 0.1. Quiere decir que efectivamente es
un problema de inestabilidad temporal regulado por el
Froude_h. Tambien verifico que el hecho de que las variaciones de `h' en
la superficie son muy pequenhas es razonable. Si aplicamos Bernoulli
entonces tenemos "0.5*Delta(v^2) = g*Delta h" de donde Delta h =
0.5*.3/10 = 0.015 que es lo que se oberva. 
%
15:35:12: Es estable for Fr=1.0 y Dt=0.1 con filtrado. 
%
17:43:41: Con Fr=2.0 y Dt=0.025 se mantiene estable con 3 filtrados
(no se si se mantendra estable si le saco los filtrados). Tambien
quiero empezar a bajar el espesor de la capa de agua. 
%
17:45:50: Voy a aumentar el t a 0.8 y bajar la velocidad, haciendo una
especie de continuacion. 
%
18:12:08: Inicialmente va OK. La perturbacion a la superficie es
pequenha. Ahora voy a aumentar la pendiente al doble, rearrancando de
la solucion anterior. (La solucion no habia convergido demasiado. 

%===Sat,  5 Apr 2003 20:18:00 -0300 mstorti@spider
%
Le baje le viscosidad un factor 100 (esta en 6.26e-5), y se mantiene
estable, pero las perturbaciones a la sup. libre bajan mucho en
tamanho. 

%===Sun,  6 Apr 2003 10:36:59 -0300 mstorti@spider
%
Aumente la pendiente mucho (antes estaba por debajo de 1/1000 y ahora
lo subi a 10% y daba mas o menos pero despues se
desestabilizaba. Lo baje a 5% y finalmente se sigue desestabilizando. 
%
12:46:13: Rearrancando, anda bien al principio, la convergencia se
empieza a deteriorar y finalmente diverge. La divergencia se produce en
un modo relativamente suave y se produce bastante
espontaneamente. Curiosamente parece que uno lo restartea y avanza
mas, hasta que de nuevo diverge. Voy a empezar  estudiar mejor el tema
del Courant critico sobre un problema mas simple, un bump de menor
espesor. 
%
16:54:48: Con el bump de t=0.1, long=2, longitud del dominio 20. De
disenho, slope= 0.03, viscosity= 0.031. Las velocidades son del orden
de 4.5, con lo cual da un Re del orden de 160 y Fr=1.6 (tomando como
longitud la prof. media de 1.). El filtrado es 20 x 0.1. Ahora voy a
probar a bajar la viscosidad un factor 10. 
%
19:40:54: Se la aguanta bien, no llega a un estacionario (o parece
disipar muy poco). Cursiosamente la velocidad no parece cambiar
mucho. 

%===Mon,  7 Apr 2003 16:10:14 -0300 mstorti@minerva
%
Con el bump (t=0.5, L=2, Lx=5, h=1, slope=0.03) el estacionario
converge casi en el limite y despues al correr el no estacionario con
(Dt=0.05, nfilt=20, fs_smoothing_coef=0.1) diverge en un modo suave,
para en el paso 18. 

%===Sun, 11 May 2003 14:11:42 -0300 mstorti@spider
%
Retomando el tema del spillway. No lo logro hacer andar sin
`cyclic_fs'. Vuelvo a como estaba antes. 
%
14:44:13: Recupero `beta-3.18.pl67' y poniendo `slope=0.03' reproduce
lo que dice en el `notes.txt' es decir que anda el FS hasta el paso
18. Voy a tratar de recuperar estos datos pero sin condiciones
periodicas. 

%===Mon, 19 May 2003 12:23:21 -0300 mstorti@node1.beowulf.gtm
%
read_vector:BUG: Sintoma: Se nota que read_vector tarda muchisimo par
vectores grandes. (spillway, 73K nodos, 290K dofs). Se observa que la
gran tardanza es en idmap->solve, y esto debido a ineficiencia en
set<int>. Se reemplaza set<int> por un vector<int> lo cual puede ser
mejor incluso en memoria. 
%
Con set<int> tarda 4minutos. 
Con vector<int> tarda 18secs.
%
12:45:17: verifico que en spillway da el mismo resultado. 

%===Thu,  5 Jun 2003 13:41:07 -0300 mstorti@minerva
%
En minerva no puede llegar a compilar el `advdif_bubbly', se queda
laburando indefinidamente. Le pongo -O0 y compila OK. 

%===Tue, 01 Jul 2003 23:25:01 -0300 mstorti@spider
%
Adaptando a RH 8.0: Hubo varias diferencias sobre todo con el cambio
del compilador. 
1./ No acepta mas valores por defeecto en las definiciones. 
2./ Hay que agregar "using namespace std;" en varios lados
3./ hay que agregar unos "typedef typename" en 
      varios templates, si no tira un error tipo "implicit
      typename..." 
4./ No acepta convertir `vector<T>::iterator' a 'T*'. Pero si se puede
       hacer `vector<T>::iterator q; T* d = &*q;
%
23:55:47: Verificado que el &*trick anda bien. Todo fmatep.cpp ->
fm2eperl.cpp compila todo OK. 

%===Wed,  2 Jul 2003 14:00:54 -0300 mstorti@minerva
%
Compila todo petscfem en minerva (GCC 2.96) con algunos agregados para
que compile en GCC 3.x. 
%
20:21:15: Compila todos los fuentes pero despues da error el
linkeditar. 
%
21:01:39: Compila y linkea con g_c++

%===Thu, 03 Jul 2003 01:29:53 -0300 mstorti@spider
%
Con g_c++ corre casi todos los tests:
> OK: 148.  Not OK: 4. Couldn't open: 9. Total: 161
> Start section: "Misc tests."
> Testing: "Idmap class" on file "idmap/tidmap.sal"...not OK.
>         --->  Couldn't find "testing for size of idmap n=.*, number of operations .*"
> Testing: "Constraint bug, cant have null rows in idmap class" on file "circ/check_circ.sal"...not OK.
>         --->  Couldn't find "error in u symmetry = .* OK . 1"
> can't open bug100/save.state.tmp
> Summary: "Misc tests." -- OK: 18.  Not OK: 2. Couldn't open: 1. Total: 21
> --------
> Start section: "Misc tests."
> Testing: "Read_vector function." on file "sqcav/verif.read_ini.tmp"...not OK.
>         --->  Couldn't find "v=p=0 OK \? 1"
> Summary: "Misc tests." -- OK: 11.  Not OK: 1. Couldn't open: 0. Total: 12
> --------
> Start section: "IISD solver. Checking with advdif"
> Testing: "newff/test case adv_dif_temp_iisd" on file "newff/adv_dif_temp_iisd.vrf.tmp"...not OK.
>         --->  Couldn't find "rel. max error  <tol\(.*\) OK\? 1"
> can't open newff/adv_temp_cp_iisd.vrf.tmp
> can't open newff/dif_temp_iisd.vrf.tmp
> can't open newff/full_full_jacs_iisd.vrf.tmp
> can't open newff/full_jacs_iisd.vrf.tmp
> can't open newff/full_jacs_cp_iisd.vrf.tmp
> can't open newff/pure_adv_iisd.vrf.tmp
> can't open newff/reac_adv_dif_temp_y_iisd.vrf.tmp
> can't open newff/std_ard_x_y_iisd.vrf.tmp
> Summary: "IISD solver. Checking with advdif" -- OK: 0.  Not OK: 1. Couldn't open: 8. Total: 9
%
07:45:09: Con BOPT=O_c++ tambien resuelve casi todos los
casos. Algunos faltan porque no termino de correr. 
> --------
> Start section: "Plano tests on advdif"
> Testing: "Adv/Sh.Water FastMat2/cons_supg" on file "plano/plano_cons.sal"...not OK.
>         --->  Couldn't find "time_step 1, time: 0.1, res = 2.56344.*-02"
> Summary: "Plano tests on advdif" -- OK: 0.  Not OK: 1. Couldn't open: 6. Total: 7
> --------
> Start section: "Misc tests."
> Testing: "Idmap class" on file "idmap/tidmap.sal"...not OK.
>         --->  Couldn't find "testing for size of idmap n=.*, number of operations .*"
> Testing: "Constraint bug, cant have null rows in idmap class" on file "circ/check_circ.sal"...not OK.
>         --->  Couldn't find "error in u symmetry = .* OK . 1"
> Summary: "Misc tests." -- OK: 18.  Not OK: 2. Couldn't open: 1. Total: 21
> --------
> Start section: "Misc tests."
> Testing: "Read_vector function." on file "sqcav/verif.read_ini.tmp"...not OK.
>         --->  Couldn't find "v=p=0 OK \? 1"
> Summary: "Misc tests." -- OK: 11.  Not OK: 1. Couldn't open: 0. Total: 12
> --------
> Start section: "New flux function tests."
> Testing: "newff/test case reac_adv_dif_temp_y" on file "newff/reac_adv_dif_temp_y.vrf.tmp"...not OK.
>         --->  Couldn't find "rel. max error  <tol\(.*\) OK\? 1"
> Testing: "newff/test case reac_dif_temp" on file "newff/reac_dif_temp.vrf.tmp"...not OK.
>         --->  Couldn't find "rel. max error  <tol\(.*\) OK\? 1"
> Summary: "New flux function tests." -- OK: 17.  Not OK: 2. Couldn't open: 0. Total: 19

%===Thu,  3 Jul 2003 09:47:31 -0300 mstorti@minerva
%
Aparentemente ahora _GNU_SOURCE viene definido por default. 

%===Fri,  4 Jul 2003 16:32:53 -0300 mstorti@node1.beowulf.gtm
%
Eficiencia: Comparando los P4 actuales en el cluster con spider (P4
2.4Ghz, 512Mb RAM DDR 333). Tiene un pico de 920Mflops para n=20 con el dyntest
y despues baja a 170MFlops. Con PETSc-FEM, cavidad cubica para n=20
(40000 elementos, 8000 nodos) da un rate d 0.2sec/ke para spider y 
0.57-0.65 secs para node1!!
Resolucion: 6.7891 node15
            11.965 node1

%===Mon,  7 Jul 2003 17:07:41 -0300 mstorti@minerva
%
Cuando compilo en minerva con la nueva version de Newmat entonces da
un SIGSEGV. Vuelvo a recompilar todo petscfem. 
%
19:26:58: Un fix para octave. EL plot en RH 8.0 (octave 2.1.36) no
grafica, mas de 6 o 7 curvas. Aparentemente es un bug en sprintf, que
va armando el gplot command y cuando tiene que imprimir un comando muy
largo se queda corto. Lo arregle con el siguiente parche. 
%
--------- cut here __plt2vm__.m patch ----------------------
--- __plt2vm__.m.orig	2002-09-03 18:29:54.000000000 -0300
+++ __plt2vm__.m	2003-07-13 19:24:48.000000000 -0300
@@ -64,8 +64,10 @@
       k++;
     endif
     for i = 2:y_nc
-      cmd = sprintf ("%s, tmp(:,%d:%d:%d) %s", cmd, 1, i, i+1,
-                     deblank (fmt (k, :)));
+#      cmd = sprintf ("%s, tmp(:,%d:%d:%d) %s", cmd, 1, i, i+1,
+#                     deblank (fmt (k, :)));
+      cmd = [cmd, sprintf (", tmp(:,%d:%d:%d) %s", 1, i, i+1,
+                     deblank (fmt (k, :)))];
       if (k < fmt_nr)
         k++;
       endif
--------- ends here __plt2vm__.m patch ----------------------

%===Thu, 24 Jul 2003 12:48:41 -0300 mstorti@node1.beowulf.gtm
%
En tempfun2.cpp `piecewise_linear' amplitude function, para verificar
que el vector estuviera ordenado comparava con t_v[n-2] y yo creo que
hay que comparar con t_v[n-1]. 

%===Wed,  6 Aug 2003 16:57:15 -0300 mstorti@node1.beowulf.gtm
%
Modifier `fstack.cpp' so that now reads DOS files. It simply replaces
the `\r' by a `\0'. 


%===Fri, 25 Jul 2003 22:51:58 -0300 mstorti@spider
%
Hice el merge de la branch beta-3.19.pl32 donde entre otras cosas
habia agregado modificaciones para que 
* fix `chmod 755 scripts' in sw target
* fix `LIBRETTO_I' makefile variable
* patch for Octave plot functions
* add `interface_full_preco_fill' option
* fix `src/tempfun.cpp' `ramp' function
* add `elasticity' fields to gmv.pl 
* adds some `axi' fixes by Beto
* adds wpg_sum for gpdata
* adds mesh update for dx_hook
* fixes tempfun2.cpp `piecewise_linear' function. 
%
22:58:51: No queria linkeditar en spider por problemas con el
ANN. Tuve que recompilar el ANN. 

%===Sun, 17 Aug 2003 10:29:56 -0300 mstorti@spider
%
Encontre un error en `dxhook.cpp', cuando lee la malla no se cerraba
el archivo lo cual ocasionaba un "Too many open files" cuando se leian
demasiados archivos. 

%===Sun, 17 Aug 2003 13:26:05 -0300 mstorti@spider
%
creo un branch/root isp-root/isp-branch para hacer lo del
precondicionamiento de interface. 

%===Sun, 24 Aug 2003 13:40:23 -0300 mstorti@spider
%
Implemento lo del precondicionamiento de interfase con un numero
arbitrario de capas. Pero por ahora me cuesta ver bien si sirve. Probe
en tets/sqcav con la cavidad cuadrada con `qharm' (Laplace) y en
general incluso es dificil que convenga el IISD. Por lo general es
resulta mejor directo o PETSc global. Paso a la cavidad cubica y
con una malla de 20x20x20x5 tetras ya cuesta un toco resolverla con
PETSc global. A las 300 iter llega a res=5e-4 y de ahi practicamente
no baja. En el punto maximo llega a unos 240MB de memoria. AHora voy a
probar con subpart=1 o sea LU. 
%
13:51:03: Con LU pasa de los 500MB. 
%
13:55:30: Con iisd_subpart=16 y sin ISP pasa algunos plateaus pero no
llega a la precision requerida en las 600 iter. Despues de las 600
iter llega a consumir unos 150MB.  Voy a probar a bajar el subpart a
8.
%
14:39:07: Habia un error que no estaban bien puestas las condiciones
de contorno (faltaba fijar algunas componentes de velocidad). Ahora
que estan bien puestas se puede llegar a Re=10 OK. Re=30 ya no puedo
capturar la solucion (de un saque por lo menos). 
%
18:41:37: Con la cavidad cubica a Re=100 parece OK. El IISD anda bien,
por lo menos el 1er paso de tiempo. PETSc-global le cuesta mucho mas
el primero y despues todavia mas el segundo Dt. 
%
18:47:38: Con use_interface_full_preco_nlay=1 todavia converge
mejor. Ahora voy a probar aumentando el nlay. 

%===Mon, 25 Aug 2003 09:41:24 -0300 mstorti@node1.beowulf.gtm
%
Corro en paralelo. Voy a correr a N=28 en paralelo y secuencial y
despues N=50 en paralelo. 
%
12:03:07: Para N=50 podria correr
foreach  $N (50) {
    foreach  $subpart (16,64) {
	@nlay = (1,2);
	@nlay = (0) if $subpart==0;
	foreach  $nlay (@nlay) {
	    @isp_maxits = (4,16);
	    @isp_maxits = (0) if $nlay==0;
%
14:43:17: Corro a N=50 con subpart=128,256,512,1024. Ojo que estoy
corriendo con rtol=1e-8. Podria hacer una corridita con rtol=1e-4 para
ver. 
%
15:43:53: Con N=50, rtol=1e-4 el optimo se produce con
sbp=256, tav=52sec, maxmem=290MB. Ahora voy a probar a ver como
escalea el directo. 

%===Wed, 27 Aug 2003 12:43:32 -0300 mstorti@node1.beowulf.gtm
%
Con N=60 (1M+ elements) puedo correr para Re=1000. Para Re=10000
revienta por memoria. Probablemente se deba a que hay que iterar mas. 

%===Thu, 28 Aug 2003 08:44:45 -0300 mstorti@node1.beowulf.gtm
%
Curiosamente detecto que ahora el dyntest da en los nodos de 1.7GHz
con Rambus picos de 450Mflops, cuando antes daba 300+. Esto quiere
decir que en realidad hubo un cambio en algo ne el cluster
(compilador, SO, ???). 
%
La cavidada cubica corrio bien durante la noche pero va demasiado
lenta y ahora que pienso me parece que el nodo que agregue que antes
creo que era el 9 y ahora lo puse como 2 cuando lo ponias la velocidad
iba a la mitad. Le corri el dyntest y da OK (la misma velocidad que en
los otros). AHora voy a correr la cavidad en secuencial en el 2 y el 4
y comparar. 
%
Evluacion de elementos cubcav 20x20x20 en node4. 
[proc] - busy[sec] - rate[sec/Ke] - upl/dwl[sec] - %idle
[proc 0]         11.5        0.286         5.66    33.1
total       17.1secs
%
en node2:
[proc] - busy[sec] - rate[sec/Ke] - upl/dwl[sec] - %idle
[proc 0]         9.63        0.241         6.05    38.6
total       15.7secs
%
Con lo cual es similar. Ahora voy a probar a correr en paralelo pero
sin node2, a ver si volvemos a la vieja tasa de calculo. 
%
10:57:29: Cubcav 30x30x30 en node4
[proc] - elems - compt[sec](rate[sec/Ke]) - upl/dwl[sec] - assmbly[sec]  - other[sec]
[proc 0]   135000       31/ 60%(   0.23)       21/ 40%   0.0078/  0%   0.0091/  0%
total       51.8secs
%
Cubcav 60x60x60 en 7 nodos:
[proc] - elems - compt[sec](rate[sec/Ke]) - upl/dwl[sec] - assmbly[sec]  - other[sec]
[proc 0]   190045       46/ 35%(   0.24)       31/ 23%       55/ 41%      1.2/  1%
[proc 1]   190935       48/ 36%(   0.25)       33/ 25%       52/ 39%      1.2/  1%
[proc 2]   191187       47/ 35%(   0.25)       34/ 26%       51/ 38%     0.89/  1%
[proc 3]   179718       44/ 33%(   0.25)       31/ 23%       57/ 43%     0.79/  1%
[proc 4]   175230       44/ 33%(   0.25)       31/ 23%       58/ 43%     0.73/  1%
[proc 5]    78092       29/ 21%(   0.37)       24/ 18%       80/ 60%     0.84/  1%
[proc 6]    74793       23/ 17%(    0.3)       16/ 12%       94/ 70%     0.86/  1%
total        133secs
%
Vemos que el assmbly (probablemente tiempo de comunicacion) es
importante (40%). Tambien el upload (insertar en la matriz).  La
insercion en la matriz hay que hacerla tambien en secuencial
asi que no afecta la eficiencia paralela y aparentemente crece
linealmente con el numero de elementos. Voy a probar a particionar
mejor a ver si se puede reducir el tiempo de comunicacion. 
%
11:33:28: Modificando algunos parametros puedo bajar
significativamente el tiempo de assembly
%
compact_profile_graph_chunk_size 40000
max_partgraph_vertices 5000
max_partgraph_vertices_proc 5000
chunk_size 5000
%
[proc] - elems - compt[sec](rate[sec/Ke]) - upl/dwl[sec] - assmbly[sec]  - other[sec]
[proc 0]   190045       44/ 43%(   0.23)       31/ 30%       26/ 26%     0.98/  1%
[proc 1]   190935       45/ 44%(   0.24)       33/ 32%       24/ 23%     0.97/  1%
[proc 2]   191187       45/ 44%(   0.24)       33/ 32%       24/ 23%     0.66/  1%
[proc 3]   179718       42/ 41%(   0.24)       31/ 30%       29/ 28%     0.52/  1%
[proc 4]   175230       42/ 41%(   0.24)       31/ 31%       29/ 28%     0.49/  0%
[proc 5]    78092       26/ 25%(   0.33)       23/ 22%       53/ 52%     0.63/  1%
[proc 6]    74793       20/ 20%(   0.27)       16/ 15%       66/ 65%     0.66/  1%
total        103secs
%
No se bien cuales son los importantes voy a probar solamente con el
chunk_size. 
%
11:51:11: Prueba a cambiar solo el chunk_size y efectivamente es lo
mas importante. El tiempo de assembly baja a 26/24secs. Ahora pruebo a
llevar el chunk_size a 10000. La memoria requerida parece no ser
demsiado influyente. 
%
Performance report elemset "nsi_tet_les_fm2" task "comp_mat_res"
[proc] - elems - compt[sec](rate[sec/Ke]) - upl/dwl[sec] - assmbly[sec]  - other[sec]
[proc 0]   190045       44/ 45%(   0.23)       31/ 32%       22/ 22%     0.96/  1%
[proc 1]   190935       45/ 46%(   0.23)       33/ 34%       19/ 19%     0.95/  1%
[proc 2]   191187       44/ 45%(   0.23)       33/ 34%       20/ 20%     0.61/  1%
[proc 3]   179718       41/ 42%(   0.23)       31/ 31%       25/ 26%     0.47/  0%
[proc 4]   175230       42/ 43%(   0.24)       31/ 31%       24/ 25%     0.43/  0%
[proc 5]    78092       25/ 26%(   0.32)       23/ 24%       48/ 50%     0.59/  1%
[proc 6]    74793       19/ 20%(   0.26)       15/ 16%       62/ 64%      0.6/  1%
total       97.4secs
%
Todavia baja un poquito mas. En todos baja 4 segundos. Voy a probar a
llevarlo a 40,000 a ver que pasa. 
%
15:13:47: Baja un poquito mas y la memoria no aumenta demasiado. A
esta altura hay que atacar el up/download time. 

%===Fri, 29 Aug 2003 12:20:11 -0300 mstorti@minerva
%
Estoy tratando de reemplazar las llamadas a MatSetValue por llamadas a
MatSetValue*s*. Ya casi lo tengo escrito pero resulta que no se parece
ganar demasiado. El calculo da bien, falta implementar el `A_LL_other'
y el `isp_set_values'. 
%
13:39:07: Para la cubcav 20x20x20 el tiempo de upload baja de 7.6 a
6.3. Si comento los MatSetValues baja a 5. Con lo cual quiere decir
que tengo 5secs de ineficiencia en mi programacion y despues el
MatSetValues se llevaba 2.6secs y con la historia del MatSetValues lo
lleve a 1.3. Puedo cuantificar cuanto se lleva mi pfmat->set_values()
comentando la llamada en upload_vector().
%
17:38:38: Se lleva poco. Aparentemente 0.9sec ya que al comentarlo
pasa de 5 a 4.1sec. El target seria bajar estos 4.1sec. 

%===Fri, 29 Aug 2003 21:46:29 -0300 mstorti@spider
%
Quiere decir entonces que la principal ineficiencia mia esta en el
upload_vector mismo. 
%
10:36:55: Voy a estimar el tiempo que tarda el ensamble del vector y
que esta mas abajo en upload_vector(). Es muy poco, baja de 3.5ec a
3.4sec. 
%
10:41:29: La llamada a matrices ahora se esta llevando 1.4secs, es
decir casi el 40% del upload_vector().
%
10:44:15: Con la vieja version PETSc esta en 4.3 secs. O sea que no
hemos ganado demasiado. 
%
14:58:18: Usando vectores C en vez de dvector<int>::e() se bajan un
poco mas los tiempos. En total pude bajar la parte de upload a la
mitad. Ejemplo cubcav 30x30x30
%
% -----------
% Nueva version
[0]   135000       15/ 67%(   0.11)      7.5/ 33%  0.00021/  0%   0.0044/  0%
total       22.5secs
% -----------
% vieja version 
[0]   135000       13/ 46%(  0.097)       15/ 54%  0.00024/  0%    0.012/  0%
total       28.4secs
% -----------
%
15:11:46: verificado que da lo mismo que con la version vieja. 
%
16:46:18: En estos momentos el upl/download lleva 7.3secs (cubcav
30x30x30), de los cuales el download es 1.2secs, 2.4secs es el
upload_vector() en si mismo y el resto 3.6 secs es el
pfmat->set_values() (ahi hay parte de codigo mio y parte de PETSc). 
%
16:55:07: Sin la llamada a MatSetValues da 4.3secs.
[0]   135000       13/ 75%(  0.098)      4.3/ 24%  0.00021/  0%    0.081/  0%
total       17.6secs
%
16:59:17: De manera que el MatSetValues se come 3secs. Tenemos
entonces mas o menos
%
download: 1.2secs
upload_vector: 2.4secs
pfmat->set_values: 0.6secs
PETSc MatSetValues: 3secs
%
O sea que sobre todo hay que bajar los tiempos del upload_vector(). 
%
18:16:24: Logre bajar los tiempos 7.3. Comentando el bloque baja a 6.6
con lo cual ese bloque consume 0.6 (antes era 0.9). 
%
18:33:19: Ahora esta dando
% upload       7.26secs, rate     0.0538 secs/Ke
Pruebo a comentarle la linea *w++ = ... ; y baja a 6.54 secs. 
% upload       6.54secs, (    0.0484 secs/Ke)
Comentando todo el bloque da similar asi que la mayor parte esta en
esa linea. 
%
18:52:01: Tenemos ahora:
donwload_vector: 0.54secs
upload_vector: 1.81secs
pfmat->set_values: 1.87secs
PETSc MatSetValues: 3.03secs
%
19:52:07: Pasando otras partes de upload_vector() a vectores C en vez
de dvector() llego a 
% upload        6.5secs, (    0.0481 secs/Ke)
Con lo cual el tiempo de upload_vector() debe haber bajado
0.8secs. O sea que estamos en:
%
donwload_vector: 0.54secs
upload_vector:   1.secs
pfmat->set_values:  1.87secs
PETSc MatSetValues: 3.03secs
%
20:23:58: Tocando un poco mas el iisdmat.cpp llego a 5.78secs. Quiere
decir que baje 0.72secs, es decir que estamos en
%
donwload_vector: 0.54secs
upload_vector:   1.secs
pfmat->set_values:  1.15secs
PETSc MatSetValues: 3.03secs

%===Sun, 31 Aug 2003 09:19:57 -0300 mstorti@spider
%
De repente los tiempos subieron mucho, pruebo a volver atras e ir
aplicando pathes de a poco. Ya el grueso lo aplique. Pero faltan
algunas cosas. Ahora pruebo a agregarle el `const' a la
nueva `get_row'. A este le tengo desconfianza aunque teoricamente no
deberia hacer nada. 
%
09:27:46: El `const' anduvo bien. 
%
10:51:56: Encontre el error. Ahora estamos en
upload       5.19secs, (    0.0385 secs/Ke)
download      0.332secs, (   0.00246 secs/Ke)
upl/dwl: 5.5secs
%
11:52:59: Ahora voy a ver el tema de que ande cuando hay constraints,
condiciones periodicas etc... Despues voy a ver el funcionamiento en
paralelo (todavia hay algunas cosas por implementar). 
%
12:17:22: Corro el sector (que usa solver PETSc) y corre pero da
mal. Asi que debe haber algun error. 
%
12:21:43: Aparentemente el error esta en dofmap.cpp
Dofmap::get_nodal_value. 
%
12:29:13: Encontre el error. salvo. 
%
12:34:53: El reporte actual es:
[proc] - elems - compt[sec](rate[sec/Ke]) - upl/dwl[sec] - assmbly[sec]  - other[sec]
[0]   135000       13/ 70%(  0.094)      5.4/ 30%  0.00021/  0%    0.015/  0%
upload       5.15secs, (    0.0381 secs/Ke)
download      0.276secs, (   0.00205 secs/Ke)
total       18.2secs
%
13:26:36: Verificado que con nlay=1 da lo mismo que daba antes. Lo
comparo con la version `petscfem-beta-3.19.pl47'. 


%===Mon, 01 Sep 2003 07:40:00 -0300 mstorti@spider
%
Corro los tests. Corre muchos. El report es:

> Testing: "Square cavity with fract. step, Re400, N=20" on file "sqcav/check.fs.verif.tmp"...not OK.
>         --->  Couldn't find "Square cavity at Re=400. Error < tol OK \? 1"
> Testing: "Nutating cylinder" on file "nutatcyl/checknutcyl.verif.tmp"...not OK.
>         --->  Couldn't find "Roll moment OK \? 1"
> Testing: "Viscous force intgrator on rigid cylinder" on file "nutatcyl/rigid.verif.tmp"...not OK.
>         --->  Couldn't find "Mz\(rigid\) OK \? 1"
> Testing: "Max volume ratio large deformation mesh mov." on file "mmove/step.verif.tmp"...not OK.
>         --->  Couldn't find "Mesh OK \(all areas >0\) \? 1"
> Testing: "Max volume ratio large deformation mesh mov." on file "mmove/step3d.verif.tmp"...not OK.
>         --->  Couldn't find "All tetra volumes > 0 OK \? 1"
> Testing: "Idmap class" on file "idmap/tidmap.sal"...not OK.
>         --->  Couldn't find "testing for size of idmap n=.*, number of operations .*"
> Testing: "Constraint bug, cant have null rows in idmap class" on file "circ/check_circ.sal"...not OK.
>         --->  Couldn't find "error in u symmetry = .* OK . 1"
> Testing: "Read_vector function." on file "sqcav/verif.read_ini.tmp"...not OK.
>         --->  Couldn't find "v=p=0 OK \? 1"
> Testing: "newff/test case reac_dif_temp" on file "newff/reac_dif_temp.vrf.tmp"...not OK.
>         --->  Couldn't find "rel. max error  <tol\(.*\) OK\? 1"
> Testing: "Unsteady advection of rotating cone" on file "advec/rotating_cone.verif.tmp"...not OK.
>         --->  Couldn't find "Error in x position < tol OK \? 1"
> Testing: "Unsteady advection of rotating cone" on file "advec/rotating_cone_noise.verif.tmp"...not OK.
>         --->  Couldn't find "Error in x position < tol OK \? 1"
> Testing: "newff/test case adv_dif_temp_iisd" on file "newff/adv_dif_temp_iisd.vrf.tmp"...not OK.
>         --->  Couldn't find "rel. max error  <tol\(.*\) OK\? 1"
> Testing: "Adv/Sh.Water FastMat2/cons_supg" on file "plano/plano_cons.sal"...not OK.
>         --->  Couldn't find "time_step 1, time: 0.1, res = 2.56344.*-02"
> Testing: "Square cavity with fract. step, Re400, N=20" on file "sqcav/check.fs.verif.tmp"...not OK.
>         --->  Couldn't find "Square cavity at Re=400. Error < tol OK \? 1"
> Testing: "Nutating cylinder" on file "nutatcyl/checknutcyl.verif.tmp"...not OK.
>         --->  Couldn't find "Roll moment OK \? 1"
> Testing: "Viscous force intgrator on rigid cylinder" on file "nutatcyl/rigid.verif.tmp"...not OK.
>         --->  Couldn't find "Mz\(rigid\) OK \? 1"
> Testing: "Max volume ratio large deformation mesh mov." on file "mmove/step.verif.tmp"...not OK.
>         --->  Couldn't find "Mesh OK \(all areas >0\) \? 1"
> Testing: "Max volume ratio large deformation mesh mov." on file "mmove/step3d.verif.tmp"...not OK.
>         --->  Couldn't find "All tetra volumes > 0 OK \? 1"
> Testing: "Idmap class" on file "idmap/tidmap.sal"...not OK.
>         --->  Couldn't find "testing for size of idmap n=.*, number of operations .*"
> Testing: "Constraint bug, cant have null rows in idmap class" on file "circ/check_circ.sal"...not OK.
>         --->  Couldn't find "error in u symmetry = .* OK . 1"
> Testing: "Read_vector function." on file "sqcav/verif.read_ini.tmp"...not OK.
>         --->  Couldn't find "v=p=0 OK \? 1"
> Testing: "Unsteady advection of rotating cone" on file "advec/rotating_cone.verif.tmp"...not OK.
>         --->  Couldn't find "Error in x position < tol OK \? 1"
> Testing: "Unsteady advection of rotating cone" on file "advec/rotating_cone_noise.verif.tmp"...not OK.
>         --->  Couldn't find "Error in x position < tol OK \? 1"

%===Tue,  2 Sep 2003 11:58:57 -0300 mstorti@node1.beowulf.gtm
%
Corre la cavidad cubica con 2Mtet. Corre toda la noche, 16hs+ sin
problemas. Lo corto y que le agrego mas memoria a un nodo y llego a
2.4Mtet. 

%===Tue, 02 Sep 2003 23:59:20 -0300 mstorti@spider
%
Corre casi todos los tests. 
Con g_c++: falla en dos:
%
> Start section: "All tests"
> Summary: "All tests" -- OK: 160.  Not OK: 2. Couldn't open: 0. Total: 162
%
> Testing: "newff/test case reac_adv_dif_temp_y" on file "newff/reac_adv_dif_temp_y.vrf.tmp"...not OK.
>         --->  Couldn't find "rel. max error  <tol\(.*\) OK\? 1"
Hay otro error similar, pero me parece ahora qye se debe a cierta
dispersion que hay en los valores, es decir en algunos tests se puede
producir una fluctuacion que haga que a veces el programa de mal. 
Con O_c++ corre todos los test. 
%
> Start section: "All tests"
> Summary: "All tests" -- OK: 162.  Not OK: 0. Couldn't open: 0. Total: 162

%===Sat, 06 Sep 2003 13:15:33 -0300 mstorti@spider
%
Visualiando la cavidad cubica uso los 37 pasos con 60x60x60 que estan
en STEPS-2003-SEP-01 en CD-2003-SEP-05-cubcav. Quiero ver el impacto
que tiene en el tiempo de visualizacion el leer los archivos del CD o
del HD.  Tarda 1'15'' en visualizar 3 pasos leyendo directamente del
disco duro. 
%
13:22:08: Leyendo del CD tarda exactamente lo mismo. 

%===Sun, 07 Sep 2003 09:35:56 -0300 mstorti@spider
%
Estoy tratandod de hacer que PETSc-FEM le mande a ExtProgImport solo
el estado, no las conectividades ni las coordenadas. Normalmente
deberiamos tener por nodo en NS:
%
coord: 3*8 = 24
connec: 5*4*4 = 100
state: 4*8 = 32
%
Con lo cual se ve que en este caso mandando solo el estado se ganaria
un factor 1/5 en velocidad. Estoy tratando de usar el cache, pero en
un cierto momento se clava. Cuando quiero hacer el DXSetCacheEntry. 
%
09:59:54: Aparentemente no es el DXSetCacheEntry, es una historia con
el restart del DX. Pasa incluso con las versiones mas viejas. 
%
10:19:10: Ahora recupera bien los datos del cache. Ahora voy a probar
a reemplazar las coordenadas actuales de aquellas sacadas del cache. 
%
10:32:45: Anda bien usando las del cache. 

%===Tue,  9 Sep 2003 13:36:00 -0300 mstorti@minerva
%
Upgrade a PETSc 2.1.6:
* Bajamos petsc.tar.gz de http://www-unix.mcs.anl.gov/petsc/ 
57fe659c51083771c081e88994b5d317  petsc-2.1.6.tar.gz
* Aplicamos el parche `petsc_patch_all-2.1.6'. 
* Configuracion: (esta en patches/petsc-2.1.6-config.patch). 
* Para usar las librerias MPI dinamicas volvemos a configurar MPI con
       ./configure --enable-sharedlib -prefix=/usr/local/mpi recompilamos y
       make install. 
* Compilamos PETSc
* En PETSc-FEM Makefile.base cambiamos la version querida por
    PETSc-FEM a 2.1.6 y agregamos -lpetscfortran a las librerias de
    PETSc usadas por PETSc-FEM. 

--- Makefile.base	2003/09/08 20:01:17	1.172
+++ Makefile.base	2003/09/09 16:47:51
@@ -43,7 +43,7 @@
 # than beta-3.04 will specify the version of PETSc needed through the variable
 # `PETSC_VERSION_USED_BY_PETSCFEM'
 ifeq ($(PETSC_VERSION_USED_BY_PETSCFEM),)
-PETSC_VERSION_USED_BY_PETSCFEM=2.1.3
+PETSC_VERSION_USED_BY_PETSCFEM=2.1.6
 PETSC_DIR=$(PETSC_ROOT_DIR)/petsc-$(PETSC_VERSION_USED_BY_PETSCFEM)
 endif
 
@@ -257,6 +257,9 @@
 	$(MAKE) clean local_dist_clean &> /dev/null
 	for dir in $(CLEAN_DIRS) ; do $(MAKE) -C $$dir distclean  ; done
 
+## PETSc Libraries used by most PETSc-FEM applications
+PETSCFEM_PETSC_LIBS := ${PETSC_SLES_LIB} -lpetscfortran
+
 LDFLAGS = -Wl,-E,-rpath,$(LD_LIBRARY_PATH)	\
           $(PROG_LIB)				\
 	  -L$(PETSCFEM_DIR)/src			\
@@ -267,7 +270,7 @@
           -lmetis  -L$(NEWMAT)  -lnewmat	\
           -lglib -L$(LIBRETTO_LIB) -libretto	\
           $(ANN_LDFLAGS)			\
-          -lc ${PETSC_SLES_LIB}			\
+          -lc ${PETSCFEM_PETSC_LIBS}            \
           $(MESCHACH)/libmes.a			\
           $(SUPERLULIB) $(SSL_LDFLAGS)		\
 	  $(PTHREADS_LIB)			\
@@ -281,7 +284,7 @@
           -lmetis  -L$(NEWMAT)  -lnewmat	\
           -lglib -L$(LIBRETTO_LIB) -libretto	\
           $(ANN_LDFLAGS)			\
-          -lc ${PETSC_SLES_LIB}			\
+          -lc ${PETSCFEM_PETSC_LIBS}            \
           $(MESCHACH)/libmes.a			\
           $(SUPERLULIB) $(SSL_LDFLAGS)		\
 	  $(PTHREADS_LIB)

%===Wed, 10 Sep 2003 23:43:38 -0300 mstorti@spider
%
No corre el caso de la cavidad cuadrada con fractional step. Da pivote
nulo. Voy a probar a correrlo con la beta-3.20. 
%
23:53:35: Con el viejo upload parece andar bien. Con la beta-3.20
parece andar mal asi que decidamente parece tener que ver mas con la
nueva versionde upload que con la nueva versiond e PETSc. 

%===Thu, 11 Sep 2003 10:26:21 -0300 mstorti@node1.beowulf.gtm
%
La cavidad cubica no queria andar porque le habia agregado el LES y no
generaba el `wall_data' etc... Lo voy a correr con A_van_Driest=0.
%
12:52:21: Anda, hace unas 43 iteraciones y da un SEGV en el nodo17
(proc 3). 
%
13:24:09: A partir de ese `outvector.out' lo arranque y parece andar
bien. Hizo 9 pasos de tiempo OK. 

%===Fri, 12 Sep 2003 21:47:57 -0300 mstorti@spider
%
Buscando un test para el disconnected graph test. 
%
14:30:20: Encontre un bug que ya habia detectado previamente en
`fstack'. No podia leer lineas de un caracter. Estaba mal que cuando
verificaba la longitud de la linea lo hacia con `readlen>1' cuando en
realidad era `readlen>0'  ya que `strlen()' devuelve la longitud del
string sin incluir el '\0' al final. 
%
14:38:48: Volviendo a generar el test para el `disconnected graph bug'
usando una mallita de 11 nodos independientes con `ns_id'. 

%===Sun, 14 Sep 2003 10:32:53 -0300 mstorti@spider
%
Tengo mas decidido el sistema de versiones, de manera de mantener la
convencion de que las impares son inestables y las pares
estables. Hariamos algo asi
%
3.21=3.20 (stable branch) --> 3.20.pl1 --> 3.20.pl2 --> ...
3.21.pl1
3.21.pl2 
...
3.23=3.22 (stable branch) --> 3.22.pl1 --> 3.22.pl2 --> ...
3.23.pl1
3.23.pl2
%
O sea que en todo momento tenemos en el HEAD la version inestable con
un tag `n.m' (m impar) y versiones `light' (n.m.pl1, n.m.pl2, n.m.pl3,
...) mientras que tenemos una rama que es una version estable que nace
a la misma altura `n.(m-1)' del HEAD y donde vamos arreglando solo
fixes a los bugs, no nuevas features. 
%
11:06:06: El mecanismo deberia ser asi: 
$ cvs tag beta--3-23
$ cvs tag -b -r beta--3-23 beta--3-22
%
Instructivo para hacer las nuevas tags con la utilidad `make tag':
*./ Salvar por si acaso $ tar czvf petscfem-cvsroot.tgz ~/cvsroot/petscfem
*./ $ make tag --> 3.23
*./ $ cvs tag -b -r beta--3-23 beta--3-22
*./ Hacer un directorio con la version estable: 
    $ cvs co -d petscfem-beta-3.22 -r beta--3-22
*./ En ese directorio reemplazar en `stable.log' y en `save.log' el
    tag no-estable por el estable 3.23 -> 3.22 y hacer commit. 
*./ Lo mismo en el directorio de la version HEAD. 
*./ Crear una version light 3.23.pl1 `administrativa'. 

%===Wed, 17 Sep 2003 07:18:02 -0300 mstorti@spider
%
Corre muchos tests. 162 en g_c++:
> Start section: "All tests"
> Summary: "All tests" -- OK: 162.  Not OK: 3. Couldn't open: 1. Total: 166
y en O_c++ reporta 
> Start section: "Navier-Stokes tests."
> Summary: "Navier-Stokes tests." -- OK: 15.  Not OK: 0. Couldn't open: 1. Total: 16
%
lo cual es medio extranio ya que reporta un total de 16 asi que debe
haber pasado algo extranio con las modificaciones a step.epl. 

%===Fri, 19 Sep 2003 08:39:21 -0300 mstorti@node1.beowulf.gtm
%
Corre la cavidad cubica durante 36horas y da un SEGV en el
proc3. Deberia ser node4. Lo relanzo. 

%===Sat, 20 Sep 2003 10:11:29 -0300 mstorti@spider
%
Haciendo los videos de la cavidad, para Re=1e5 al salvar pocos pasos
de tiempo (nsaverot muy grande creo que era 10 o 20) sale demasiado
cortado y feo, es decir no se puede interpolar y que quede
continuo. De paso no es necesario usar tantos pasos de tiempo, digamos
que 710 son como 30 secs. lo cual es demasiado. Perfectamente se puede
salvar cada 5 pasos de tiempo e interpolar por 5, con lo cual con 40
estados (200 pasos de tiempo) sacas 200 frames que son 3'20'' de
filmacion. 
%
Asi que ahora el plan es hacer los de Re=1e6 que ya tengo los estados
y volver a correr un poco a Re=1e5 para (solo hacen falta unos
200Dt). 
%
14:26:28: Paso a hacer los de Re=1e6. Con 3 pasos frames interpolados
por paso de tiempo se ve bastante bien. Tengo 107 pasos de tiempo de
manera que obtengo unos 107*3=321 frames lo cual representa unos 15
secs. Cada frame esta tardando unos 10secs en spider. 
%
19:56:56: Genero los frames para
  drwxr-xr-x    2 mstorti  users        8.0K Sep 20 19:57 YUV-Re1e6-z0.01
  drwxr-xr-x    2 mstorti  users         12K Sep 20 16:37 YUV-Re1e6-z0.25
  drwxr-xr-x    2 mstorti  users         12K Sep 20 15:20 YUV-Re1e6-z0.5
%
36528	YUV-Re1e6-z0.01
38684	YUV-Re1e6-z0.25
33140	YUV-Re1e6-z0.5
%
Despues deberia hacer un corte a y=0.1, x=0.9 e isosuperficies. 
%
%
20:01:38:  Aparentemente corre todos los tests en g_c++ y O_c++. 

%===Wed, 24 Sep 2003 08:52:02 -0300 mstorti@node1.beowulf.gtm
%
Probando los nuevos nodos. Se me cuelga cuando quiero correr la
cavidad con N=80 (2.5Mtet). Creo que da un error en `dvector.h', esto
tal vez se deba a alguna limitacion en el tamanho de los enteros en
algun punto.  Cuando corro con menos va mejor. Corro con 76 (2.2Mtet)
y me da un SEGV en el proc4 (node18). Despues vuelvo a correr con 76 y
como que no quiere arrancar (esto es medio difuso). Vuelvo a lanzarlo
(sacando el node18) y arranca bien. 
%
12:18:10: Sacando el node18 da error en el 20:
> -------------------------------------------------
> --------------------------------------------------------------------------
> Petsc Version 2.1.6, Patch 4, Released Aug 14, 2003
>        The PETSc Team
>     petsc-maint@mcs.anl.gov
>  http://www.mcs.anl.gov/petsc/
> 
> See docs/changes/index.html for recent updates.
> See docs/troubleshooting.html for hints about trouble shooting.
> See docs/index.html for manual pages.
> -----------------------------------------------------------------------
> /u/mstorti/PETSC/petscfem-stable/applications/ns/ns_O.bin on a linux named node20.beowulf.gtm by mstorti Wed Sep 24 12:14:23 2003
> Libraries linked from /u/mstorti/PETSC/petsc-2.1.6/lib/libO_c++/linux
> -----------------------------------------------------------------------
> [5] Aborting program!
> [5]PETSC ERROR: User provided function() line 0 in Unknown directoryUnknown file
> [5]PETSC ERROR:   Signal received!
> [5]PETSC ERROR:   Caught signal number 11 SEGV:
%
12:20:06: Habia trabajos sin matar (con killfem o killpar). Asi que
pruebo  matar bien todo y relanzar. 

%===Wed, 24 Sep 2003 22:58:03 -0300 mstorti@spider
%
Corro los tests en spider (make torture). Da 164 OK, 2 not OK. 

> Testing: "newff/test case reac_adv_dif_temp_y" on file \
>         "newff/reac_adv_dif_temp_y.vrf.tmp"...not OK.
>         ---> Couldn't find "rel. max error <tol\(.*\) OK\? 1" 
> Testing: "newff/test case reac_dif_temp" \
> on file "newff/reac_dif_temp.vrf.tmp"...not OK.
>             --->  Couldn't find "rel. max error  <tol\(.*\) OK\? 1"
%
23:00:42: make lclean en aquifer borra uno de los arhcivos .ans. 
%
23:18:06: Lo borraba por una regla que debe ser de PETSc. Fixed.

%
12:08:20: Corrio bien toda la noche (350+ pasos) sin el nodo 18. Ahora
voy a probar a agregar el nodo18 a ver si no revienta. 
%
12:47:20: Booteo todo (server+nodos) y lo largo con el 18. 
%
13:40:04: Parece andar bien incluso con el 18. 
%
14:17:42: Hizo unos 40Dt OK. Asumo que esta bien y quiero ver que pasa
con N=80 y eventualmente N=78. 
%
14:24:56: Con N=80 da 
%
> ns_O.bin: readmesh.cpp:983: 
>     int read_mesh (Mesh *&, char *, Dofmap *&, int &, int, int): 
>     Assertion `jpos!=n2eptr[node]' failed.
%
15:27:24: El procesador y el nodo en el cual se produce el error
parece ser aleatorio. Primero dio esto:

> ns_O.bin: readmesh.cpp:991: 
>      int read_mesh (Mesh *&, char *, Dofmap *&, int &, int, int): Assertion `0' failed.
> node 10282, elems 32, range in node2elem 136684 - 136715 17970 17972
> 17973 17974 17976 17977 17978 17979 18370 18372 18373 18374 18376
> 18377 18378 18379 49970 49971 49972 49974 49975 49976 49977 49979
> 50370 50371 50373 -131073 50374 50375 50376 50378
%
y ahora da esto
%
> ns_O.bin: readmesh.cpp:995: 
>    int read_mesh (Mesh *&, char *, Dofmap *&, int &, int, int): Assertion `0' failed.
> node 336292, elems 32, range in node2elem 6496384 - 6496415
> 1607895 1607897 -33554433 1607898 1607899 1607901 1607902 1607903
> 1607904 1608295 1608297 1608298 1608299 1608301 1608302 1608303
> 1608304 1639895 1639896 1639897 1639899 1639900 1639901 1639902
> 1639904 1640295 1640296 1640298 1640299 1640300 1640301 1640303
%
Lo vuelvo a correr y
> ns_O.bin: readmesh.cpp:995: 
> int read_mesh (Mesh *&, char *, Dofmap *&, int &, int, int): Assertion `0' failed.
> node 497332, elems 32, range in node2elem 9639024 - 9639055
> 2393555 2393557 2393558 2393559 2393561 2393562 2393563 2393564
> 2393955 2393957 2393958 2393959 2393961 2393962 2393963 2393964
> 2425555 2425556 -33554433 2425557 2425559 2425560 2425561 2425562
> 2425564 2425955 2425956 2425958 2425959 2425960 2425961 2425963
Lo vuelvo a correr y caga en 2:
%
> node 74168, elems 32, range in node2elem 1382624 - 1382655
> 329455 2147483647 329457 329458 329459 329461 329462 329463 329464
> 329855 329857 329858 329859 329861 329862 329863 329864 361455 361456
> 361457 361459 361460 361461 361462 361464 361855 361856 361858 361859
> 361860 361861 361863
> 
> rm_l_4_1255: p4_error: net_recv read: probable EOF on socket: 1
> ns_O.bin: readmesh.cpp:995: int read_mesh (Mesh *&, char *, Dofmap *&,
> int &, int, int): Assertion `0' failed.
> node 443652, elems 32, range in node2elem 8591464 - 8591495
> 
> 2131665 2131667 2131668 2131669 2131671 2131672 2131673 2131674
> 2132065 2132067 2132068 2132069 2132071 2132072 2132073 2132074
> 2163665 2163666 2163667 2163669 2163670 2163671 2163672 2163674
> 2164065 2164066 -33554433 2164068 2164069 2164070 2164071 2164073
%
Notar que siempre aparece el numero ese -33554433. 
%
15:50:59: Tambien caga la fruta de la misma forma con N=78. 
%
15:52:07: Pruebo a largarlo con N=80 y hexas. Si se trata de algun
tipo de overflow de enteros puede que se la banque mas. 
%
16:51:32: Con `use_tetra=0' da error al leer una fijacion
%
> In line: 493047 1 0.
> Fixation 73492, imposed on an invalid node/field combination.
> Assertion failed: "row.begin()->second == 1."
> Fixation imposed on a bad node/field combination
> node: 493047, field: 1
> cubcav.fixa.tmp:73492: "493047 1 0."
%
Ahora voy a probar a correr sin optimizacion. 

%===Fri, 26 Sep 2003 19:05:43 -0300 mstorti@node1.beowulf.gtm
%
Ahora corre bien con todos los nodos y con N=78 (2.4Mtet). 
%
19:23:21: Con N=80 da un error de 
%
> ns_O.bin: readmesh.cpp:995: int read_mesh (Mesh *&, char *, Dofmap *&,
> int &, int, int): Assertion `0' failed.
> node 601646, elems 32, range in node2elem 11689164 - 11689195
> 2901815 2901817 2901818 2901819 2901821 2901822 2901823 2901824
> 2902265 2902267 2902268 2902269 2902271 2902272 2902273 2902274
> 2942315 2942316 2942317 2942319 2942320 2147483647 2942321 2942322
> 2942324 2942765 2942766 2942768 2942769 2942770 2942771 2942773
> rm_l_4_1001:  p4_error: net_recv read:  probable EOF on socket: 1
%
en node18!!. Saco el node18 y lo largo de nuevo. 
%
19:29:25: Sin el node18 pasa bien el punto de particionamiento. 

%===Thu, 25 Sep 2003 21:00:51 -0300 mstorti@spider
%
Con O_c++ en spider con dos procs anda bien. Asi que el problema
parece estar o bien en el cluster o bien para un numero de procs
alto. 
%
21:14:38: Pruebo con N=80 pero anda bien. 
%
12:23:44: Corre bastante, creo que unas 16hs (283 Dt) con 3.7Mtet y
finalmente da un SEGV en el proc 7 (node21??). Guarda que justo node21
es el que no se banca el modo agresivo!!
> [7] MPI Abort by user Aborting program !
> --------------------------------------------------------------------------
> Petsc Version 2.1.6, Patch 4, Released Aug 14, 2003
>        The PETSc Team
>     petsc-maint@mcs.anl.gov
>  http://www.mcs.anl.gov/petsc/
> 
> See docs/changes/index.html for recent updates.
> See docs/troubleshooting.html for hints about trouble shooting.
> See docs/index.html for manual pages.
> -----------------------------------------------------------------------
> 
> /u/mstorti/PETSC/petscfem.mario/test/cubcav/../../applications/ns/ns_O.bin
> on a linux named node22.beowulf.gtm by mstorti Fri Sep 26 19:24:42
> 2003
> 
> Libraries linked from /u/mstorti/PETSC/petsc-2.1.6/lib/libO_c++/linux
> -----------------------------------------------------------------------
> [7] Aborting program!
> [7]PETSC ERROR: User provided function() line 0 in Unknown directoryUnknown file
> [7]PETSC ERROR:   Signal received!
> [7]PETSC ERROR:   Caught signal number 13 Broken Pipe
%
08:36:44: Vuelve a reventar ahora en el proc 9. Veo el tema de que el
`weigths.dat' parece quedar mal y el `machi.dat' tambien, pero
pareceria que no, que se debe a los hooks despues. Voy a bootear todo
el cluster. 
%
09:38:44: Rebooteando todo arranca y se planta en el Dt=4 en proc 4
(node19). 
%
2 cosas a probar son: 
* usar el kernel anterior (el <1GB) y otra es
* setear las BIOS en modo no-agresivo. Voy a probar lo de las Bios
  ahora.
%
13:47:47: Parece ser definitivamente un tema de la memoria. Con la
Pato probamos a cambiar la memoria de una maquina a la otra y maquinas
que no booteaban en agresivo ahora si bootean. Sacamos 2 nodos y
entonces lo largo en 13 nodos. (ya estaba sacado el 18). 
%
13:55:11: Los cambios que hicimos en los nodos son:
%
12: Def -> Agr
14: Def -> Agr
15: agresivo OK
17: agresivo OK
19: Auto-> Auto (no se banca agresivo)
20: Def -> Agr
21: Auto-> Auto (no se banca agresivo)
22: Def -> Agr
23: Def -> Agr
%
14:16:11: Para poder hacer rsh como root a los nodos debe hacerse
1./ Agregar `node1.beowulf.gtm root' al /root/.rhosts de los
       nodos. Hacerle `chmod 600'. 
2./ Agregar 'rsh' a los `/etc/securetty' de los nodos. 

%===Wed,  1 Oct 2003 12:47:02 -0300 mstorti@node1.beowulf.gtm
%
La corrida de cubcav se corto a los 148 Dt. (de las 13:00 hasta
despues de las 21:00) y se quedo esperando. El nodo5 estaba apagado y
no quiere arrancar, aparentemente tiene un problema electrico. Asi que
no no esta en claro si fue un SEGV como en los otros
casos. Aparentemente estaria tardando 1h para hacer 5 pasos (me parece
un poco demasiado). Esta corriendo con 3.2Mtet (N=86). 
%
13:13:02: Suena por memoria. Con respecto a la eficiencia dicha
anteriormente. Guarda que aparentemente estaba corriendo
simultaneamente con gburgener/genetico. Lo largo tranqui con N=82
(2.8Mtet). 

%===Fri, 17 Oct 2003 07:03:49 -0300 mstorti@minerva
%
Para setear correctamente los permisos de `cvsroot':
%
$ chmod -R g+sw cvsroot ; pushd cvsroot ; \
	chmod -R g+sw `find . -type d` ; popd
$ chgrp -R petscfem cvsroot

# Current line ===========  %
# Local Variables: $
# eval: (local-set-key "\C-cD" (quote notas-insert-date)) $
# eval: (local-set-key "\C-ct" (quote notas-insert-time)) $
# eval: (setq paragraph-separate "[ \t\f%]*$") $
# eval: (setq paragraph-start "[ \t\n\f%]") $
# End: $
