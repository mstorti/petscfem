%__INSERT_LICENSE__
\Section{The FastMat2 matrix class}

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSection{Introduction} 

Finite element codes usually have two levels of programming.
In the outer level a large vector describes the \emph{``state''} of
the physical system. Usually this vector has as many entries as the
number of nodes times the number of fields minus the number of
fixations (i.e. Dirichlet boundary conditions). This vector can be
computed at once by assembling the right hand side and the stiffness
matrix in a linear problem, iterated in a non-linear problem or
updated at each time step through solution of a linear or non-linear
system. The point is that, at this outer level, you perform global
assemble operations that build this vector and matrices. At the inner
level, you perform a loop over all the elements in the mesh, compute
the vector and matrix contributions of each element and assemble them
in the global vector/matrix. From one application to another, the
strategy at the outer level (linear/non-linear, steady/temporal
dependent, etc...) and the physics of the problem that defines the
FEM matrices and vectors may vary. 

The FastMat2 matrix class has been designed in order to perform matrix
computations at the element level. It is assumed that we have an outer
loop (usually the loop over elements) that is executed many times, and
at each execution of the loop a series of operations are performed
with a rather reduced set of local vectors and matrices. There are
many matrix libraries but often the performance is degraded for small
dimensions. Sometimes performance is good for operations on the whole
matrices, but it is degraded for operations on a subset of the elment
of the matrices, like columns, rows, or individual elements. 
This is due to the fact that acessing a given element in the matrix
implies a certain number of arithmetic operations. Otherwise, we can
copy the row or column in an intermediate object, but then there is an
overhead due to the copy operations. 

The particularity of FastMat2 is that at the first execution of the
loop the address of the elements used in the operation are cached in
an internal object, so that in the second ans subsequent executions of
the loop the addresses are retrieved from the cache. 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSSection{Example}\label{sec:fastmat2_example}  
%
Consider the following simple example. We are given a 2D finite
element composed of triangles, i.e. an array \verb+xnod+ of
$2\times\Nnod$ doubles with the node coordinates and an array
\verb+icone+ with $3\times\Nelem$ elements with node
connectivities. For each element $0<j<\Nelem$ its nodes are stored at
\verb|icone[3*j+k]| for $0\le k\le 2$. We are required to compute the
maximum and minimum value of the area of the triangles. This is a
computation which is similar to those found in FEM analysis. 
For each element we have to load the node coordinates in local vectors
$!x_1$, $!x_2$ and $x_3$, compute the vectors along the sides of the
elements $!a=!x_2-!x_1$ and $!b=!x_3-!x_1$. The area of the element
is, then, the determinant of the $2\times 2$ matrix \lfrc{$!J$} formed by
putting \lfrc{$!a$} and \lfrc{$!b$} as rows.

The FastMat2 code for the computations goes like this,

%%   // define and load xnod, icone...
%%   double 
%%     total_area = 0.0,
%%     minarea = NAN,
%%     maxarea = NAN;

%\listinginput{1}{fm2ex.cpp}
\begin{verbatim}
  FastMat2::CacheCtx2 ctx;
  FastMat2::CacheCtx2::Branch b1;
  FastMat2 x(&ctx,2,3,2),a(&ctx,1,2),b(&ctx,1,2),J(&ctx,2,2,2);
  chrono.start();
  for (int ie=0; ie<nelem; ie++) {
    ctx.jump(b1);
    for (int k=1; k<=3; k++) {
      int node = icone.e(ie,k-1);
      x.ir(1,k).set(&xnod.e(node-1,0)).rs();
    }
    x.rs();
    a.set(x.ir(1,2));
    a.rest(x.ir(1,1));

    b.set(x.ir(1,3));
    b.rest(x.ir(1,1));

    J.ir(1,1).set(a);
    J.ir(1,2).set(b);
    
    double area = J.rs().det()/2.;
    total_area += area;
    if (ie==0) {
      minarea = area;
      maxarea = area;
    }

    if (area>maxarea) maxarea=area;
    if (area<minarea) minarea=area;
  }
  printf("total_area %g, min area %g,max area %g, ratio: %g\n",
	 total_area,minarea,maxarea,maxarea/minarea);
  printf("Total area OK? : %s\n",
	 (fabs(total_area-1)<1e-8 ? "YES" : "NOT"));
  double cpu = chrono.elapsed();
\end{verbatim}

Calls to the \verb+FastMat2::CacheCtx2 ctx+ object are related
with the caching manipulation and will be discussed later. Matrix are
dimensioned in line 3, the first argument is the number of dimensions,
and then follow the dimensions, for instance \verb+FastMat2 x(2,3,2)+
defines a matrix which 2 indices ranging from 1 to 3, and 1 to 2
respectively. The rows of this matrix will store the coordinates of
the local nodes to the element. FastMat2 matrices may have any number
of indices. Actually the library is compiled for a maximum number of
indices (10 by default. This limit may be modified by redefining
variable {\tt\$max\_arg} in script \verb+readlist.eperl+ and
recompile.) Also they can have zero dimensions, which stands for
scalars.

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSSection{Current matrix views (a.k.a. masks)} \label{sec:fm2-masks}  
%
In lines 10 to 13 the coordinates of the nodes are loaded in matrix
\verb+x+. The underlying philosophy in FastMat2 is that you can set
\emph{``views''} (a.k.a. \emph{``masks''}) of the matrix without
actually made any copies of the underlying values. For instance the
operation \verb+x.ir(1,k)+ (for \emph{``index restriction''}) sets a
view of \verb+x+ so that index 1 is restricted to take the value $k$
reducing in one the number of dimensions of the matrix. As \verb+x+
has two indices, the operation \verb+x.ir(1,k)+ gives a matrix of
dimension one consisting in the $k$-th row of $x$. A call without
arguments like in \verb+x.ir()+ cancels the restriction. Also, the
function \verb+rs()+ (for \emph{``reset''}) cancels the actual view.

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSSection{Set operations} 
The operation \verb+a.set(x.ir(1,2))+ copies the contents of the
argument \verb+x.ir(1,2)+ in \verb+a+. Also we can use \verb+x.set(y)+
with \verb+y+ a Newmat matrix (\verb+Matrix y+) or an array of doubles
(\verb+double *y+). 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSSection{Dimension matching} The \verb+x.set(y)+ operation requires
that \verb+x+ and \verb+y+ have the same ``viewed'' dimensions.  As the
\verb+.ir(1,2)+ operation restricts index to the value of 2,
\verb+x.ir(1,2)+ is seen as a row vector of size 2 and then can be
copied to $a$. If the ``viewed'' dimensions don't fit then an error is
issued.

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSSection{Automatic dimensioning} 
In the example, \verb+a+ has been dimensioned at line
3, but most operations perform the dimensioning if the matrix has not
been already dimensioned. For instance, if at line 3 we would declared
\verb+FastMat2 a+ only, without specifying dimensions, then at line
12, the matrix is created and dimensioned taking the dimensions from
the argument. The same applies to \verb+set(Matrix &)+ but not to
\verb+set(double *)+ since in this last case the argument
(\verb+double *+) don't posses information about his dimensions. Other
operations that define dimensions are products and contraction
operations. 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSSection{Concatenation of operations} Many operations return a
reference to the matrix (return value \verb+FastMat2 &+) so that
operations may be concatenated as in \verb+A.ir(1,k).ir(2,j)+.

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSection{Caching the adresses used in the operations} 

If \emph{caching} is not used the performance of the library is
poor, typically one to two orders of magnitude slower than the cached
version, and the cached version is very fast, in the sense that almost
all the CPU time us spent in performing multiplications and additions,
and negligible CPU time is spent in auxiliary operations.

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>
\SSSection{The FastMat2 operation cache concept} 

The idea with caches is that they are objects
(\verb+class FastMatCache+) that store the adresses needed for the
current operation. In the first pass through the body of the loop a
cache object is created for each of the operations, and stored in a
list. This list is basically an STL list of cache objects. When the
body of the loop is executed the second time and the following, the
adresses of the matrix elements are not needed to be recomputed but
they are read from the cache instead. The use of the cache is rather
automatic and requires some intervention by the user but in some
cases the position in the cache-list can get out of sync with respect
to the execution of the operations and severe errors may occur.

The basic use of caching is to create the cache structure
\verb+FastMat2::CacheCtx2 ctx+ and keep the position in the cache
structure \emph{synchronized} with the position of the code. The
process is very simple, when the code consists in a linear sequence of
FastMat2 operations that are executed always in the same order. In
this case the \verb+CacheCtx2+ object stores a list of the cache
objects (one for each FastMat2 operation). As the operations are
executed the internal FastMat2 code is incharge of advancing the cache
position in the cache list automatically. A linear sequence of cache
operations that are executed \emph{always} in the same order is called
a \emph{branch}. 

Looking at the previous code, we have one branch starting at the
\verb+x.ir(1,k).set(...)+ line, through the \verb+J.rs().det()+
line. This sequence is repeated many times (one for each element) so
that it is interesting to \emph{reuse the cache list}. For this, we
create a \emph{branch} object \verb+b1+ (class
\verb+FastMat2::CacheCtx2::Branch+) object and \emph{jump} to this
branch each time we enter the loop. The first time we enter the loop
the cache list is created and stored in the first position of the
cache structure. In the next and subsequent executions of the loop,
the cache is resued avoiding recomputation of many administrative work
related with the matrices. 

The problem is when the sequence of operations is not always the
same. In that case we must issue several \verb+jump()+ commands, each
one to the start of a sequence of FastMat2 operations. Consider for
instance the following code,

\begin{verbatim}
  FastMat2::CacheCtx2 ctx;
  FastMat2::CacheCtx2::Branch b1, b2;
  FastMat2 x(&ctx,1,3);
  ctx.use_cache = 1;
  int N = 10000, in=0, out=0;
  
  for (int j=0; j<N; j++) {
    ctx.jump(b1);
    x.fun(rnd);
    double len = x.norm_p_all();
    if (len<1.0) {
      in++;
      ctx.jump(b2);
      x.scale(1.0/len);
    }
  }
  printf("total %d, in %d (%f%%)\n",
         N,in,double(in)/N);
\end{verbatim}

A vector \verb+x+ of size 3 is randomly generated in a loop (the line
\verb+x.fun(rnd);+). Then its length is computed, and if is shorter
than 1.0 it is scaled by \verb+1.0/len+, so that its final length is
one. In this case we have to branches, 
%
\begin{itemize}
\compactlist 
\item[] branch \verb+b1+: operations \verb+x.fun()+  and
  \verb+x.norm_p_all()+,
\item[] branch \verb+b2+: operation \verb+x.scale()+. 
\end{itemize}
%
so that we define two branch objects \verb+b1,b2+ and do the
corresponding jumps. 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSSection{Branching is not always needed} 

However, branching is needed \emph{only} if the instruction sequence
changes during the same execution of the code. For instance if you
have a code like this
%
\begin{verbatim}
  FastMat2::CacheCtx2 ctx;
  ctx.use_cache=1;
  for (int j=0; j<N; j++) {
    ctx.jump(b1);
    // Some FastMat2 code...
    if (method==1) {
       // Some FastMat2 code for method 1 ...
    } else if (method==2) {
       // Some FastMat2 code for method 2 ...
    }
    // More FastMat2 code...
  }
\end{verbatim}
%
If the \verb+method+ flag is determined at the moment of reading
the data and then is left unchanged for the whole execution of the
code, then it is not necessary to protect do branching since the
instruction sequence will be always the same. 

Moreover if you perform a hevy loop (\verb+N+ large) with some value
for \verb+method+, and then you change \verb+method+, then branching
is still not needed, provided that the cache structure (\verb+ctx+) is
rebuilt, as in the code above. However, if the \verb+ctx+ is a global
variable, and \verb+method+ changed you only have to clear the cache
structure
%
\begin{verbatim}
  // ctx is some global variable
  if (method!=last_used_method) ctx.clear();
  ctx.use_cache=1;
  for (int j=0; j<N; j++) {
     //...
  }
\end{verbatim}

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSSection{Cache mismatch} 

The cache process may fail if a \emph{cache mismatch} is produced. For
instance, consider the following variation of the previous code
%
\begin{verbatim}
  FastMat2::CacheCtx2 ctx;
  FastMat2::CacheCtx2::Branch b1, b2, b3;
  //...
  for (int j=0; j<N; j++) {
    ctx.jump(b1);
    x.fun(rnd);
    double len = x.norm_p_all();
    if (len<1.0) {
      in++;
      ctx.jump(b2);
      x.scale(1.0/len);
    } else if (len>1.1) {
      ctx.jump(b3);
      x.set(0.0);
    }
  }
\end{verbatim}
%
There is an additional block in the conditional, if the length of the
vector is greater than 1.1, then the vector is set to the null
vector. 

We have now three branches. The code shown works OK, but if, let's
say, we forgot to add a third branch, and replace the
\verb+ctx.jump(b3)+ for a \verb+ctx.jump(b2)+, then when reaching the
\verb+x.set(0.0);+ operation the corresponding cache would be the
cache corresponding to the \verb+x.scale()+ operation, and most
probably and error or incorrect computation will occur. Every time
that the retrieved cache doesn't exist or doesn't match with the
operation that will be computed we say that there is a \emph{cache
  mismatch}. 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSSection{When a cache mismatch is produced} 

Basically, the information stored in the cache (and then, retrieved
from the objects that were passed in the moment of creating the cache)
must be the same needed for performing the current FastMat2 operation,
that is
%
\begin{itemize}
\compactlist 
\item The FastMat2 matrices involved must be the same, 
  (i.e. their pointers to the matrices must be the same). 
\item The indices passed in the operation (for instance for the
  \verb+prod()+, \verb+ctr+, \verb+sum()+ operations). 
\item The masks (see~\S\ref{sec:fm2-masks}) applied to each of the
  matrix arguments. 
\end{itemize}
 
%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSSection{Branch arrays} \label{sec:branchv}  

If some cases you need branching to a certain number of branches, and
doing it by hand is cumbersome. You should need a branch object
for each branch, so you can either define a plain STL \verb+vector<>+
of branches or use the \verb+FastMat2::CacheCtx2::Branchv+
class. Consider the following code,
%
\begin{verbatim}
  FastMat2::CacheCtx2 ctx;
  FastMat2::CacheCtx2::Branch b1;
  FastMat2::CacheCtx2::Branchv b2v(5);
  FastMat2 x(&ctx,2,5,3);
  int N = 10000;
  ctx.use_cache = 1;
  double sum=0;
  for (int j=0; j<N; j++) {
    ctx.jump(b1);
    x.fun(rnd);
    int k = rand()%5;
    ctx.jump(b2v(k));
    x.ir(1,k+1);
    sum += x.norm_p_all();
    x.rs();
  }
  ctx.use_cache = 0;
\end{verbatim}
%
At each iteration of the loop you randomly generate a matrix \verb+x+
of 5x3. Then you randomly pick a row of it, take its norm and
accumulate on variable \verb+sum+. If you omit in the code the
\verb+ctx.jump(b2v(k))+ line, then there is only the main \verb+b1+
branch, and when the \verb+norm_p_all()+ instruction is executed it
can be reached with a different value of \verb+k+ so that it will not
give an error but it will compute with the \verb+k+ stored in the
cache. 

As mentioned above you can either define a plain STL \verb+vector<>+
of branches or use the \verb+FastMat2::CacheCtx2::Branchv+ class, as
shown above. The branch array can be given dimensions either at the
constructor as above, or with the \verb+init+ method,
%
\begin{verbatim}
  FastMat2::CacheCtx2::Branchv b2v;
  // ... later...
  b2v.init(5);
\end{verbatim}
%
Multi dimensional arrays can be created, just give more integer
arguments. Currently, up to 4 dimensions are allowed. 

Please note that, in the case above, if just the same columns is
visited always then the branching is not needed, for instance, int eh
following code
%
\begin{verbatim}
  int k = 3;
  for (int j=0; j<N; j++) {
    ctx.jump(b1);
    x.fun(rnd);
    x.ir(1,k);
    sum += x.norm_p_all();
    x.rs();
  }
\end{verbatim}
% 
the second branching is not neededm because we use the mask
\emph{always} with the third column. Moreover, in the following case
%
\begin{verbatim}
  for (int j=0; j<N; j++) {
    ctx.jump(b1);
    x.fun(rnd);
    for (int k=1; k<=5; k++) {
      x.ir(1,k);
      sum += x.sum_square_all();
    }
    x.rs();
  }
\end{verbatim}
% 
you don't need the branching either, because all the columns are
visited always in the same order, i.e. the sequence of operations that
are seen by the cache system are
%
\begin{itemize}
  \compactlist 
\item \verb+x.fun(rnd)+
\item \verb+x.sum_square_all()+ with \verb+x+ masked to column 1 
\item \verb+x.sum_square_all()+ with \verb+x+ masked to column 2
\item \verb+x.sum_square_all()+ with \verb+x+ masked to column 3 
\item \verb+x.sum_square_all()+ with \verb+x+ masked to column 4 
\item \verb+x.sum_square_all()+ with \verb+x+ masked to column 5
\end{itemize}
%
so that, the code above, with only one branch will work OK. 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSSection{Debugging tools} 

If you receive segmentation violation errors and you suspect that it
is due to the caching system, then you can enable a self check by
issuing the \verb+ctx.check_labels()+ instruction. In this way, a
check string identifying the operations is generated and stored in the
cache. Then, each time the cache is reused, the string stored in the
cache is checked against the label for the operation to be performed,
and if differences are found, the system stops execution. Currently
the string stores only the type of operation and the pointers to the
matrices involved, not the masks (this will be done in a future). So
that an error as discussed in~\S\ref{sec:branchv} will not be
detected. 

Activating this feature is very expensive, so that it is only expected
to be used in the debugging process. Production code should not have
this feature enabled. 

The feature can be selectively activated in some parts of the code and
deactivated in others. To deactivate the check use
\verb+ctx.check_labels(0)+.

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSSection{Multithreading, reentrancy} 

If caching is not enabled, FastMat2 is thread safe. If caching is
enabled, then it is thread safe in the following sense, a ctx must be
created for each thread, and the matrices used in each thread must be
associated with the context of that thread.

If creating the cache structures each time is too bad for efficiency,
then the context and the matrices may be used in a parallel region,
stored in variables, and reused in a subsequent parallel region.

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSSection{Debugging FastMat2 code} 

The following tips can help in debugging FastMat2 code. 

\begin{itemize}
  \compactlist 
\item As with the debugging of any other code, it is better to first
  switch to the \emph{simplest situation} and debug, then restore each
  feature at a time. For this, deactivate caching
  (\verb+ctx.use_cache=0+ ), run in single-thread mode, and go to
  debug mode code (\verb|BOPT=g_c++|) and debug the code.  This is the
  simplest situation, you should debug here until you reach a working
  code that gives the results you expect.\\
  % 
  (Note: currently PETSc-FEM does not use multi-threading so that we
  refer here to deactivating multithreading for other code that may be
  using FastMat2, as for instance the \verb+trcprtf+ code for particle
  tracking.)
\item Enable caching and activate \verb+check_labels()+. Debug until
  no errors are found, and check that the results are the same as with
  the non-cached code. (re-checking)
\item Go to optimized mode and re-check. 
\item Go to multithread mode and re-check. 
\end{itemize}
 
%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSSection{FastMat2 tips} 

\begin{itemize}
\compactlist 
\item Do not leave caching activated in the whole code. After the loops
  that are most time consuming set \verb+ctx.use_cache=0+. 
\item Some initialization code or other isolated operations are better
  to be left uncached.
\item You can access the raw storage (an array of doubles) inside the
  matrix with \verb+double *aptr = a.storage_begin()+. This can be
  used for complex operations that are not easy to code with the
  FastMat2 methods. 
\end{itemize}

%% FIXME:= Write on
%% . OK branch vectors
%% . OK debugging tools
%% . OK reentrancy
%% . OK- tracing tools (not implemented yet)
%% . OK debugging process (deact optimiz, deact multithr, 
%%                      deact cache... activ cache...)
%% . OK isolated FastMat2 ops can be left uncached...
%% . OK deactivate cache after loops. 
%% . branch array vs. continued loop

\SSection{An older version of cache structure}

\textbf{NOTE: This version of cache structure (called
  {\tt CacheCtx1} will be probably declared obsolete in a future. We
  encourage users to use the new {\tt CacheCtx2} version).}

The functions for manipulating the cache structure are different for
\verb+FastMat2::CacheCtx1+ than for \verb+FastMat2::CacheCtx2+. The
example above is rewritten as follows, 

\begin{verbatim}
  Chrono chrono;
  FastMat2 x(2,3,2),a(1,2),b(1,2),J(2,2,2);
  FastMatCacheList cache_list;
  FastMat2::activate_cache(&cache_list);
  // Compute area of elements
  chrono.start();
  for (int ie=0; ie<nelem; ie++) {
    FastMat2::reset_cache();
    for (int k=1; k<=3; k++) {
      int node = ICONE(ie,k-1);
      x.ir(1,k).set(&XNOD(node-1,0)).rs();
    }
    x.rs();
    a.set(x.ir(1,2));
    a.rest(x.ir(1,1));

    b.set(x.ir(1,3));
    b.rest(x.ir(1,1));

    J.ir(1,1).set(a);
    J.ir(1,2).set(b);
    
    double area = J.rs().det()/2.;
    total_area += area;
    if (ie==0) {
      minarea = area;
      maxarea = area;
    }

    if (area>maxarea) maxarea=area;
    if (area<minarea) minarea=area;
  }
  printf("total_area %g, min area %g,max area %g, ratio: %g\n",
	 total_area,minarea,maxarea,maxarea/minarea);
  printf("Total area OK? : %s\n",
	 (fabs(total_area-1)<1e-8 ? "YES" : "NOT"));
  double cpu = chrono.elapsed();
  FastMat2::print_count_statistics();
  printf("CPU: %g, number of elements: %d\n"
	 "rate: %g [sec/Me], %g Mflops\n",
	 cpu,nelem,cpu*1e6/nelem,
	 nelem*FastMat2::operation_count()/cpu/1e6);
  FastMat2::void_cache();
  FastMat2::deactivate_cache();
\end{verbatim}

The typical use can be seen in the example shown in section
\S\ref{sec:fastmat2_example}. First we have to create a
\verb+FastMatCacheList+ object as shown in line 3 and activate it with
as in line 4. The outer loop here is the loop over elements. For the
second and following executions of the body of the loop, you must
``rewind'' the cache-list, this is done in line 8 with the
\verb+FastMat2::reset_cache()+ operation (see figure~\ref{fg:cache}).

The \verb+deactivate_cache()+ call at line 43 causes that subsequent
operations after this line will not be cached. This call is required
if subsequent calls to FastMat2 cached operations will be
executed. Otherwise there will be an error since those posterior calls
will not have a corresponding cache in the cache-list. 

The \verb+void_cache()+ call deletes the cache claiming the space used
by it. It's not required but it is a good idea in order to save memory
space. It may be required if the loop will be called with other
dimensions. For instance a FEM loop may be called first for triangles
and then for quadrangles, and in this two calls the dimensions of the
involved matrices are different. 

The general layout of a code section which uses caching is like this
%
\begin{verbatim}
...           // Initialization. Not cached operations. 

FastMatCacheList cache_list;           // Cache-list object
FastMat2::activate_cache(&cache_list); // activates the cache

for (j=0; j<N; j++) {    // Outer loop. N very large number

  FastMat2::reset_cache();            // Rewinds the cache list
          
  op_1;
  op_2;
  op_3;                               // Cached operations 
  ...                                       
  op_N;
  
   
}                        // End of outer loop 

FastMat2::void_cache();               // free memory
FastMat2::deactivate_cache();         // deactivates cache
\end{verbatim}

\begin{figure*}[htb]
\centerline{\includegraphics{./OBJ/cache}}
\caption{Cache of operations for linear segment of code}
\label{fg:cache}
\end{figure*}
 
\SSSSection{Branching} 

Caching \{ is straightforward if the sequence of operations are executed
linearly, with the same local variables, without branching or looping.
Special operations have to be executed if there are branching
conditions (\verb+if+ sentences) that alter the order of execution of
the operations in the loop. For instance an ``if'' sequence like this
%
\begin{alltt}
\allttbraces%
....
op_prev_1;               // operations previous to the branch
op_prev_2;
...
op_prev_k;
if (\Arg{condition 0}) \{
  op_0_1;
  op_0_2;                // conditional block for branch 0
  ...
  op_0_N;
\} else if (\Arg{condition 1}) \{
  op_1_1;
  op_1_2;                // conditional block for branch 1
  ...
  op_1_N;
\} else \{
  op_1_1;
  op_1_2;               // conditional block for the `else' branch
  ...
  op_1_N;
\}
....
op_pos_1;               // operations posterior to the branch
op_pos_2;
...
op_pos_k;
\end{alltt}
%
If branch '0' is followed in the first execution of the block, then
the cache will look like that shown in figure~\ref{fg:branchnotok}.
%
\begin{figure*}[ht]
\centerline{\includegraphics[height=0.7\hsize]{./OBJ/branch}}
\caption{Cache list produced when branch 0 is chosen.}
\label{fg:branch}
\end{figure*}
%
\begin{figure*}[ht]
\centerline{\includegraphics[height=0.7\hsize]{./OBJ/branchnotok}}
\caption{Cache list produced when branch 0 is chosen.}
\label{fg:branchnotok}
\end{figure*}
%
When in a subsequent execution of the loops another branch is chosen,
say branch 1, then when reading trying to execute operation
\verb+op_1_1+ the library will find in the cache list a cache
corresponding to operation \verb+op_0_1+. 

This is solved by creating \emph{``branch points''} in the cache list
and choosing the appropriate branch as shown in the following
code (see figure~\ref{fg:branch}):
%
\begin{alltt}
\allttbraces%
....
op_prev_1;               // operations previous to the branch
op_prev_2;
...
op_prev_k;
FastMat2::branch();
if (\Arg{condition 0}) \{
  FastMat2::choose(0);
  op_0_1;
  op_0_2;                // conditional block for branch 0
  ...
  op_0_N;
\} else if (\Arg{condition 1}) \{
  FastMat2::choose(1);
  op_1_1;
  op_1_2;                // conditional block for branch 1
  ...
  op_1_N;
\} else \{
  FastMat2::choose(\Arg{N});
  op_e_1;
  op_e_2;               // conditional block for the `else' branch
  ...
  op_e_N;
\}
FastMat2::leave();
....
op_prev_1;               // operations posterior to the branch
op_prev_2;
...
op_prev_k;
\end{alltt}
%

The \verb+branch()+ call tells the library that several branchs will
start from there and a branch point is created. Then each conditional
block code must start with
\begin{alltt}choose(\Arg{j})\end{alltt} where \Arg{j} is a number that
must be unique among all other branches. Finally when leaving all the
branches we must call \verb+leave()+ in order to tell the library that
the mainstream of the cache list must be retaken. 
Branches can be nested at any level. 

The call to branching is not needed if the ``execution path'' is the
same for all executions of the loop.  This usually happens when the
condition refer to some global option that is uniform over all
elements.  For instance if branch '0' corresponds to ``include
turbulence model A'' and branch '0' to model B, then the same branch
is executed for all the elements and there is no need to call the
static functions.

\SSSSection{Loops executed a non constant number of times}\label{sec:loop}  

Another special case is when there are loops inside the body of the
outer loop.  Note that no special branching is needed in general if
the loop is executed a fixed number of times, since the sequence of
operations is not altered from one execution to another.  
For instance consider the following piece of code
%
\begin{verbatim}
//   Case A. Inner code executed a fixed number of times
...
for (int k=0; k<N;k++) {              // N very large - Outer loop 
  block_before;
  for (int ll=0; ll<3; ll++) {
    inner_block;                      // Operations that act on the
                                      // same matrices.
  }
  block_after;
}
\end{verbatim}
%
Then the cache list generated in the first execution of the loop will
be
%
\begin{verbatim}
block_before
inner_block
inner_block
inner_block
block_after
\end{verbatim}
%
and this is OK since the number of times \verb+inner_block+ is
executed is always the same.  If the operations that are performed
inside the loop are the the same for all executions of the loop but
are executed an irregular number of times, then we can use a sequence
as follows
%
\begin{verbatim}
//   Case B. Inner code executed a variable number of times
FastMatCachePosition cp1;
...
for (int k=0; k<N;k++) {              // N very large - Outer loop 
  block_before
  FastMat2::get_cache_position(cp1);
  int n=irand(1,5);
  for (int ll=0; ll<n; ll++) {
    FastMat2::jump_to(cp1);
    inner_block;                      // Operations that act on the
                                      // same matrices.
  }
  FastMat2::resync_was_cached();
  block_after;
\end{verbatim}
%
Here the number of times the inner block is executed may vary randomly
from 1 to 5.  (\verb+irand(m,n)+ returns an integer number randomly
distributed between \verb+m+ and \verb+n+.) The
\verb+FastMatCachePosition+ class objects store the position of the
actual computation in the cache list. So that the call to
\verb+jump_to()+ at the start of the loop restarts the position in the
cache to the desired one. After leaving the loop we call to
\verb+resync_was_cached()+ in order to resync the cache list. 

This is OK if the inner loop is executed at least once the in the
first execution of the outer loop. If it happens that in the first
execution of the loop the inner loop is not entered, then the cache
list will contain \verb|block_before,block_after| and when the inner
block will be entered in subsequent executions of the loop and error
will arise since there will be missimng caches. 

To fix this we have to combine this with branching as here
%
\begin{verbatim}
FastMatCachePosition cp1;
for (int k=0; k<N;k++) {    // N very large - Outer loop 
  ....                      // Previous block
  FastMat2::branch();       // Allows conditional execution 
  FastMat2::choose(0);
  
  FastMat2::get_cache_position(cp1);  
  n=irand(0,5);
  if (k==0) n=0; // This is the critical case. 
                 // n=0 the first execution
    	         // of the loop. 
  for (int ll=0; ll<n; ll++) {
    FastMat2::jump_to(cp2);
    ...                     // inner_block
  }
  FastMat2::resync_was_cached();
  FastMat2::leave();
  ...                       // posterior block
}
\end{verbatim}
%
Off course, if the number of times the inner loop is executed is very
large, and the most time consuming part is the execution of this loop,
then it may be convenient to choose this loop as the ``outer'' one. 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>
\SSSSection{Masks can't traverse branches} 

Another restriction is that if branching is used, the mask that is
active at a certain \verb+FastMat2+ cached operation must be the same
independently of the path that the code have followed, for instance 
consider the following code
%
\begin{verbatim}
FastMat2 a,b,c;
// resize and set `a,b,c'
for (int j=0; j<N; j++) {
  FastMat2::branch();
  if (condition) {
    FastMat2::choose(0);
    a.is(...).ir(...);      // (B)
    // operate on masked `a'
  }
  FastMat2::leave();
  c.prod(a,b);              // (A) Wrong!! `a' may have or
                            // not the mask set
}
\end{verbatim}
%
When the code reaches the \verb+prod()+ method at line \verb+(A)+, it
can have executed or not the block inside the \verb+if+, so that the
mask set in line \verb+(B)+ may or may not be active at line
\verb+(A)+. This is clearly an error, and to avoid it the safest way is to
always reset the masks at the outlet of a branched block like in line
\verb+(C)+ as follows. 
%
\begin{verbatim}
FastMat2 a,b,c;
// resize and set `a,b,c'
for (int j=0; j<N; j++) {
  FastMat2::branch();
  if (condition) {
    FastMat2::choose(0);
    a.is(...).ir(...);      // (B)
    // operate on masked `a'
    a.rs();                 // (C)
  }
  FastMat2::leave();
  c.prod(a,b);              // (A) OK!! `a' has not mask. 
}
\end{verbatim}

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>
\SSSSection{Efficiency} 

%Efficiency, memory required by the caches...
%Amortization time

As we mentioned before, When caching is enabled there is a gain in
speed of ten to one hundreth, and the library is very performant. Of
course, the first execution of loop is not cached and represents and
overhead that has to be amortized by executing the loop in cached mode
many times. The average speed increases when the number of executions
of the loop is increased. The cut point, i.e. the number of executions
of the loop for which the excution speed falls to one half the speed
obtained for very large number o execution is currently between 10 and
30, so that for loops larger than 200 the overhead time spent in
building the caches is negligible. 

Another issue is the memory required by the caches. First there is
some space required by the caches themselves and then, there is a copy
of the addresses of the elements involved. For instance in a
\verb+a.set(b)+ operation with \verb+a+ and \verb+b+ of size $n\times
m$, say, we have to store $2mn$ addresses. Usually this overhead in
memory requirement is negligible, since the amount of variables and
operations needed in the element routines are very small as compared
with the size of the problem itself. However, some care must be taken
when caching large inner loops. For instance in code A,
section~\S\ref{sec:loop}, if the inner loop is executed a constant,
but very large, number $M$ of times, then the amount of the cache
required is proportional to $M$. Then, even if, as discussed before,
no operations like those used in code B are required, it may be
adviceable to spend some time in insert these calls in order to reduce
memory cache and overhead time. Again, in the limit of $M$ very large,
it will be more convenient to choose this loop as the ``outer'' one.

\SSection{Synopsis of operations} 
%A synopsis of the current implemented operations can be found in the
%online reference manual.

{\raggedright 
\SSSection{One-to-one operations} These are operations that take one
FastMat2 argument as in \verb+FastMat2& add(const FastMat2 & A)+.\hfil 
The operations are from one element of \verb+A+ to the corresponding
element in \verb+*this+. 

The one-to-one operations implemented so far are
%
\begin{itemize}
\item \verb+FastMat2& set(const FastMat2 & A)+
              Copy matrix 

\item \verb+FastMat2& add(const FastMat2 & A)+
              Add matrix 

\item \verb+FastMat2& rest(const FastMat2 & A)+
              Substract a matrix 

\item \verb+FastMat2& mult(const FastMat2 & A)+ 
              Multiply (element by element) (like Matlab \verb+.*+). 

\item \verb+FastMat2& div(const FastMat2 & A)+ 
             Divide matrix (element by element, like Matlab \verb+./+). 

\item \verb+FastMat2& axpy(const FastMat2 & A, const double alpha)+ 
             Axpy operation (element by element): \verb+(*this) += alpha * A+ 
\end{itemize}

\SSSection{In-place operations}

These operations perform an action on all the elements of a matrix. 

\begin{itemize}
\item \verb+FastMat2& set(const double val=0.)+
              Sets all the element of a matrix to a constant value 
\item \verb+FastMat2& scale(const double val)+ 
              Scale by a constant value 
\item\verb+FastMat2& add(const double val)+
              Adds constant val 
\item\verb+FastMat2& fun(scalar_fun_t *function)+
              Apply a function to all elements 
\item\verb+FastMat2& fun(scalar_fun_with_args_t *function, void *user_args)+
              Apply a function with optional arguments to all elements 
\end{itemize}

\SSSection{Generic ``sum'' operations (sum over indices)}

These operations perform some operation an all the indices of a given
dimension resulting in a matrix which has less number of indices. It's
a generalization of the \verb+sum/max/min+ operations in Matlab that
returns the specified operation per columns, resulting in a row vector
result (one element per column). Here you specify a number of integer
arguments, in such a way that 
%
\begin{itemize}
\item if the $j$-th integer argument is
positive it represents the position of the index in the resulting
matrix, otherwise 
\item if the $j$-th argument is -1 then we perform the specified
operation (sum/max/min etc...) over all this index.
\end{itemize}
%
For instance if we declare \verb+FastMat2 A(4,2,2,3,3)+ then
\verb+B.sum(A,-1,2,1,-1)+ means
%
\begin{equation} 
   B_{ij} = \sum_{k=1..2,l=1..3} A_{kjil},\ \ \text{for}\ \
   i=1..3,\ j=1..2
\end{equation}
%
These operation can be extended to any binary associative
operation. So far we have implemented the following
%
\begin{itemize}
\item \verb+FastMat2& sum(const FastMat2 & A, const int m=0, ...)+
              Sum over all selected indices 
\item \verb+FastMat2& sum_square(const FastMat2 & A, const int m=0, ...) +
              Sum of squares over all selected indices 
\item \verb+FastMat2& sum_abs(const FastMat2 & A, const int m=0, ...) +
              Sum of absolute values all selected indices 
\item \verb+FastMat2& min(const FastMat2 & A, const int m=0, ...) +
              Minimum over all selected indices 
\item \verb+FastMat2& max(const FastMat2 & A, const int m=0, ...) +
              Maximum over all selected indices 
\item \verb+FastMat2& min_abs(const FastMat2 & A, const int m=0, ...) +
              Min of absolute value over all selected indices 
\item \verb+FastMat2& max_abs(const FastMat2 & A, const int m=0, ...) +
              Max of absolute value over all selected indices 
\end{itemize}

\SSSection{Sum operations over all indices}

When the sum is over all indices the resulting matrix has zero
dimensions, so that it is a scalar. You can get this scalar by
creating an auxiliar matrix (with zero dimensions) casting with
operator \verb+double()+ as in
%
\begin{verbatim}
   FastMat2 A(2,3,3),Z;

   ...  // assign elements to A

   double a = double(Z.sum(A,-1,-1));
\end{verbatim}
%
or using the \verb+get()+ function 
%
\begin{verbatim}
   double a = Z.sum(A,-1,-1).get();
\end{verbatim}
%
without arguments, which returns a double. In addition there is for
each of the previous mentioned ``generic sum'' function a companion
function that sums over all indices. The name of this function is
obtained by appending \verb+_all+ to the generic function
%
\begin{verbatim}
   double a = A.sum_square_all();
\end{verbatim}
%
The list of these functions is
%
\begin{itemize}
\item \verb+double sum_all() const +
              Sum over all indices 
\item \verb+double sum_square_all() const +
              Sum of squares over all indices 
\item \verb+double sum_abs_all() const +
              Sum of absolute values over all indices 
\item \verb+double min_all() const +
              Minimum over all indices 
\item \verb+double max_all() const +
              Maximum over all indices 
\item \verb+double min_abs_all() const +
              Minimum absolute value over all indices 
\item \verb+double max_abs_all() const +
              Maximum absolute value over all indices 
\end{itemize}

\SSSection{Export/Import operations}

These routines allow to convert matrices from or to arrays of doubles
and Newmat matrices
%
\begin{itemize}
\item \verb+FastMat2& set(const Matrix & A) +
              Copies to argument from Newmat matrix 
\item \verb+FastMat2& set(const double *a) +
              Copy from array of doubles 
\item \verb+const FastMat2& export(double *a) const +
              exports to a double vector 
\item \verb+FastMat2& export(double *a) +
              exports to a double vector 
\item \verb+const FastMat2& export(Matrix & A) const +
              Exports to a Newmat matrix 
\item \verb+const FastMat2& export(Matrix & A) const +
              Exports to a Newmat matrix 
\end{itemize}

\SSSection{Static cache operations}

These routines control the use of the cache list.

\begin{itemize}
\item \verb+static void activate_cache(FastMatCacheList *cache_list_=NULL) +
              Activates use of the cache 
\item \verb+static(void) +
              Deactivates use of the cache 
\item \verb+static void reset_cache(void) +
              Resets the cache 
\item \verb+static void void_cache(void) +
              Voids the cache 
\item \verb+static void branch(void) +
              Creates a branch point 
\item \verb+static void choose(const int j) +
              Follows a branch 
\item \verb+static void leave(void) +
              Leaves the current branch 
\item \verb+static double operation_count(void) +
              Computes the total number of operations in the cache list 
\item \verb+static void print_count_statistics() +
              Print statistics about the number of operations of
                each type in the current cache list 
\end{itemize}
} % end of \raggedright

% \SSection{Caching} 

% This is rather straight-forward when the matrix operations are
% performed in all the execution of the loop in the same maner. When
% there is some

% Local Variables: *
% mode: latex *
% latex-occur-section-regexp: "^\\\\S*ection" *
% tex-main-file: "petscfem.tex" *
% End: *
