%__INSERT_LICENSE__

\index{PFMat@\verb+PFMat+}
\Section{The PFMat class} 

\index{domain decomposition}
\index{IISDMat}
\index{PFMat@\verb+PFMat+} 
The \verb+PFMat+ class is a matrix class that acts either as a wrapper
to the \PETSc{} \verb+Mat+ or to other representations of
matrix/solvers. Currently there is the basic \PETSc{} class named
\verb+PETScMat+ and a class called \verb+IISDMat+ (for
\emph{``Interface Iterative -- Sub-domain Direct''}, method) that has
a special solver that solves the linear system by solving iteratively
over the interface nodes, while solving with a direct method in the
sub-domain interiors (this is commonly referred as the \emph{``Domain
Decomposition Method''}).

\SSection{The PFMat abstract interface} 

\index{d_nnz@\verb+d_nnz+}
\index{o_nnz@\verb+o_nnz+}
The \verb+create(...)+ member should create the matrix from the matrix
profile \verb+da+ and the \verb+dofmap'+. For the \verb+PETScMat+
matrix class it calls the old \verb+compute_prof+ routine calculating
the \verb+d_nnz+ and \verb+o_nnz+ arrays, and calling the
\verb+MatCreate+ routine. For the \verb+IISD+ matrix it has to
determine which dofs are in the local blocks and create the
appropriate numbering.

The \verb+set_value+ member is equivalent to the 'MatSetValues'
routine and allows to enter values in the matrix. For the
\verb+IISDMat+ class it sets the value in the appropriate block
($A_{LL}$, $A_{IL}$, $A_{LI}$ or $A_{II}$ \PETSc{} matrices). In
addition, for the $A_{LL}$ (\emph{``local-local''} block) it has to
\emph{``buffer''} those values that are not in this processor (this
can happen when dealing with periodic boundary conditions, or bad
partitionings, for instance,  an element that is connected to all
nodes that belong to other processor. This last case is not the most
common but it can happen. 

Once you have filled the entries in the matrix you have to call the
\verb+assembly_begin()+ and \verb+assembly_end()+  members (as in
\PETSc{}). 

The \verb+solve(...)+ member solves a linear ssytem associated to the
given operator. 

The \verb+zero_entries()+ is also the counterpart of the corresponding
\PETSc() routine. The \verb+build_sles()+ member creates internally the
SLES needed by the solution (included the preconditioner). It takes as
an argument a \verb+TextHashTable+ from where it takes a series of
options. The \verb+destroy_sles()+ member has to be called afterwards,
in order to destroy it (and free space). The \verb+monitor()+ member
allows the user to redefine the convergence monitoring routine. 

The \verb+view()+ member prints operator information to the output. 

\SSection{IISD solver} 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\begin{figure*}[htb]
\centerline{\includegraphics{./OBJ/iisd.\EPSPDF}}
\caption{IISD deccomposition by subdomains}
\label{fg:iisd}
\end{figure*}
%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 

\index{connected dof's}
Let's consider a mesh as in figure ~\ref{fg:iisd}, partitioned such
that a certain number of elements and nodes belong to processor 0 and
others to processor 1. We assume that one unknown is associated to
each node and no Dirichlet boundary conditions are imposed so that
each node corresponds to one unknown. We split the nodes unknowns in
three disjoint subsets $L_{1,2}$ and $I$ such that the nodes in $L_1$
are not connected to those in $L_2$ (i.e. they not share an element,
and then, the FEM matrix elements $A_{i,j}$ and $A_{j,i}$ with $i\in
L_1$ and $j\in L_2$ are null. The matrix is split in blocks as follows
%
\begin{equation} 
\begin{split}
  !A &= \begin{sqmat}{cc}
             !A_{LL} & !A_{LI} \\
              !A_{IL} & !A_{II}
  \end{sqmat}\\
%
!A_{LL} &= \begin{sqmat}{cc}
        !A_{00} & 0\\
        0 & !A_{11} 
       \end{sqmat}\\
%
!A_{LI} &= \begin{sqmat}{cc}
        !A_{0I} & !A_{1I}
        \end{sqmat}\\
%
!A_{IL} &= \begin{sqmat}{c}
        !A_{I0} \\ 
        !A_{I1}
        \end{sqmat}
\end{split}
\end{equation}
%
Now consider the system of equations
%
\begin{equation} 
 !A!x = !b
\end{equation}
%
which is split as
%
\begin{equation} 
\begin{split}
  !A_{LL} \, !x_L + !A_{LI} \, !x_I & = !b_L \\
  !A_{IL} \, !x_L + !A_{II} \, !x_I & = !b_I 
\end{split}
\end{equation}
%
Now consider eliminating $!x_L$ from the first equation and replacing
in the second so that, we have an equation for $!x_I$
%
\begin{equation}  \label{eq:interfp}  
\begin{split}
  (!A_{II} - !A_{IL} \, !A_{LL}\muno \, !A_{LI}) \, !x_I 
        &= (!b_I - !A_{IL} \, !A_{LL}\muno \, !b_L)\\
  \btA \, !x_I &= \btb_I 
\end{split}
\end{equation}
%
We consider solving this system of equations by an iterative method
such as GMRES, for instance. For such an iterative method, we have
only to specify how to compute the modified right hand side $\btb$ and
also how to compute the matrix-vector product $!y=\btA\,!x$. Computing
the matrix product involves the following steps
%
\begin{enumerate}
\item Compute $!y = !A_{II} \, !x$
\item Compute $!w = !A_{LI} \, !x$
\item Solve $!A_{LL} \, !z = !w$ for $!z$
\item Compute $!v = !A_{IL} \, !z$
\item Add $!y \gets !y - !v$
\end{enumerate}
%
involving three matrix products with matrices $!A_{II}$, $!A_{IL}$ and
$!A_{LI}$ and to solve the system with $!A_{LL}$. As the matrix
$!A_{LL}$ has no elements connecting unknowns in different processors
the solution system may be computed very efficiently in parallel.

\SSSection{Interface preconditioning} 

\index{interface preconditioning} 
\index{Schur matrix} 
To improve convergence of the interface problem(\ref{eq:interfp}) some
preconditioning can be introduced. As the interface matrix is never
built, true Jacobi preconditioning (i.e. using the diagonal part of
the Schur complement, $!P=\diag{\btA}$, where $!P$ is the
preconditioning matrix), can not be used. However we can use the
diagonal part of the interface matrix ($!P=\diag{!A_{II}}$) matrix or
even the whole interface matrix ($!P=!A_{II}$). Using the diagonal
preconditining helps to reduce bad conditioning due to refinement and
inter-equation bad scaling.  If the whole interface matrix is used
($!P=!A_{II}$) then the linear system $!A_{II}!w = !x$ has to be
solved at each iteration. This can be done with a direct solver or
iteratively. In the 2D case the connectivity of the interface matrix
is 1D or 1D like, so that the direct option is possible. However in 3D
the connectiviy is 2D like, and the direct solver is much more
expensive. In addition the interface matrix is scattered among all
processors. The iterative solution is much more appealing since the
fact that the matrix is scattered among processors is not a
problem. In addition the interface matrix is usually diagonally
dominant, even when the whole matrix is not. For instance for the
Laplace equation in 2D the stencil of the interface matrix on a planar
interface in a homogeneous grid of step $h$ with bilinear quad
elements is $[-1 8 -1]/3h^2$. Such matrix has a condition number which
is independent of $h$ and is asymptotically $\kappa(A_{II}) = 5/3$. In
the case of using triangular elements (by the way, this is equivalent
to the finite difference case) the stencil is $[-1 4 -1]/2h^2$ whose
condition number is $\kappa(A_{II}) = 3$. This low condition number
also favors the use of an iterative method. However a disdvantage for
the iterative solution is that non-stationary iterative solvers (i.e.,
those where the next iteration $x^{k+1}$ doesn't depend only on the
previous one: $x^{k+1} = f(x^k)$, like CG or GMRES) can not be nested
inside other non-stationary method, unless the inner preconditioning
loop is iterated to a very low error. This is because the conjugate
gradient directions will lose orthogonality. But using a very low
error bound for the preconditioning problem may be expensive so that
non-stationary iterative methods are discarded. Then, the use of
Richardson iteration is suggested. Sometimes this can diverge and some
under-relaxation must be used. Options controlling iteration for the
preconditioning problem can be found in section \S~\ref{sec:iisdopt}.

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSection{Implementation details of the IISD solver} 

\begin{itemize}
\item Currently unknowns and elements are partitioned in the same way
as for the PETSc solver. The best partitioning criteria could be
different for this solver than for the \PETSc{} iterative solver. 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\begin{figure*}[htb]
\centerline{\includegraphics{./OBJ/iisd2.\EPSPDF}}
\caption{IISD deccomposition by subdomains. Actual decomposition.}
\label{fg:iisd2}
\end{figure*}
%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 

\index{partitioning}
\item Selecting \emph{``interface''} and \emph{``local''} dof's:
One strategy could be to mark all dof's that are connected to a dof in
other processor as \emph{``interface''}. However this could lead to an
\emph{``interface''} dof set twice larger in the average than the minimum
needed. As the number of nodes in the \emph{``interface''} set
determines the size of the interface problem (\ref{eq:interfp}) it is
clear that we should try to choose an interface set as small as
possible. 

In \verb+read_mesh()+ partitioning is done on the dual graph, i.e. on
the elements. Nodes are then partitioned in the following way: A node
that is connected to elements in different processors is assigned to
the highest numbered processor. Referring to the mesh in
figure~\ref{fg:iisd} and with the same element partitioning all
nodes in the interface would belong to processor 1, as shown in
figure~\ref{fg:iisd2}. 

Now, if a dof $i$ is connected to a dof $j$ on other processor
we mark as \emph{``interface''} that dof that belongs to the highest
numbered processor. So, in the mesh of figure~\ref{fg:iisd2} all dof's
in the interface between element sub-domains are marked to belong to
processor 1. The nodes in the shadowed strip belong to processor 0 and
are connected to nodes in processor 1 but they are not marked as
\emph{``interface''} since they belong to the lowest numbered
processor. Note that this strategy leads to an interface set of 4
nodes, whereas the simpler strategy mentioned first would lead to an
interface set of 4 (i.e. including the nodes in the shadowed strip),
which is two times larger. 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\begin{figure*}[htb]
\centerline{\includegraphics{./OBJ/iisd3.\EPSPDF}}
\caption{Non local element contribution due to bad partitioning}
\label{fg:iisd3}
\end{figure*}
%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 

\index{DistMatrix@\verb+DistMatrix+}
\index{A_LL_other@\verb+A_LL_other+}
\item The \verb+IISDMat+ matrix object contains three \verb+MPI+
\PETSc{} matrices for the $A_{LI}$, $A_{IL}$ and $A_{II}$ blocks and a
sequential \PETSc{} matrix in each processor for the local part of the
$A_{LL}$ block. The $A_{LL}$ block must be defined as sequential
because otherwise we couldn't factorize it with the LU solver of
\PETSc{}. However this constrains that \verb+MatSetValues+ has to be
called in each processor for the matrix that belongs to its block,
i.e. elements in a given processor shouldn't contribute to $A_{LL}$
elements in other processors.  Normally, this is so, but for some
reasons this condition may be violated. One, is periodic boundary
conditions and constraints in general (they are not taken into account
for the partitioning). Another reasons is very bad partitioning that
may arise in some not so common situations. Consider for instance
figure~\ref{fg:iisd3}. Due to bad partitioning a rather isolated
element $e$ belongs to processor 0, while being surrounded by elements
in processor 1. Now, as nodes are assigned to the highest numbered
processor of the elements connected to the node, nodes $p$, $q$ and
$r$ are assigned to processor 1. But then, nodes $q$ and $r$ will
belong to the local subset of processor 1 but will receive
contributions from element $e$ in processor 0. However, the solution
is not to define this matrices as \PETSc{} because, so far, \PETSc{}
doesn't support for a distributed LU factorization. The solution we
devised is to store those $A_{LL}$ contributions that belong to other
processors in a temporary buffer and after, to send those
contributions to the correct processors directly with MPI
messages. This is performed with the DistMatrix object
\verb+A_LL_other+. 
\end{itemize}

\SSection{Efficiency considerations} 

\index{efficiency}
\index{block uploading}
The uploading time of elements in PETSc matrices can be significantly
reduced by using \emph{``block uploading''}, i.e. uploading an array
of values corresponding to a rectangular sub-matrix (not necessarily
being contiguous indices) instead of uploading each element at a
time. The following code snippets show the both types of uploading. 

\index{MatSetValue@\verb+MatSetValue+}
\index{MatSetValues@\verb+MatSetValues+}
\begin{verbatim}
// ELEMENT BY ELEMENT UPLOADING
// The global matrix
Mat A;
// row and column indices (both of length `nen')
int *row_indx,*col_indx;
// Elementary matrix (size `nen*nen')
double *Ae;
// ... define row_indx, col_indx and fill Ae
for (int j=0; j<nen; j++) {
  for (int k=0; k<nen; k++) {
    ierr = MatSetValue(A,row_int[j],col_indx[k],Ae[j*nen+k],ADD_VALUES);
  }  
}
\end{verbatim}

\begin{verbatim}
// BLOCK UPLOADING
// ... same stuff as before ...
ierr = MatSetValues(A,nen,row_int[j],nen,col_indx[k],Ae,ADD_VALUES);
\end{verbatim}

\index{masks}
\index{uploading}
\index{block uploading@\verb+block_uploading+}
In PETSc-FEM, the computed elemental matrices can be uploaded in
the global matrices with both methods, as selected with the
\verb+block_uploading+ global option (set to 1 by default, i.e. use
block uploading). However, in some cases block uploading can be
actually slower due to the use of \emph{``masks''}. A mask is a matrix
of the same size as the elemental matrix with 0's or 1's indicating
whether some coefficients are (structurally, i.e. not for a particular
state) null. Moreover, the mask do not depends on the particular
element, but it is rather a property of the terms being evaluated in
the Jacobian. 

\index{envelope mask} 
For instance, for the Navier-Stokes equations the Galerkin term only
has non null coefficients for the velocity unknowns in the continuity
equation, while the pressure gradient term only has coefficients for
the pressure unknowns in the momentum equations. Both terms together
have a mask as shown in figure~\ref{fg:nsprof}. When the application
writer codes such a term, he defines the mask. At the moment of
uploading the elements, if \verb+block_uploading+ is in effect, then
PETSc-FEM computes the \emph{``envelope''} of the mask, i.e. the
rectangular mask that contains the mask in order to make just one call
to \verb+MatSetValues+. In this case, the envelope is just a matrix
filled with 1's, so that block uploading pays the benefit of using the
faster \verb+MatSetValues+ routine, with the cost of loading much more
coefficients than the original mask. In addition, the PETSc matrix
will be bigger, with the corresponding increase in RAM demand and CPU
time in computing factorizations (IISD solver) and matrix/vector
products (PETSc solver). (In a future, such a combination of terms will
be loaded more efficiently with two calls to \verb+MatSetValues+.)

\index{report_consumed_time}
The conclusion is that, if the terms to be loaded have a very sparse
structure but a dense envelope, then may be block uploading is
slower. (The worst case is a diagonal-like mask.) Note that also, it's
not sufficient to have a sparse structure of the elemental matrix, but
also the application writer has to compute and return the mask. 
Finally, note that you can always check whether block uploading is
faster or slower by activating the time statistics for the elemset
(the \verb+report_consumed_time+) and run a large example with both
kinds of uploading. 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\begin{figure*}[htb]
\centerline{\includegraphics{./OBJ/nsprof}}
\caption{Example of mask for the continuity equation (Galerkin term) in 
   the Navier-Stokes equations}
\label{fg:nsprof}
\end{figure*}
%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\Section{The DistMap class} 

\index{DistMap@\verb+DistMap+}
\index{STL@\verb+STL+}
%
This class is an STL \verb+map<Key,Val>+ template container, where
each process can insert values and, at a certain point a
\verb+scatter()+ call sends contributions to the corresponding
processor. 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSection{Abstract interface} 

\index{BufferPack@\verb+BufferPack+}
To insert or access values one uses the standard \verb+insert()+
member or the \verb+[]+ operator of the basic class. The
\verb+processor(iterator k)+ member (to be defined by the user of the
template) should return the process rank to which the pair
\verb+(key,val)+ pointed by \verb+k+ should belong. After calling the
\verb+scatter()+ member by all processes, all entries are sent to
their processors. In order to send the data across processors, the
user has to define the \verb+size_of_pack()+ and \verb+pack()+
routines. The \verb+pack()+ and \verb+unpack()+ functions in the
\verb+BufferPack+ namespace can help to do that for basic types
(i.e. those for which the \verb+sizeof()+ operator and the \verb+libc+
\verb+memcpy()+ routine work). Finally the \verb+combine+ member
defines how new entries that have been sent from other processors have
to be merged in the local object. 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\SSection{Implementation details} 

When calling the \verb+scatter+ member, each entry in the basic
container is scanned and if it doesn't belong to the processor it is
buffered (using the \verb+pack+ member) to a buffer to be sent to
the other processor. Once all the data to be sent is buffered, the
entries are scanned again and the entries that have been buffered are
deleted. 

The buffers are sent to the other processors following a strategy
preventing deadlock in $\nproc-1$ stages, where $\nproc$ is the number
of processors. In the $k$-th stage, data is sent from processor $j$ to
processor $(j+k) \% \nproc$ where $\%$ is the remainder operator as in
the C language. In each stage, all processors other than the server do first a
\verb+MPI_Recv()+ and after a \verb+MPI_Send()+, while the server does
in the other sense, i.e. first a \verb+MPI_Send()+ and after a
\verb+MPI_Recv()+, initiating the process 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\begin{figure*}[htb]
\centerline{\includegraphics{./OBJ/sched.\EPSPDF}}
\caption{Simple scheduling algorithm for transferring data among processors}
\label{fg:sched}
\end{figure*}
%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
 
This is a rather inefficient strategy because at each stage all
sending is tied to the previous one, making the whole process
$\sim\nproc^2 T)$ where $T$ is the time needed to send a typical
individual buffer from one process to other. 

%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 
\begin{figure*}[htb]
\centerline{\includegraphics{./OBJ/sched2.\EPSPDF}}
\caption{Improved scheduling algorithm for transferring data among processors}
\label{fg:sched2}
\end{figure*}
%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 

An improved strategy consists in a multilevel scheduling
algorithm. Assume first that $\nproc$ is even and divide processors in
subsets $S_1 = \{0,1,\ldots,\nproc/2-1\}$, $S_2 =
\{\nproc/2,\nproc/2+1,\ldots,\nproc-1\}$. We first exchange all information
between processors in $S_1$ with those in $S_2$ in $\nproc/2$
stages. In stage 0 (see figure~\ref{fg:sched2}) processor $0$
exchanges data with $n/2$, 1 with $\nproc/2+1$, $\ldots$ and $n/2-1$
with $\nproc$.  i.e. processor 0 calls \verb+exchange+$(n/2)$ and
processor $n/2$ calls \verb+exchange+$(0)$, the code of procedure
\verb+exchange()+ is shown in the procedure below. In the stage 1
processor 0 exchanges with $\nproc/2+2$, 1 with $\nproc/2+3$,
$\nproc/2-2$ with $\nproc$ and $\nproc/2-1$ with $\nproc/2$,
i.e. processor $j$ exchanges with $(n/2)+(j+1)\%(\nproc/2)$. In
general, in stage $k$ processor $j$ exchanges with processor
$(n/2)+(j+k)\%(\nproc/2)$. Note that in each stage all communications
are paired and can be performed simultaneously, so that each stage can
be performed in $2T$ (sending plus receiving). This $\nproc/2$
stages take then a total of $2(\nproc/2)T = \nproc T$ secs. Now, all communication
between processors in $S_1$ and $S_2$ has been performed, it only
remains to perform the communication between processors in $S_1$ and
processors in $S_2$. But then, we can apply this idea recursively and
divide the processors in $S_1$ in two subsets $S_{11}$ and $S_{12}$
with $\nproc/4$ each (let's assume that the number of processors is a
power of 2, $\nproc = 2^m$), with a required time of $(\nproc/2) T$. 
Applying the idea recursively we arrive
to an estimation of a total time of
%
\begin{equation} 
\begin{split}
  T(n) &= [\nproc + (\nproc/2) + \ldots + 1 ]T\\
        &= (2\nproc-1) T \\
        &\sim O(2\nproc T)
\end{split}
\end{equation}
%
Then, it is significantly better than the previous algorithm. 

\begin{verbatim}
// Scheduling algorithm for exchanging data between processors
void exchange(j) {
  if (myrank>j) {
    // send data to processor j ...
    // receive data from processor j ...
  } else {
    // send data to processor j ...
    // receive data from processor j ...
  }
}
\end{verbatim}



% %\begin{latexonly}
% \begin{figure*}[htb]
% %\begin{algorithm}[htb]
% \label{al:exch}
% \caption{Scheduling algorithm for exchanging data between processors}
% \begin{algorithmic}
% \STATE {\textbf{procedure exchange}(j)}
%   \IF{{\tt myrank}$>j$}
%     \STATE send to $j$
%     \STATE receive from $j$
%   \ELSE
%     \STATE receive from $j$
%     \STATE send to $j$
%   \ENDIF
% \end{algorithmic}
% %\end{algorithm}
% \end{figure*}
% %\end{latexonly}
%<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*>---<*> 

% Local Variables: *
% mode: latex *
% tex-main-file: "petscfem.tex" *
% End: *
